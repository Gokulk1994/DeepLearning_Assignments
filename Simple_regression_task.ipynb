{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple_regression_task.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjlEFdXpMgvw",
        "colab_type": "text"
      },
      "source": [
        "# Deep learning programming I-A: Regression\n",
        "Felix Wiewel, Institute of Signal Processing and System Theory, University of Stuttgart, 23.05.2019\n",
        "\n",
        "## Introduction\n",
        "This programming exercise is the first of a series of exercises, which are intended as a supplement to the theoretical part of the Deep Learning course offered by the ISS. The goal is to introduce you to basic tasks and applications of methods you have encountered in the lecture. After completing the exercise you should be familiar with the basic ideas and one, possibly simple, way of solving the respective task. It is worth mentioning that most of the tasks can be solved in many different, not necessarily deep learning based, ways and the solution presented here is just one of them.\n",
        "\n",
        "## Regression\n",
        "\n",
        "In this exercise we consider the problem of regression, where we are interested in modeling a functional dependence between different variables with, possibly noisy, observations of input-output pairs. Mathematically such a dependence can be formulated as\n",
        "\n",
        "$\\mathbf{y}=f(\\mathbf{x})+\\boldsymbol{\\epsilon}$,\n",
        "\n",
        "where $\\mathbf{y}\\in\\mathbb{R}^{M}$ and $\\mathbf{x}\\in\\mathbb{R}^{N}$ are the input and output observations, $f:\\mathbb{R}^{N}\\rightarrow\\mathbb{R}^{M}$ is the function mapping inputs to outputs and $\\boldsymbol{\\epsilon}\\in\\mathbb{R}^{M}$ is a random vector, which models noise in our observations. Note that this assumes additive noise that only acts on the output and not on the input variable, which might not be true in all practical applications but is a reasonable approximation. For regression we are now interested in estimating the functional relationship $f$ between the inputs and outputs. This can be done in many different ways, not just with neural networks, but for this exercise we focus on approximating this relationship with a neural network $g_{\\boldsymbol{\\theta}}:\\mathbb{R}^{N}\\rightarrow\\mathbb{R}^{M}$ with parameter vector $\\boldsymbol{\\theta}$. The task is now to choose the parameters of the neural network in a way that results in a \"good\" approximation of $f$ with $g_{\\boldsymbol{\\theta}}$.\n",
        "\n",
        "In order to quantify how \"good\" our neural network can approximate $f$, we adopt a probabilistic view. For this we make the assumption that the noise $\\boldsymbol{\\epsilon}$ is a random vector drawn from a known dustribution, which enables us to derive a suitable cost function for training our neural network and also for quantifying a \"good\" approximation.\n",
        "\n",
        "### Mathematical formulation\n",
        "If we assume that the noise $\\boldsymbol{\\epsilon}$ is drawn from a gaussian distribution, e.g. $\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I})$, we can use\n",
        "\n",
        "$\\mathbf{y}=g_{\\boldsymbol{\\theta}}(\\mathbf{x})+\\boldsymbol{\\epsilon}\\Rightarrow \\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})=\\boldsymbol{\\epsilon}$\n",
        "\n",
        "to derive a log likelihood. Since the probability density function (pdf) of a multivariate normal distribution is given by\n",
        "\n",
        "$p(\\mathbf{x})=\\dfrac{1}{\\sqrt{(2\\pi)^{D}\\vert\\mathbf{C}\\vert}}\\mathrm{e}^{-\\dfrac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})\\mathbf{C}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})^{T}}$,\n",
        "\n",
        "we get\n",
        "\n",
        "$\\ln{p(\\boldsymbol{\\epsilon})}=\\ln{\\dfrac{1}{\\sqrt{(2\\pi)^{M}\\sigma^{2}}}\\mathrm{e}^{-\\dfrac{1}{2\\sigma^{2}}\\Vert\\boldsymbol{\\epsilon}\\Vert_{2}^{2}}}=-\\dfrac{1}{2\\sigma^{2}}\\Vert\\boldsymbol{\\epsilon}\\Vert_{2}^{2}-\\dfrac{1}{2}\\ln{(2\\pi)^{M}\\sigma^{2}}$.\n",
        "\n",
        "Replacing $\\boldsymbol{\\epsilon}$ by $\\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})$ yields the log likelihood for one particular input-output pair:\n",
        "\n",
        "$\\mathcal{L}(\\mathbf{x},\\mathbf{y},\\boldsymbol{\\theta})=\\ln {p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})}=-\\dfrac{1}{2\\sigma^{2}}\\Vert\\boldsymbol{\\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})}\\Vert_{2}^{2}-\\dfrac{1}{2}\\ln{(2\\pi)^{M}\\sigma^{2}}$\n",
        "\n",
        "This log likelihood measures how likely the input-output pair is and we can use it to train our neural network. For this we maximize the expected log likelihood over all input-output pairs under the assumption that the noise is idependent and identically distributed (i.i.d.) over all input-output pairs. This corresponds to finding the parameters $\\boldsymbol{\\theta}^{\\star}$ of our neural network, which maximize the the expected probability for observing the corresponding input-output pairs. Mathematically the optimal parameters for our neural network are given by\n",
        "\n",
        "$\\boldsymbol{\\theta}^{\\star}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathbb{E}\\left[\\mathcal{L}(\\mathbf{x},\\mathbf{y},\\boldsymbol{\\theta})\\right]=\\arg\\max_{\\boldsymbol{\\theta}}\\mathbb{E}\\left[-\\Vert\\boldsymbol{\\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})}\\Vert_{2}^{2}\\right]\\approx\\arg\\min_{\\boldsymbol{\\theta}}\\dfrac{1}{N_{D}}\\sum_{i=1}^{N_{D}}\\Vert\\boldsymbol{\\mathbf{y}_{i}-g_{\\boldsymbol{\\theta}}(\\mathbf{x}_{i})}\\Vert_{2}^{2}$,\n",
        "\n",
        "where all terms, which are independent of $\\boldsymbol{\\theta}$, are ignored and the expectation operator is approximated by the mean over all $N_{D}$ input-output pairs. In other words we are maximizing the log likelihood by minimizing the mean squared error loss over all input-output pairs in our dataset, hence this approach is called Maximum Likelihood (ML) estimation. For solving this optimization problem and obtaining the optimal network parameters, stochastic gradient descent (SGD) or one of it's many variants is typically used.\n",
        "\n",
        "It is worth noting, that choosing different distributions for the noise $\\boldsymbol{\\epsilon}$ leads to different log likelihoods and therefore different cost functions for training the neural network. Another commonly used distribution for modelling the noise in regression tasks is the laplace distribution. Deriving the log likelihood and the corresponding costfunction leads to the mean absolute error, which is given by the $l_{1}$-norm of the difference between observations predictions of the neural network. This cost function is considered more robust against outliers since these have less influence on the averall loss compared to the mean squared error.\n",
        "\n",
        "###  Implementation\n",
        "\n",
        "In the following we consider a simple regression task, implement a neural network and train it based on the mathematical fomrulation above. For this we first need to create a set of input-output pairs, which then needs to be partitioned into a training, validation and test set. We also define some constants to be used for partitioning the data and the hyperparameters for our neural network.\n",
        "\n",
        "But before we can start, we need to install tensorflow and import the necessary packages tensorflow, numpy and matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JL0d398Mgvz",
        "colab_type": "code",
        "outputId": "fd301190-a125-4f55-aca8-bc7f1b7e6e4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!pip3 install tensorflow-gpu==2.0.0-beta1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-beta1 in /usr/local/lib/python3.6/dist-packages (2.0.0b1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.16.4)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.14.0a20190603)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.33.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.1.7)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.2.2)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.14.0.dev2019060501)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta1) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-beta1) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPuVp2lyNK2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J84v9uucMgv5",
        "colab_type": "text"
      },
      "source": [
        "Next we define our constants and set the random seeds of tensorflow and numpy in order to get reproducable results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwC-1OnHMgv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_train_samples = 600\n",
        "N_validation_samples = 100\n",
        "N_test_samples = 100\n",
        "N_samples = N_train_samples + N_validation_samples + N_test_samples\n",
        "noise_sig = 0.1\n",
        "N_epochs = 150\n",
        "batch_size = 8\n",
        "learning_rate = 0.01\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngFFSyG-MgwA",
        "colab_type": "text"
      },
      "source": [
        "We create $600$ training samples, $100$ validation samples to optimize our hyperparameters and $100$ test samples, which are used to check if our model can generalize to unseen data. Furthermore we set the level of noise added to the observations. For training the model we plan to train it for $150$ epochs with a batch size of $8$ and a learning rate of $0.81$. Next we create the actual signal and plot it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s8AtsdQMgwB",
        "colab_type": "code",
        "outputId": "e049fc4d-02a7-4ae1-d317-d0e203a85e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "x = np.linspace(0.0, 3.0, N_samples, dtype=np.float32)\n",
        "y = np.expand_dims(np.sin(1.0+x*x) + noise_sig*np.random.randn(N_samples).astype(np.float32), axis=-1)\n",
        "y_true = np.sin(1.0+x*x)\n",
        "plt.plot(x, y)\n",
        "plt.plot(x, y_true)\n",
        "plt.legend([\"Observation\", \"Ground truth\"])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3wx8vJlMgwI",
        "colab_type": "text"
      },
      "source": [
        "With the input-output pairs created, your first task is now to partition the data in the training, validation and test sets. Keep in mind that we have created the data in a structured way, i.e. the input-output pairs are ordered. This means you need to shuffle the data before partitioning it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDIMUZs0MgwK",
        "colab_type": "code",
        "outputId": "a869ad21-6181-4af3-9cce-64bf2778d05d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\"\"\" Shuffle and partition the data set accordingly. you can use the predefined constants \"N_train_samples\", \"N_validation_samples\" and \"N_test_samples\". Use the variable names that are already in the below code \n",
        "to store the final shuffled and partitioned data. Hint: Shuffle the data and the labels in such a way that the pairing between an image and it's label is preserved.\"\"\"\n",
        "\n",
        "# Shuffle the data\n",
        "InputDataSet = np.array(np.column_stack((x,y)))\n",
        "\n",
        "np.random.shuffle(InputDataSet) # In place operation. no need of reassignment to InputDataset variable\n",
        "\n",
        "x_New = InputDataSet[:,0].reshape(N_samples,1)\n",
        "y_New = InputDataSet[:,1].reshape(N_samples,1)\n",
        "\n",
        "# Partition the data\n",
        "x_train = x_New[0:N_train_samples]\n",
        "y_train = y_New[0:N_train_samples]\n",
        "x_validation = x_New[N_train_samples:(N_train_samples+N_validation_samples)]\n",
        "y_validation = y_New[N_train_samples:(N_train_samples+N_validation_samples)]\n",
        "x_test = x_New[-N_test_samples:]\n",
        "y_test = y_New[-N_test_samples:]\n",
        "\n",
        "print(\"Size of Train Sample : \",x_train.size,y_train.size )\n",
        "print(\"Size of Validation Sample : \",x_validation.size,y_validation.size )\n",
        "print(\"Size of Test Sample : \",x_test.size,y_test.size )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Train Sample :  600 600\n",
            "Size of Validation Sample :  100 100\n",
            "Size of Test Sample :  100 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ucvKRhOMgwN",
        "colab_type": "text"
      },
      "source": [
        "In order to feed the data to our model, we will use the Dataset class provided by tensorflow. This class is simple to use and provides all the functionality we need for shuffling, batching and feeding the data to our model. It is also tightly integrated into the tensorflow framework, which makes it very performant. Performance is not an aspect we need to worry about in this exercise, but it is important in more demanding applications.\n",
        "\n",
        "In this exercise we instantiate a separate Dataset object for the training, validation and test data sets, where we shuffle and repeat just the training data set. Shuffling the validation and test data sets is not necessary, since we only evaluate the loss on those data sets and do not perform SGD on it. Please fill in the missing part of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HifQ63iPMgwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Create three tensorflow Dataset objects that can be used to feed the training test and validation data to a neural network. Hint: For the training data set use shuffling, batching with the size according to\n",
        "the predefined constant \"batch_size\" and repeat the data set indefinetly. For the validation and test data sets no shuffling or batching is needed.\"\"\"\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(N_train_samples).batch(batch_size).repeat()\n",
        "\n",
        "validation_ds = tf.data.Dataset.from_tensor_slices((x_validation, y_validation))\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Crr6fIkMgwT",
        "colab_type": "text"
      },
      "source": [
        "In this exercise we will create a a simple neural network with two hidden layers containing $10$ neurons. For creating a model and keeping track of its weights a class called MyModel is used. When initializing an instance of this class the necessary variables are created and stored in a list called \"trainable_variables\". This makes it easy to get all trainable variables of the model. We also override the \\__call__ method of this class in order to implement the forward pass of the neural network. This method should accept the inputs to the neural network and should return the result of the forward pass as an output. Please fill in the missing part of the code and select suitable activation functions for the different layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq8ri416MgwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Implement a neural network with two hidden dense layers containing 10 neurons each. As an activation function use the tangens hyperbolicus (tf.nn.tanh()). Since we are not using Keras, we need to create and \n",
        "manage all the variables that we need ourselves. The varaibles are created in the constructor of our model class. Since we want to be able to just call the class with some inputs in order to make a prediction, \n",
        "we implement a __call__ method which computes the forward pass and returns the output of the network.\"\"\"\n",
        "\n",
        "class MyModel(object):\n",
        "    def __init__(self):\n",
        "        NUM_HIDDEN_NEURONS = 10 # No of neurons in both hidden layers are same\n",
        "        # Create model variables\n",
        "        self.W0 = tf.Variable(tf.random.normal([1,NUM_HIDDEN_NEURONS],0,1.0))\n",
        "        self.b0 = tf.Variable(tf.ones([NUM_HIDDEN_NEURONS]))\n",
        "        self.W1 = tf.Variable(tf.random.normal([NUM_HIDDEN_NEURONS,NUM_HIDDEN_NEURONS],0,1.0))\n",
        "        self.b1 = tf.Variable(tf.ones([NUM_HIDDEN_NEURONS]))\n",
        "        self.W2 = tf.Variable(tf.random.normal([NUM_HIDDEN_NEURONS,1],0,1.0))\n",
        "        self.b2 = tf.Variable(tf.ones([1,1]))\n",
        "        self.trainable_variables = [self.W0, self.b0, self.W1, self.b1, self.W2, self.b2]\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        # Compute forward pass\n",
        "        output = tf.cast(tf.reshape(inputs, [-1, 1]),tf.float32)           # Flattening the input\n",
        "        output = tf.matmul(output,self.W0) + self.b0   # Dense layer 1\n",
        "        output = tf.nn.tanh(output)                    # Activation of 1st hidden layer\n",
        "        output = tf.matmul(output,self.W1) + self.b1      # Dense layer 2\n",
        "        output = tf.nn.tanh(output)                    # Activation of 2st hidden layer\n",
        "        output = tf.matmul(output,self.W2) + self.b2   # Dense layer 2 # No activation in output layer as it is a regression model\n",
        "   \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf6m8zXoMgwb",
        "colab_type": "text"
      },
      "source": [
        "Now after the model class is defined we can instantiate a MyModel object by running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSfI8wjLMgwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdl = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KraeH6MsMgwh",
        "colab_type": "text"
      },
      "source": [
        "We can now use the model to make predictions by calling it. In the following we predict on the inputs an plot the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1at5RObMgwi",
        "colab_type": "code",
        "outputId": "b6f0cff5-94ae-43d0-a1be-e691ded46397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "\"\"\" We want to plot a prediction on the complete data set with a model before training. For this make a prediction on the variable \"x\". \"\"\"\n",
        "\n",
        "y_pred = mdl(tf.cast(x,tf.float32))\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.plot(x, y_true)\n",
        "plt.plot(x, y_pred.numpy())\n",
        "plt.legend([\"Observation\", \"Target\", \"Prediction\"])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utFkv4F3NULQ",
        "colab_type": "text"
      },
      "source": [
        "Since we have initialized the variables of the neural network randomly, it's prediction is also random. In order to fit the model we need to minimize the expected mean squared error over all input-ouput pairs in our training data set. For this we need to create a function, that performs a training step when provided with the model, an optimizer and a batch of input-ouput pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4zFN0-kOdu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" For training we need to implement a function that executes one training step. Fill in the missing code pieces for this function.\"\"\"\n",
        "\n",
        "def train_step(model, optimizer, x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "        y_pred = model(x)# Compute a prediction with \"model\" on the input \"x\"\n",
        "        loss_val = tf.reduce_mean(tf.square(y-y_pred))# Compute the Mean Squared Error (MSE) for the prediction \"y_pred\" and the targets \"y\"\n",
        "    grads = tape.gradient(loss_val, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax-Kfd-tOm7I",
        "colab_type": "text"
      },
      "source": [
        "This function uses the GradientTape to record the operations for which gradients have to be calculated. In our case this is the forward pass through our model and the computation of the loss function. After these operations are recoded we can get its gradients and apply these through the use of an optimizer. Finally we return the loss value in order to print it.\n",
        "\n",
        "With the training step function defined we now need to choose a suitable optimizer. Tensorflow offers a wide variety of optimizers but in this exercise we will use the Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PahsqMscPcKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.optimizers.RMSprop(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_WjHrAwQB9e",
        "colab_type": "text"
      },
      "source": [
        "We now have everything we need to start training the model. For this we repeatedly sample a batch of input-output pairs from our training data set and use the train_step function to minimize the loss function over this batch. We repeat this until we have iterated over the complete training data set once. After this we compute the loss on the validation data set and print it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3LqS3d_QrX6",
        "colab_type": "code",
        "outputId": "040bb829-ecb5-4583-c864-1103070a75f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\" We can now use the train_step function to perform the training. Fill in the missing code parts.\"\"\"\n",
        "\n",
        "epoch = 0\n",
        "train_iters = 0\n",
        "train_loss = 0.0\n",
        "\n",
        "for x_t, y_t in train_ds:\n",
        "    train_loss += train_step(mdl,opt,x_t,y_t) # Perform a training step with the model \"mdl\" and the optimizer \"opt\" on the inputs \"x_t\" and the corresponding targets \"y_t\"\n",
        "    train_iters += 1\n",
        "    if ((N_train_samples / batch_size) == train_iters ):# An epoch is completed\n",
        "        for x_v, y_v in validation_ds:\n",
        "            y_pred = mdl(x_v)# Compute a prediction with \"mdl\" on the input \"x_v\"\n",
        "            validation_loss = tf.reduce_mean(tf.square(y_v-y_pred))# Compute the Mean Squared Error (MSE) for the prediction \"y_pred\" and the targets \"y_v\"\n",
        "        print(\"Epoch: {} Train loss: {:.5} Validation loss: {:.5}\".format(epoch, train_loss/train_iters, validation_loss))\n",
        "        train_iters = 0\n",
        "        train_loss = 0.0\n",
        "        epoch += 1\n",
        "    if (epoch == N_epochs):\n",
        "        print(epoch)\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Train loss: 0.89237 Validation loss: 0.036111\n",
            "Epoch: 1 Train loss: 0.21175 Validation loss: 0.011659\n",
            "Epoch: 2 Train loss: 0.17614 Validation loss: 0.039997\n",
            "Epoch: 3 Train loss: 0.15405 Validation loss: 0.0078682\n",
            "Epoch: 4 Train loss: 0.14482 Validation loss: 0.0050608\n",
            "Epoch: 5 Train loss: 0.12283 Validation loss: 0.00037334\n",
            "Epoch: 6 Train loss: 0.10846 Validation loss: 0.015318\n",
            "Epoch: 7 Train loss: 0.091853 Validation loss: 0.010181\n",
            "Epoch: 8 Train loss: 0.071443 Validation loss: 0.03676\n",
            "Epoch: 9 Train loss: 0.055598 Validation loss: 0.0086735\n",
            "Epoch: 10 Train loss: 0.044249 Validation loss: 0.0012351\n",
            "Epoch: 11 Train loss: 0.041018 Validation loss: 4.5867e-06\n",
            "Epoch: 12 Train loss: 0.03546 Validation loss: 0.0087902\n",
            "Epoch: 13 Train loss: 0.033857 Validation loss: 0.00010458\n",
            "Epoch: 14 Train loss: 0.029381 Validation loss: 0.02551\n",
            "Epoch: 15 Train loss: 0.029599 Validation loss: 0.001229\n",
            "Epoch: 16 Train loss: 0.027126 Validation loss: 0.02364\n",
            "Epoch: 17 Train loss: 0.031515 Validation loss: 0.0019992\n",
            "Epoch: 18 Train loss: 0.0277 Validation loss: 0.0074351\n",
            "Epoch: 19 Train loss: 0.027407 Validation loss: 0.00093709\n",
            "Epoch: 20 Train loss: 0.026201 Validation loss: 0.00097092\n",
            "Epoch: 21 Train loss: 0.027673 Validation loss: 0.0063352\n",
            "Epoch: 22 Train loss: 0.026683 Validation loss: 0.0049716\n",
            "Epoch: 23 Train loss: 0.025972 Validation loss: 0.013\n",
            "Epoch: 24 Train loss: 0.024923 Validation loss: 0.002774\n",
            "Epoch: 25 Train loss: 0.025643 Validation loss: 0.0019625\n",
            "Epoch: 26 Train loss: 0.02483 Validation loss: 0.0020369\n",
            "Epoch: 27 Train loss: 0.024324 Validation loss: 0.0042789\n",
            "Epoch: 28 Train loss: 0.025229 Validation loss: 0.0027846\n",
            "Epoch: 29 Train loss: 0.024791 Validation loss: 0.0061022\n",
            "Epoch: 30 Train loss: 0.026812 Validation loss: 0.0039401\n",
            "Epoch: 31 Train loss: 0.025392 Validation loss: 0.0011243\n",
            "Epoch: 32 Train loss: 0.022061 Validation loss: 0.017081\n",
            "Epoch: 33 Train loss: 0.026402 Validation loss: 0.00085632\n",
            "Epoch: 34 Train loss: 0.023517 Validation loss: 0.00095829\n",
            "Epoch: 35 Train loss: 0.023526 Validation loss: 0.0020861\n",
            "Epoch: 36 Train loss: 0.022874 Validation loss: 0.0018416\n",
            "Epoch: 37 Train loss: 0.023889 Validation loss: 0.00095086\n",
            "Epoch: 38 Train loss: 0.024349 Validation loss: 0.021495\n",
            "Epoch: 39 Train loss: 0.023755 Validation loss: 0.00066883\n",
            "Epoch: 40 Train loss: 0.024007 Validation loss: 0.0035051\n",
            "Epoch: 41 Train loss: 0.022996 Validation loss: 0.0052937\n",
            "Epoch: 42 Train loss: 0.02518 Validation loss: 0.0030911\n",
            "Epoch: 43 Train loss: 0.023224 Validation loss: 0.020228\n",
            "Epoch: 44 Train loss: 0.025034 Validation loss: 0.011836\n",
            "Epoch: 45 Train loss: 0.020452 Validation loss: 0.036212\n",
            "Epoch: 46 Train loss: 0.02293 Validation loss: 0.00010949\n",
            "Epoch: 47 Train loss: 0.024386 Validation loss: 0.00069384\n",
            "Epoch: 48 Train loss: 0.023335 Validation loss: 0.011926\n",
            "Epoch: 49 Train loss: 0.022582 Validation loss: 0.0017855\n",
            "Epoch: 50 Train loss: 0.024 Validation loss: 0.00033479\n",
            "Epoch: 51 Train loss: 0.02239 Validation loss: 0.0087339\n",
            "Epoch: 52 Train loss: 0.022046 Validation loss: 0.01539\n",
            "Epoch: 53 Train loss: 0.023143 Validation loss: 0.00023318\n",
            "Epoch: 54 Train loss: 0.023735 Validation loss: 0.0030105\n",
            "Epoch: 55 Train loss: 0.022395 Validation loss: 0.013913\n",
            "Epoch: 56 Train loss: 0.021847 Validation loss: 0.043085\n",
            "Epoch: 57 Train loss: 0.020989 Validation loss: 0.0014222\n",
            "Epoch: 58 Train loss: 0.02119 Validation loss: 0.014811\n",
            "Epoch: 59 Train loss: 0.021977 Validation loss: 0.021029\n",
            "Epoch: 60 Train loss: 0.021646 Validation loss: 0.0015503\n",
            "Epoch: 61 Train loss: 0.020847 Validation loss: 0.0022835\n",
            "Epoch: 62 Train loss: 0.021001 Validation loss: 0.00066443\n",
            "Epoch: 63 Train loss: 0.022391 Validation loss: 0.027488\n",
            "Epoch: 64 Train loss: 0.023755 Validation loss: 0.0047063\n",
            "Epoch: 65 Train loss: 0.0217 Validation loss: 0.00012303\n",
            "Epoch: 66 Train loss: 0.020358 Validation loss: 0.0039473\n",
            "Epoch: 67 Train loss: 0.019384 Validation loss: 0.002605\n",
            "Epoch: 68 Train loss: 0.020589 Validation loss: 0.0069291\n",
            "Epoch: 69 Train loss: 0.020712 Validation loss: 0.00027204\n",
            "Epoch: 70 Train loss: 0.020308 Validation loss: 0.014428\n",
            "Epoch: 71 Train loss: 0.020958 Validation loss: 0.010143\n",
            "Epoch: 72 Train loss: 0.020524 Validation loss: 0.0073987\n",
            "Epoch: 73 Train loss: 0.020056 Validation loss: 0.0029543\n",
            "Epoch: 74 Train loss: 0.020429 Validation loss: 0.021137\n",
            "Epoch: 75 Train loss: 0.019012 Validation loss: 0.0049887\n",
            "Epoch: 76 Train loss: 0.020406 Validation loss: 0.0011819\n",
            "Epoch: 77 Train loss: 0.022322 Validation loss: 0.005314\n",
            "Epoch: 78 Train loss: 0.022224 Validation loss: 0.0012219\n",
            "Epoch: 79 Train loss: 0.021815 Validation loss: 0.0004426\n",
            "Epoch: 80 Train loss: 0.020367 Validation loss: 0.0076358\n",
            "Epoch: 81 Train loss: 0.020433 Validation loss: 0.0051866\n",
            "Epoch: 82 Train loss: 0.020284 Validation loss: 0.003446\n",
            "Epoch: 83 Train loss: 0.021106 Validation loss: 0.0022599\n",
            "Epoch: 84 Train loss: 0.018792 Validation loss: 0.00039552\n",
            "Epoch: 85 Train loss: 0.021224 Validation loss: 0.0042787\n",
            "Epoch: 86 Train loss: 0.019013 Validation loss: 0.010657\n",
            "Epoch: 87 Train loss: 0.019526 Validation loss: 0.0016157\n",
            "Epoch: 88 Train loss: 0.02085 Validation loss: 0.00027449\n",
            "Epoch: 89 Train loss: 0.019939 Validation loss: 0.0060593\n",
            "Epoch: 90 Train loss: 0.019221 Validation loss: 0.006308\n",
            "Epoch: 91 Train loss: 0.021135 Validation loss: 0.00032873\n",
            "Epoch: 92 Train loss: 0.019184 Validation loss: 0.0012471\n",
            "Epoch: 93 Train loss: 0.018364 Validation loss: 0.014245\n",
            "Epoch: 94 Train loss: 0.022161 Validation loss: 0.00097908\n",
            "Epoch: 95 Train loss: 0.017599 Validation loss: 0.0069761\n",
            "Epoch: 96 Train loss: 0.019105 Validation loss: 0.0034565\n",
            "Epoch: 97 Train loss: 0.020512 Validation loss: 0.027807\n",
            "Epoch: 98 Train loss: 0.021604 Validation loss: 0.029974\n",
            "Epoch: 99 Train loss: 0.018923 Validation loss: 2.249e-05\n",
            "Epoch: 100 Train loss: 0.019866 Validation loss: 0.00074553\n",
            "Epoch: 101 Train loss: 0.020816 Validation loss: 0.0012718\n",
            "Epoch: 102 Train loss: 0.020402 Validation loss: 0.001194\n",
            "Epoch: 103 Train loss: 0.020373 Validation loss: 0.0029381\n",
            "Epoch: 104 Train loss: 0.018798 Validation loss: 0.00058548\n",
            "Epoch: 105 Train loss: 0.020054 Validation loss: 0.00045674\n",
            "Epoch: 106 Train loss: 0.020551 Validation loss: 0.011775\n",
            "Epoch: 107 Train loss: 0.019613 Validation loss: 0.0013408\n",
            "Epoch: 108 Train loss: 0.019404 Validation loss: 0.00068091\n",
            "Epoch: 109 Train loss: 0.017305 Validation loss: 0.00013248\n",
            "Epoch: 110 Train loss: 0.018964 Validation loss: 0.012644\n",
            "Epoch: 111 Train loss: 0.020157 Validation loss: 0.0018956\n",
            "Epoch: 112 Train loss: 0.019926 Validation loss: 0.00012163\n",
            "Epoch: 113 Train loss: 0.020664 Validation loss: 0.0087389\n",
            "Epoch: 114 Train loss: 0.019648 Validation loss: 0.0006457\n",
            "Epoch: 115 Train loss: 0.020402 Validation loss: 0.00059406\n",
            "Epoch: 116 Train loss: 0.020267 Validation loss: 0.0019039\n",
            "Epoch: 117 Train loss: 0.019447 Validation loss: 0.00017553\n",
            "Epoch: 118 Train loss: 0.02014 Validation loss: 0.002251\n",
            "Epoch: 119 Train loss: 0.019153 Validation loss: 0.0084661\n",
            "Epoch: 120 Train loss: 0.021457 Validation loss: 0.02602\n",
            "Epoch: 121 Train loss: 0.019468 Validation loss: 0.029574\n",
            "Epoch: 122 Train loss: 0.018123 Validation loss: 0.0090562\n",
            "Epoch: 123 Train loss: 0.019976 Validation loss: 0.0055985\n",
            "Epoch: 124 Train loss: 0.020205 Validation loss: 0.001354\n",
            "Epoch: 125 Train loss: 0.019043 Validation loss: 0.012153\n",
            "Epoch: 126 Train loss: 0.020158 Validation loss: 0.0021761\n",
            "Epoch: 127 Train loss: 0.018115 Validation loss: 0.0026448\n",
            "Epoch: 128 Train loss: 0.020546 Validation loss: 0.0021459\n",
            "Epoch: 129 Train loss: 0.018401 Validation loss: 0.0023647\n",
            "Epoch: 130 Train loss: 0.017837 Validation loss: 0.004257\n",
            "Epoch: 131 Train loss: 0.019458 Validation loss: 0.00020011\n",
            "Epoch: 132 Train loss: 0.018897 Validation loss: 0.00091826\n",
            "Epoch: 133 Train loss: 0.019321 Validation loss: 6.5229e-05\n",
            "Epoch: 134 Train loss: 0.020202 Validation loss: 0.0034707\n",
            "Epoch: 135 Train loss: 0.018879 Validation loss: 0.00022064\n",
            "Epoch: 136 Train loss: 0.017358 Validation loss: 0.019059\n",
            "Epoch: 137 Train loss: 0.018867 Validation loss: 6.7231e-05\n",
            "Epoch: 138 Train loss: 0.01867 Validation loss: 0.016221\n",
            "Epoch: 139 Train loss: 0.018544 Validation loss: 0.0011393\n",
            "Epoch: 140 Train loss: 0.018135 Validation loss: 0.000213\n",
            "Epoch: 141 Train loss: 0.019778 Validation loss: 0.013829\n",
            "Epoch: 142 Train loss: 0.018708 Validation loss: 0.0113\n",
            "Epoch: 143 Train loss: 0.018302 Validation loss: 0.003949\n",
            "Epoch: 144 Train loss: 0.019331 Validation loss: 0.0099875\n",
            "Epoch: 145 Train loss: 0.018511 Validation loss: 0.00047819\n",
            "Epoch: 146 Train loss: 0.018671 Validation loss: 0.0011243\n",
            "Epoch: 147 Train loss: 0.019215 Validation loss: 0.0022923\n",
            "Epoch: 148 Train loss: 0.017302 Validation loss: 0.0067015\n",
            "Epoch: 149 Train loss: 0.0185 Validation loss: 0.00067325\n",
            "150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DduaR_pCQ0k-",
        "colab_type": "text"
      },
      "source": [
        "After completion of the training process we use the test data set to test the models generalization to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ofClNnHRAUy",
        "colab_type": "code",
        "outputId": "055ba988-0e42-4eef-c7fd-2ac93a4f6e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for x_t, y_t in test_ds:\n",
        "    y_pred = mdl(x_t)# Compute a prediction with \"mdl\" on the input \"x_t\"\n",
        "    test_loss = tf.reduce_mean(tf.square(y_t-y_pred))# Compute the Mean Squared Error (MSE) for the prediction \"y_pred\" and the targets \"y_t\"\n",
        "print(\"Test loss: {:.5}\".format(test_loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.0040575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzJ7wOZmRbez",
        "colab_type": "text"
      },
      "source": [
        "After we have verified that our model achieves a similar loss on the test as on the validation and training data set, we can conclude that our model is not overfitting or underfitting and generalizes to unseen data. We can now predict on the inputs again and plot the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyvFN03bR8ez",
        "colab_type": "code",
        "outputId": "bd36b535-19b1-495c-b994-99ac741c04fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "\"\"\" Now we want to plot the prediction after training. Predict on the variable \"x\" again. \"\"\"\n",
        "\n",
        "y_pred = mdl(x)# Compute a prediction on the variable \"x\"\n",
        "plt.plot(x, y)\n",
        "plt.plot(x, y_true)\n",
        "plt.plot(x, y_pred.numpy())\n",
        "plt.legend([\"Observation\", \"Target\", \"Prediction\"])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKmuAAmLSmiR",
        "colab_type": "text"
      },
      "source": [
        "Now our model has learned to approximate the function mapping from the input to the output. The capability of neural networks to learn from input-ouput pairs alone and approximate an arbitrary function, see universal approximation theorem, can be very useful if the mapping between the input and output is too complex to be captured with model based approaches. But learning from input-ouput pairs alone implies that the model will only be able to make accurate predictions over input ranges it has seen during training. In order to demonstrate this we will predict on an interval that exeeds the $\\left[0,3\\right]$ interval the model was trained on, i.e. we will predict on the interval $\\left[-2,5\\right]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRF8hAmmVFcH",
        "colab_type": "code",
        "outputId": "dc2c3c82-0373-4d0c-f99d-64d90d4c8709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "x_generalize = np.linspace(-2.0, 5.0, N_samples, dtype=np.float32)\n",
        "y_generalize = np.sin(1.0+x_generalize*x_generalize) + noise_sig*np.random.randn(N_samples).astype(np.float32)\n",
        "y_truey_generalize = np.sin(1.0+x_generalize*x_generalize)\n",
        "y_pred = mdl(x_generalize)\n",
        "plt.plot(x_generalize, y_generalize)\n",
        "plt.plot(x_generalize, y_truey_generalize)\n",
        "plt.plot(x_generalize, y_pred.numpy())\n",
        "plt.legend([\"Observation\", \"Target\", \"Prediction\"])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nypOFYQsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2yr-0UCVIli",
        "colab_type": "text"
      },
      "source": [
        "As expected, the model is able to make farely accurate predictions on the interval it was trained on but makes unreliable predictions outside this interval.\n",
        "\n",
        "### Regularization\n",
        "In this section we will explore the concept of regularization. As there is no theorem that can be used to determine the required size and structure of a neural network given a certain task, one has to find a suitable neural architecture by trial and error. This can result in choosing a architecture with a capacity that is higher than required for solving the given task and hence overfitting might occur. A common way to prevent large neural networks networks from overfitting is to employ some sort of regularization, e.g. weight norm penalty, dropout, early stopping and data augmentation. In this section we will focus on weight norm penalty as a regularization and derive a probabilistic interpretation for some of those.\n",
        "\n",
        "We start by restating the conditional probability of the output $\\mathbf{y}$ given the input $\\mathbf{x}$ and the networks parameters $\\boldsymbol{\\theta}$\n",
        "\n",
        "$p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})=\\dfrac{1}{\\sqrt{(2\\pi)^{M}\\sigma^{2}}}\\mathrm{e}^{-\\dfrac{1}{2\\sigma^{2}}\\Vert\\boldsymbol{\\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})}\\Vert_{2}^{2}}$,\n",
        "\n",
        "which we used to derive the log likelihood. If we have some prior knowledge about the parameters of the neural network, which is given by a pdf $p(\\boldsymbol{\\theta})$ over the weights, we can use Bayes theorem to derive a posterior distribution\n",
        "\n",
        "$p(\\boldsymbol{\\theta}\\vert\\mathbf{x},\\mathbf{y})=\\dfrac{p(\\boldsymbol{\\theta},\\mathbf{y}\\vert\\mathbf{x})}{p(\\mathbf{y})}=\\dfrac{p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\mathbf{y})}$\n",
        "\n",
        "where $p(\\boldsymbol{\\theta})$ is the  prior over the networks parameters. Similar to the derivation at the beginning of the exercise, we can use this posterior distribution to derive a cost function for training the neural network. In this case, however, we are not maximizing the log likelihood but the posterior distribution over the weights, hence this approach is called Maximum A Posteori (MAP) estimation of the parameters. Mathematically we can formulate this as\n",
        "\n",
        "$\\boldsymbol{\\theta}^{\\star}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathbb{E}\\left[p(\\boldsymbol{\\theta}\\vert\\mathbf{x},\\mathbf{y})\\right]=\\arg\\max_{\\boldsymbol{\\theta}}\\mathbb{E}\\left[\\ln{p(\\boldsymbol{\\theta}\\vert\\mathbf{x},\\mathbf{y})}\\right]=\\arg\\max_{\\boldsymbol{\\theta}}\\ln{p(\\boldsymbol{\\theta})}+\\mathbb{E}\\left[\\ln{p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})}\\right]$,\n",
        "\n",
        "where we used the fact that applying a strictly increasing function, e.g. $\\ln{}$, does not change the position of the maximum of a cost function, ignored $p(\\mathbf{y})$, since it is independent of the network parameters and dropped the expectation operator for $\\ln{p(\\boldsymbol{\\theta})}$, since it is not depending on the random variable $\\mathbf{x}$. Comparing the MAP estimate of the parameters with the ML estimate we derived above, shows that the only difference is the addition of $\\ln{p(\\boldsymbol{\\theta})}$. This term is the regularization, i.e. the weight norm penalty. Depending on the distribution over $\\boldsymbol{\\theta}$ it can have different forms. If we choose a standard normal distribution, i.e. $\\boldsymbol{\\theta\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})}$,  we get\n",
        "\n",
        "$\\boldsymbol{\\theta}^{\\star}=\\arg\\min_{\\boldsymbol{\\theta}}\\lambda\\Vert\\boldsymbol{\\theta}\\Vert_{2}^{2}+\\dfrac{1}{N_{D}}\\sum_{i=1}^{N_{D}}\\Vert\\boldsymbol{\\mathbf{y}_{i}-g_{\\boldsymbol{\\theta}}(\\mathbf{x}_{i})}\\Vert_{2}^{2}$,\n",
        "\n",
        "where we have made the same simplifications as for the ML estimation and also introduced the parameter $\\lambda=\\sigma^{2}$, which is used to control the strength of the regularization. This form of regularization is commonly known as $l_{2}$-norm or weight decay regularization. Choosing a prior where the weights follow an i.i.d laplacian distribution, i.e. $p(\\boldsymbol{\\theta})=\\prod_{j}\\dfrac{1}{2}\\mathrm{e}^{\\vert\\theta_{j}\\vert}$, leads to\n",
        "\n",
        "$\\boldsymbol{\\theta}^{\\star}=\\arg\\min_{\\boldsymbol{\\theta}}\\lambda\\Vert\\boldsymbol{\\theta}\\Vert_{1}+\\dfrac{1}{N_{D}}\\sum_{i=1}^{N_{D}}\\Vert\\boldsymbol{\\mathbf{y}_{i}-g_{\\boldsymbol{\\theta}}(\\mathbf{x}_{i})}\\Vert_{2}^{2}$,\n",
        "\n",
        "where the strength of the regularization is again controlled by $\\lambda=\\sigma^{2}$. this type of regularization is known as  $l_{1}$-norm and it has the property to induce sparsity in the parameters of the network.\n",
        "\n",
        "With this theoretical background on regularization we can now implement it and observe it's effects on the regression problem covered in this exercise. For this we will define a model with a high capacity and train it for a extended time to provoke overfitting. For this, we will increase the number of hidden neurons in both hidden layers to $100$ and $50$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Fv2J-VK_vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Implement a bigger model with again two hidden layers contatining 100 and 50 neurons. As an activation use the tangens hyperbolicus function where it is appropiate. \"\"\"\n",
        "\n",
        "N_Neurons_1 = 100\n",
        "N_Neurons_2 = 50\n",
        "\n",
        "class MyBigModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "      \n",
        "        \n",
        "        \n",
        "        # Create model variables\n",
        "        self.W0 = tf.Variable(tf.random.normal([1,N_Neurons_1],0,1.0))\n",
        "        self.b0 = tf.Variable(tf.ones([N_Neurons_1,1]))\n",
        "        self.W1 = tf.Variable(tf.random.normal([N_Neurons_1,N_Neurons_2],0,1.0))\n",
        "        self.b1 = tf.Variable(tf.ones([N_Neurons_2,1]))\n",
        "        self.W2 = tf.Variable(tf.random.normal([N_Neurons_2,1],0,1.0))\n",
        "        self.b2 = tf.Variable(tf.ones([1,1]))\n",
        "        self.trainable_variables = [self.W0, self.b0, self.W1, self.b1, self.W2, self.b2]\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        # Compute forward pass\n",
        "        output = tf.reshape(inputs, [-1, 1])\n",
        "        output = tf.matmul(output,self.W0) + tf.reshape(self.b0,[N_Neurons_1,])\n",
        "        output = tf.nn.tanh(output)\n",
        "        output = tf.matmul(output,self.W1) + tf.reshape(self.b1,[N_Neurons_2,])\n",
        "        output = tf.nn.tanh(output) \n",
        "        output = tf.matmul(output,self.W2) + self.b2\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1XpaZTNLV-p",
        "colab_type": "text"
      },
      "source": [
        "After creating one instance of this class we can again train it on our data set. We will also create a new optimizer for training this bigger model, since some optimizers adapt the learning rates for individual parameters during a training process and we do not want to train our bigger model with learning rates adopted from an earlier training run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Yq-a1FLi0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "big_mdl = MyBigModel()\n",
        "big_opt = tf.optimizers.SGD(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBrf0I68MnMy",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to train this bigger model using the same training step and training loop. In order to provoke overfitting we also reduce the number of samples in the training data set a lot, increase the batch size and train for a more epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz_lXM8LMwXo",
        "colab_type": "code",
        "outputId": "328cfc39-cfb1-4eb2-9188-8f5605291459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\" Implement the training for the bigger model similar to the training of the small model before. \"\"\"\n",
        "\n",
        "N_train_samples_overfit = 30\n",
        "N_epochs = 1000\n",
        "batch_size = 30\n",
        "\n",
        "sel_idx = np.arange(0, N_train_samples)\n",
        "sel_idx = np.random.choice(sel_idx, N_train_samples_overfit)\n",
        "x_train_overfit = x_train[sel_idx]\n",
        "y_train_overfit = y_train[sel_idx]\n",
        "print(x_train_overfit.shape)\n",
        "train_overfit_ds = tf.data.Dataset.from_tensor_slices((x_train_overfit, y_train_overfit)).shuffle(N_train_samples_overfit).batch(batch_size).repeat()\n",
        "\n",
        "\n",
        "def train_step(model, optimizer, x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "        y_pred = model(x)# Compute a prediction with \"model\" on the input \"x\"\n",
        "        loss_val = tf.reduce_mean(tf.square(y-y_pred))# Compute the Mean Squared Error (MSE) for the prediction \"y_pred\" and the targets \"y\"\n",
        "    grads = tape.gradient(loss_val, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss_val\n",
        "\n",
        "epoch = 0\n",
        "train_iters = 0\n",
        "train_loss = 0.0\n",
        "for x_t, y_t in train_overfit_ds:\n",
        "    train_loss += train_step(big_mdl,big_opt,x_t,y_t)# Perform a training step with the model \"big_mdl\" and the optimizer \"big_opt\" on the inputs \"x_t\" and the corresponding targets \"y_t\"\n",
        "    train_iters += 1\n",
        "    if((N_train_samples_overfit / batch_size) == train_iters ):\n",
        "        for x_v, y_v in validation_ds:\n",
        "            y_pred = big_mdl(x_v)# Compute a prediction with \"big_mdl\" on the input \"x_v\"\n",
        "            validation_loss = tf.reduce_mean(tf.square(y_v-y_pred)) # Compute the Mean Squared Error (MSE) for the prediction \"y_pred\" and the targets \"y_v\"\n",
        "        print(\"Epoch: {} Train loss: {:.5} Validation loss: {:.5}\".format(epoch, train_loss/train_iters, validation_loss))\n",
        "        train_iters = 0\n",
        "        train_loss = 0.0\n",
        "        epoch += 1\n",
        "    if (epoch == N_epochs):\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30, 1)\n",
            "Epoch: 0 Train loss: 45.09 Validation loss: 3.6322\n",
            "Epoch: 1 Train loss: 10.987 Validation loss: 0.57208\n",
            "Epoch: 2 Train loss: 6.9673 Validation loss: 0.18825\n",
            "Epoch: 3 Train loss: 4.6614 Validation loss: 0.98095\n",
            "Epoch: 4 Train loss: 1.9062 Validation loss: 0.051766\n",
            "Epoch: 5 Train loss: 1.035 Validation loss: 0.1922\n",
            "Epoch: 6 Train loss: 0.69543 Validation loss: 0.066407\n",
            "Epoch: 7 Train loss: 0.50474 Validation loss: 0.068351\n",
            "Epoch: 8 Train loss: 0.3846 Validation loss: 0.01703\n",
            "Epoch: 9 Train loss: 0.29731 Validation loss: 0.0076343\n",
            "Epoch: 10 Train loss: 0.22349 Validation loss: 2.1341e-05\n",
            "Epoch: 11 Train loss: 0.17034 Validation loss: 0.0034655\n",
            "Epoch: 12 Train loss: 0.14296 Validation loss: 0.011626\n",
            "Epoch: 13 Train loss: 0.12414 Validation loss: 0.015506\n",
            "Epoch: 14 Train loss: 0.11024 Validation loss: 0.022111\n",
            "Epoch: 15 Train loss: 0.09975 Validation loss: 0.022742\n",
            "Epoch: 16 Train loss: 0.091702 Validation loss: 0.028052\n",
            "Epoch: 17 Train loss: 0.085556 Validation loss: 0.026075\n",
            "Epoch: 18 Train loss: 0.080849 Validation loss: 0.03186\n",
            "Epoch: 19 Train loss: 0.077418 Validation loss: 0.027042\n",
            "Epoch: 20 Train loss: 0.075059 Validation loss: 0.035377\n",
            "Epoch: 21 Train loss: 0.074162 Validation loss: 0.025978\n",
            "Epoch: 22 Train loss: 0.074843 Validation loss: 0.040332\n",
            "Epoch: 23 Train loss: 0.079509 Validation loss: 0.022167\n",
            "Epoch: 24 Train loss: 0.088419 Validation loss: 0.04955\n",
            "Epoch: 25 Train loss: 0.11368 Validation loss: 0.014363\n",
            "Epoch: 26 Train loss: 0.1454 Validation loss: 0.067515\n",
            "Epoch: 27 Train loss: 0.23817 Validation loss: 0.0042845\n",
            "Epoch: 28 Train loss: 0.25453 Validation loss: 0.08611\n",
            "Epoch: 29 Train loss: 0.417 Validation loss: 0.00015575\n",
            "Epoch: 30 Train loss: 0.24284 Validation loss: 0.067799\n",
            "Epoch: 31 Train loss: 0.3102 Validation loss: 0.0008917\n",
            "Epoch: 32 Train loss: 0.19526 Validation loss: 0.057773\n",
            "Epoch: 33 Train loss: 0.23375 Validation loss: 0.0031537\n",
            "Epoch: 34 Train loss: 0.1576 Validation loss: 0.052795\n",
            "Epoch: 35 Train loss: 0.18131 Validation loss: 0.0063601\n",
            "Epoch: 36 Train loss: 0.13773 Validation loss: 0.051995\n",
            "Epoch: 37 Train loss: 0.15925 Validation loss: 0.0086345\n",
            "Epoch: 38 Train loss: 0.12868 Validation loss: 0.052147\n",
            "Epoch: 39 Train loss: 0.15001 Validation loss: 0.0097796\n",
            "Epoch: 40 Train loss: 0.12512 Validation loss: 0.052379\n",
            "Epoch: 41 Train loss: 0.1472 Validation loss: 0.010105\n",
            "Epoch: 42 Train loss: 0.124 Validation loss: 0.052357\n",
            "Epoch: 43 Train loss: 0.14672 Validation loss: 0.010046\n",
            "Epoch: 44 Train loss: 0.12341 Validation loss: 0.051979\n",
            "Epoch: 45 Train loss: 0.14611 Validation loss: 0.0099216\n",
            "Epoch: 46 Train loss: 0.12229 Validation loss: 0.051277\n",
            "Epoch: 47 Train loss: 0.14423 Validation loss: 0.0098976\n",
            "Epoch: 48 Train loss: 0.12036 Validation loss: 0.050375\n",
            "Epoch: 49 Train loss: 0.14106 Validation loss: 0.01001\n",
            "Epoch: 50 Train loss: 0.11785 Validation loss: 0.049412\n",
            "Epoch: 51 Train loss: 0.13716 Validation loss: 0.01022\n",
            "Epoch: 52 Train loss: 0.11513 Validation loss: 0.048488\n",
            "Epoch: 53 Train loss: 0.13314 Validation loss: 0.010471\n",
            "Epoch: 54 Train loss: 0.11251 Validation loss: 0.047647\n",
            "Epoch: 55 Train loss: 0.12941 Validation loss: 0.010715\n",
            "Epoch: 56 Train loss: 0.11013 Validation loss: 0.046892\n",
            "Epoch: 57 Train loss: 0.12612 Validation loss: 0.010927\n",
            "Epoch: 58 Train loss: 0.10803 Validation loss: 0.046208\n",
            "Epoch: 59 Train loss: 0.12326 Validation loss: 0.011098\n",
            "Epoch: 60 Train loss: 0.10618 Validation loss: 0.045579\n",
            "Epoch: 61 Train loss: 0.12075 Validation loss: 0.011236\n",
            "Epoch: 62 Train loss: 0.10451 Validation loss: 0.044986\n",
            "Epoch: 63 Train loss: 0.1185 Validation loss: 0.011348\n",
            "Epoch: 64 Train loss: 0.10297 Validation loss: 0.044419\n",
            "Epoch: 65 Train loss: 0.11642 Validation loss: 0.011444\n",
            "Epoch: 66 Train loss: 0.10152 Validation loss: 0.043871\n",
            "Epoch: 67 Train loss: 0.11445 Validation loss: 0.011533\n",
            "Epoch: 68 Train loss: 0.10013 Validation loss: 0.043341\n",
            "Epoch: 69 Train loss: 0.11256 Validation loss: 0.011618\n",
            "Epoch: 70 Train loss: 0.098773 Validation loss: 0.042826\n",
            "Epoch: 71 Train loss: 0.11073 Validation loss: 0.0117\n",
            "Epoch: 72 Train loss: 0.097459 Validation loss: 0.042327\n",
            "Epoch: 73 Train loss: 0.10896 Validation loss: 0.01178\n",
            "Epoch: 74 Train loss: 0.096186 Validation loss: 0.041846\n",
            "Epoch: 75 Train loss: 0.10725 Validation loss: 0.011857\n",
            "Epoch: 76 Train loss: 0.094953 Validation loss: 0.04138\n",
            "Epoch: 77 Train loss: 0.10562 Validation loss: 0.01193\n",
            "Epoch: 78 Train loss: 0.093763 Validation loss: 0.040931\n",
            "Epoch: 79 Train loss: 0.10404 Validation loss: 0.011999\n",
            "Epoch: 80 Train loss: 0.092614 Validation loss: 0.040498\n",
            "Epoch: 81 Train loss: 0.10254 Validation loss: 0.012062\n",
            "Epoch: 82 Train loss: 0.091504 Validation loss: 0.040079\n",
            "Epoch: 83 Train loss: 0.10109 Validation loss: 0.012121\n",
            "Epoch: 84 Train loss: 0.090431 Validation loss: 0.039674\n",
            "Epoch: 85 Train loss: 0.099699 Validation loss: 0.012176\n",
            "Epoch: 86 Train loss: 0.089391 Validation loss: 0.039282\n",
            "Epoch: 87 Train loss: 0.098358 Validation loss: 0.012226\n",
            "Epoch: 88 Train loss: 0.08838 Validation loss: 0.038901\n",
            "Epoch: 89 Train loss: 0.09706 Validation loss: 0.012273\n",
            "Epoch: 90 Train loss: 0.087393 Validation loss: 0.038531\n",
            "Epoch: 91 Train loss: 0.0958 Validation loss: 0.012317\n",
            "Epoch: 92 Train loss: 0.086429 Validation loss: 0.038171\n",
            "Epoch: 93 Train loss: 0.094574 Validation loss: 0.012359\n",
            "Epoch: 94 Train loss: 0.085482 Validation loss: 0.03782\n",
            "Epoch: 95 Train loss: 0.093377 Validation loss: 0.012398\n",
            "Epoch: 96 Train loss: 0.084552 Validation loss: 0.037478\n",
            "Epoch: 97 Train loss: 0.092206 Validation loss: 0.012436\n",
            "Epoch: 98 Train loss: 0.083635 Validation loss: 0.037143\n",
            "Epoch: 99 Train loss: 0.091057 Validation loss: 0.012472\n",
            "Epoch: 100 Train loss: 0.082731 Validation loss: 0.036818\n",
            "Epoch: 101 Train loss: 0.089927 Validation loss: 0.012508\n",
            "Epoch: 102 Train loss: 0.081836 Validation loss: 0.036499\n",
            "Epoch: 103 Train loss: 0.088815 Validation loss: 0.012542\n",
            "Epoch: 104 Train loss: 0.08095 Validation loss: 0.036186\n",
            "Epoch: 105 Train loss: 0.087718 Validation loss: 0.012576\n",
            "Epoch: 106 Train loss: 0.08007 Validation loss: 0.035879\n",
            "Epoch: 107 Train loss: 0.086633 Validation loss: 0.012609\n",
            "Epoch: 108 Train loss: 0.079194 Validation loss: 0.03558\n",
            "Epoch: 109 Train loss: 0.085557 Validation loss: 0.012642\n",
            "Epoch: 110 Train loss: 0.078322 Validation loss: 0.035287\n",
            "Epoch: 111 Train loss: 0.084488 Validation loss: 0.012677\n",
            "Epoch: 112 Train loss: 0.077451 Validation loss: 0.034998\n",
            "Epoch: 113 Train loss: 0.083424 Validation loss: 0.012711\n",
            "Epoch: 114 Train loss: 0.076579 Validation loss: 0.034714\n",
            "Epoch: 115 Train loss: 0.082362 Validation loss: 0.012747\n",
            "Epoch: 116 Train loss: 0.075705 Validation loss: 0.034436\n",
            "Epoch: 117 Train loss: 0.0813 Validation loss: 0.012784\n",
            "Epoch: 118 Train loss: 0.074828 Validation loss: 0.03416\n",
            "Epoch: 119 Train loss: 0.080236 Validation loss: 0.012823\n",
            "Epoch: 120 Train loss: 0.073945 Validation loss: 0.033888\n",
            "Epoch: 121 Train loss: 0.079166 Validation loss: 0.012865\n",
            "Epoch: 122 Train loss: 0.073056 Validation loss: 0.033619\n",
            "Epoch: 123 Train loss: 0.078091 Validation loss: 0.012909\n",
            "Epoch: 124 Train loss: 0.072158 Validation loss: 0.033352\n",
            "Epoch: 125 Train loss: 0.077006 Validation loss: 0.012957\n",
            "Epoch: 126 Train loss: 0.07125 Validation loss: 0.033087\n",
            "Epoch: 127 Train loss: 0.075909 Validation loss: 0.013009\n",
            "Epoch: 128 Train loss: 0.070331 Validation loss: 0.032823\n",
            "Epoch: 129 Train loss: 0.074799 Validation loss: 0.013065\n",
            "Epoch: 130 Train loss: 0.069399 Validation loss: 0.032558\n",
            "Epoch: 131 Train loss: 0.073674 Validation loss: 0.013126\n",
            "Epoch: 132 Train loss: 0.068454 Validation loss: 0.032294\n",
            "Epoch: 133 Train loss: 0.072534 Validation loss: 0.013192\n",
            "Epoch: 134 Train loss: 0.067495 Validation loss: 0.032028\n",
            "Epoch: 135 Train loss: 0.071377 Validation loss: 0.013263\n",
            "Epoch: 136 Train loss: 0.066521 Validation loss: 0.031761\n",
            "Epoch: 137 Train loss: 0.070202 Validation loss: 0.013341\n",
            "Epoch: 138 Train loss: 0.065533 Validation loss: 0.031492\n",
            "Epoch: 139 Train loss: 0.069012 Validation loss: 0.013424\n",
            "Epoch: 140 Train loss: 0.064532 Validation loss: 0.03122\n",
            "Epoch: 141 Train loss: 0.067808 Validation loss: 0.013514\n",
            "Epoch: 142 Train loss: 0.06352 Validation loss: 0.030946\n",
            "Epoch: 143 Train loss: 0.066592 Validation loss: 0.01361\n",
            "Epoch: 144 Train loss: 0.062498 Validation loss: 0.030669\n",
            "Epoch: 145 Train loss: 0.065368 Validation loss: 0.013711\n",
            "Epoch: 146 Train loss: 0.061469 Validation loss: 0.030392\n",
            "Epoch: 147 Train loss: 0.064141 Validation loss: 0.013817\n",
            "Epoch: 148 Train loss: 0.060439 Validation loss: 0.030113\n",
            "Epoch: 149 Train loss: 0.062915 Validation loss: 0.013928\n",
            "Epoch: 150 Train loss: 0.059409 Validation loss: 0.029833\n",
            "Epoch: 151 Train loss: 0.061698 Validation loss: 0.014042\n",
            "Epoch: 152 Train loss: 0.058387 Validation loss: 0.029554\n",
            "Epoch: 153 Train loss: 0.060495 Validation loss: 0.014159\n",
            "Epoch: 154 Train loss: 0.057377 Validation loss: 0.029278\n",
            "Epoch: 155 Train loss: 0.059316 Validation loss: 0.014276\n",
            "Epoch: 156 Train loss: 0.056386 Validation loss: 0.029005\n",
            "Epoch: 157 Train loss: 0.058167 Validation loss: 0.014392\n",
            "Epoch: 158 Train loss: 0.055419 Validation loss: 0.028737\n",
            "Epoch: 159 Train loss: 0.057054 Validation loss: 0.014505\n",
            "Epoch: 160 Train loss: 0.054483 Validation loss: 0.028476\n",
            "Epoch: 161 Train loss: 0.055986 Validation loss: 0.014614\n",
            "Epoch: 162 Train loss: 0.053582 Validation loss: 0.028225\n",
            "Epoch: 163 Train loss: 0.054969 Validation loss: 0.014716\n",
            "Epoch: 164 Train loss: 0.052723 Validation loss: 0.027983\n",
            "Epoch: 165 Train loss: 0.054008 Validation loss: 0.01481\n",
            "Epoch: 166 Train loss: 0.051908 Validation loss: 0.027755\n",
            "Epoch: 167 Train loss: 0.053107 Validation loss: 0.014894\n",
            "Epoch: 168 Train loss: 0.051142 Validation loss: 0.02754\n",
            "Epoch: 169 Train loss: 0.052269 Validation loss: 0.014965\n",
            "Epoch: 170 Train loss: 0.050428 Validation loss: 0.027341\n",
            "Epoch: 171 Train loss: 0.051497 Validation loss: 0.015023\n",
            "Epoch: 172 Train loss: 0.049768 Validation loss: 0.027157\n",
            "Epoch: 173 Train loss: 0.050793 Validation loss: 0.015066\n",
            "Epoch: 174 Train loss: 0.049164 Validation loss: 0.026991\n",
            "Epoch: 175 Train loss: 0.050158 Validation loss: 0.015093\n",
            "Epoch: 176 Train loss: 0.048617 Validation loss: 0.026844\n",
            "Epoch: 177 Train loss: 0.049593 Validation loss: 0.015103\n",
            "Epoch: 178 Train loss: 0.048126 Validation loss: 0.026715\n",
            "Epoch: 179 Train loss: 0.049097 Validation loss: 0.015095\n",
            "Epoch: 180 Train loss: 0.047693 Validation loss: 0.026605\n",
            "Epoch: 181 Train loss: 0.04867 Validation loss: 0.015069\n",
            "Epoch: 182 Train loss: 0.047316 Validation loss: 0.026514\n",
            "Epoch: 183 Train loss: 0.04831 Validation loss: 0.015024\n",
            "Epoch: 184 Train loss: 0.046996 Validation loss: 0.026443\n",
            "Epoch: 185 Train loss: 0.048017 Validation loss: 0.014962\n",
            "Epoch: 186 Train loss: 0.04673 Validation loss: 0.026389\n",
            "Epoch: 187 Train loss: 0.04779 Validation loss: 0.014881\n",
            "Epoch: 188 Train loss: 0.046518 Validation loss: 0.026353\n",
            "Epoch: 189 Train loss: 0.047627 Validation loss: 0.014784\n",
            "Epoch: 190 Train loss: 0.046357 Validation loss: 0.026334\n",
            "Epoch: 191 Train loss: 0.047524 Validation loss: 0.014671\n",
            "Epoch: 192 Train loss: 0.046246 Validation loss: 0.026329\n",
            "Epoch: 193 Train loss: 0.04748 Validation loss: 0.014542\n",
            "Epoch: 194 Train loss: 0.04618 Validation loss: 0.026338\n",
            "Epoch: 195 Train loss: 0.04749 Validation loss: 0.014402\n",
            "Epoch: 196 Train loss: 0.046156 Validation loss: 0.026357\n",
            "Epoch: 197 Train loss: 0.047547 Validation loss: 0.014251\n",
            "Epoch: 198 Train loss: 0.046167 Validation loss: 0.026385\n",
            "Epoch: 199 Train loss: 0.047647 Validation loss: 0.014093\n",
            "Epoch: 200 Train loss: 0.04621 Validation loss: 0.026418\n",
            "Epoch: 201 Train loss: 0.047781 Validation loss: 0.013929\n",
            "Epoch: 202 Train loss: 0.046275 Validation loss: 0.026454\n",
            "Epoch: 203 Train loss: 0.04794 Validation loss: 0.013762\n",
            "Epoch: 204 Train loss: 0.046357 Validation loss: 0.026489\n",
            "Epoch: 205 Train loss: 0.048115 Validation loss: 0.013595\n",
            "Epoch: 206 Train loss: 0.046446 Validation loss: 0.026521\n",
            "Epoch: 207 Train loss: 0.048295 Validation loss: 0.013432\n",
            "Epoch: 208 Train loss: 0.046537 Validation loss: 0.026547\n",
            "Epoch: 209 Train loss: 0.048472 Validation loss: 0.013272\n",
            "Epoch: 210 Train loss: 0.046621 Validation loss: 0.026566\n",
            "Epoch: 211 Train loss: 0.048637 Validation loss: 0.01312\n",
            "Epoch: 212 Train loss: 0.046694 Validation loss: 0.026576\n",
            "Epoch: 213 Train loss: 0.048785 Validation loss: 0.012974\n",
            "Epoch: 214 Train loss: 0.046753 Validation loss: 0.026578\n",
            "Epoch: 215 Train loss: 0.048913 Validation loss: 0.012837\n",
            "Epoch: 216 Train loss: 0.046796 Validation loss: 0.026569\n",
            "Epoch: 217 Train loss: 0.049018 Validation loss: 0.012707\n",
            "Epoch: 218 Train loss: 0.046823 Validation loss: 0.026552\n",
            "Epoch: 219 Train loss: 0.049102 Validation loss: 0.012585\n",
            "Epoch: 220 Train loss: 0.046835 Validation loss: 0.026528\n",
            "Epoch: 221 Train loss: 0.049166 Validation loss: 0.012468\n",
            "Epoch: 222 Train loss: 0.046835 Validation loss: 0.026496\n",
            "Epoch: 223 Train loss: 0.049214 Validation loss: 0.012357\n",
            "Epoch: 224 Train loss: 0.046825 Validation loss: 0.02646\n",
            "Epoch: 225 Train loss: 0.049249 Validation loss: 0.012249\n",
            "Epoch: 226 Train loss: 0.046808 Validation loss: 0.026419\n",
            "Epoch: 227 Train loss: 0.049277 Validation loss: 0.012145\n",
            "Epoch: 228 Train loss: 0.046787 Validation loss: 0.026376\n",
            "Epoch: 229 Train loss: 0.049298 Validation loss: 0.012043\n",
            "Epoch: 230 Train loss: 0.046763 Validation loss: 0.02633\n",
            "Epoch: 231 Train loss: 0.049316 Validation loss: 0.011943\n",
            "Epoch: 232 Train loss: 0.046738 Validation loss: 0.026282\n",
            "Epoch: 233 Train loss: 0.049332 Validation loss: 0.011844\n",
            "Epoch: 234 Train loss: 0.046712 Validation loss: 0.026232\n",
            "Epoch: 235 Train loss: 0.049346 Validation loss: 0.011746\n",
            "Epoch: 236 Train loss: 0.046685 Validation loss: 0.026181\n",
            "Epoch: 237 Train loss: 0.049358 Validation loss: 0.01165\n",
            "Epoch: 238 Train loss: 0.046656 Validation loss: 0.026127\n",
            "Epoch: 239 Train loss: 0.049366 Validation loss: 0.011556\n",
            "Epoch: 240 Train loss: 0.046625 Validation loss: 0.026073\n",
            "Epoch: 241 Train loss: 0.04937 Validation loss: 0.011464\n",
            "Epoch: 242 Train loss: 0.046591 Validation loss: 0.026015\n",
            "Epoch: 243 Train loss: 0.049369 Validation loss: 0.011374\n",
            "Epoch: 244 Train loss: 0.046553 Validation loss: 0.025956\n",
            "Epoch: 245 Train loss: 0.049361 Validation loss: 0.011287\n",
            "Epoch: 246 Train loss: 0.04651 Validation loss: 0.025895\n",
            "Epoch: 247 Train loss: 0.049345 Validation loss: 0.011201\n",
            "Epoch: 248 Train loss: 0.046462 Validation loss: 0.025831\n",
            "Epoch: 249 Train loss: 0.049322 Validation loss: 0.011119\n",
            "Epoch: 250 Train loss: 0.046409 Validation loss: 0.025765\n",
            "Epoch: 251 Train loss: 0.049291 Validation loss: 0.011039\n",
            "Epoch: 252 Train loss: 0.04635 Validation loss: 0.025697\n",
            "Epoch: 253 Train loss: 0.049252 Validation loss: 0.010961\n",
            "Epoch: 254 Train loss: 0.046287 Validation loss: 0.025626\n",
            "Epoch: 255 Train loss: 0.049205 Validation loss: 0.010887\n",
            "Epoch: 256 Train loss: 0.046218 Validation loss: 0.025555\n",
            "Epoch: 257 Train loss: 0.049152 Validation loss: 0.010813\n",
            "Epoch: 258 Train loss: 0.046145 Validation loss: 0.025481\n",
            "Epoch: 259 Train loss: 0.049093 Validation loss: 0.010743\n",
            "Epoch: 260 Train loss: 0.046068 Validation loss: 0.025406\n",
            "Epoch: 261 Train loss: 0.049028 Validation loss: 0.010673\n",
            "Epoch: 262 Train loss: 0.045988 Validation loss: 0.02533\n",
            "Epoch: 263 Train loss: 0.048958 Validation loss: 0.010606\n",
            "Epoch: 264 Train loss: 0.045905 Validation loss: 0.025253\n",
            "Epoch: 265 Train loss: 0.048884 Validation loss: 0.01054\n",
            "Epoch: 266 Train loss: 0.04582 Validation loss: 0.025174\n",
            "Epoch: 267 Train loss: 0.048806 Validation loss: 0.010476\n",
            "Epoch: 268 Train loss: 0.045732 Validation loss: 0.025096\n",
            "Epoch: 269 Train loss: 0.048724 Validation loss: 0.010413\n",
            "Epoch: 270 Train loss: 0.045641 Validation loss: 0.025016\n",
            "Epoch: 271 Train loss: 0.048639 Validation loss: 0.010351\n",
            "Epoch: 272 Train loss: 0.045549 Validation loss: 0.024936\n",
            "Epoch: 273 Train loss: 0.048551 Validation loss: 0.010291\n",
            "Epoch: 274 Train loss: 0.045455 Validation loss: 0.024856\n",
            "Epoch: 275 Train loss: 0.04846 Validation loss: 0.010232\n",
            "Epoch: 276 Train loss: 0.045359 Validation loss: 0.024775\n",
            "Epoch: 277 Train loss: 0.048366 Validation loss: 0.010174\n",
            "Epoch: 278 Train loss: 0.045261 Validation loss: 0.024694\n",
            "Epoch: 279 Train loss: 0.048269 Validation loss: 0.010117\n",
            "Epoch: 280 Train loss: 0.045161 Validation loss: 0.024612\n",
            "Epoch: 281 Train loss: 0.048169 Validation loss: 0.010062\n",
            "Epoch: 282 Train loss: 0.04506 Validation loss: 0.02453\n",
            "Epoch: 283 Train loss: 0.048067 Validation loss: 0.010008\n",
            "Epoch: 284 Train loss: 0.044957 Validation loss: 0.024447\n",
            "Epoch: 285 Train loss: 0.047962 Validation loss: 0.0099547\n",
            "Epoch: 286 Train loss: 0.044853 Validation loss: 0.024364\n",
            "Epoch: 287 Train loss: 0.047855 Validation loss: 0.0099026\n",
            "Epoch: 288 Train loss: 0.044747 Validation loss: 0.024281\n",
            "Epoch: 289 Train loss: 0.047746 Validation loss: 0.0098518\n",
            "Epoch: 290 Train loss: 0.04464 Validation loss: 0.024198\n",
            "Epoch: 291 Train loss: 0.047634 Validation loss: 0.0098019\n",
            "Epoch: 292 Train loss: 0.044531 Validation loss: 0.024115\n",
            "Epoch: 293 Train loss: 0.047521 Validation loss: 0.0097526\n",
            "Epoch: 294 Train loss: 0.044422 Validation loss: 0.024032\n",
            "Epoch: 295 Train loss: 0.047405 Validation loss: 0.0097045\n",
            "Epoch: 296 Train loss: 0.044312 Validation loss: 0.023948\n",
            "Epoch: 297 Train loss: 0.047289 Validation loss: 0.0096573\n",
            "Epoch: 298 Train loss: 0.044201 Validation loss: 0.023865\n",
            "Epoch: 299 Train loss: 0.047172 Validation loss: 0.0096109\n",
            "Epoch: 300 Train loss: 0.04409 Validation loss: 0.023782\n",
            "Epoch: 301 Train loss: 0.047053 Validation loss: 0.0095654\n",
            "Epoch: 302 Train loss: 0.043978 Validation loss: 0.023698\n",
            "Epoch: 303 Train loss: 0.046933 Validation loss: 0.0095206\n",
            "Epoch: 304 Train loss: 0.043865 Validation loss: 0.023616\n",
            "Epoch: 305 Train loss: 0.046812 Validation loss: 0.0094762\n",
            "Epoch: 306 Train loss: 0.043752 Validation loss: 0.023532\n",
            "Epoch: 307 Train loss: 0.04669 Validation loss: 0.0094329\n",
            "Epoch: 308 Train loss: 0.043638 Validation loss: 0.02345\n",
            "Epoch: 309 Train loss: 0.046567 Validation loss: 0.0093902\n",
            "Epoch: 310 Train loss: 0.043524 Validation loss: 0.023368\n",
            "Epoch: 311 Train loss: 0.046444 Validation loss: 0.0093484\n",
            "Epoch: 312 Train loss: 0.043409 Validation loss: 0.023286\n",
            "Epoch: 313 Train loss: 0.04632 Validation loss: 0.0093071\n",
            "Epoch: 314 Train loss: 0.043295 Validation loss: 0.023204\n",
            "Epoch: 315 Train loss: 0.046195 Validation loss: 0.009266\n",
            "Epoch: 316 Train loss: 0.04318 Validation loss: 0.023123\n",
            "Epoch: 317 Train loss: 0.04607 Validation loss: 0.0092259\n",
            "Epoch: 318 Train loss: 0.043065 Validation loss: 0.023041\n",
            "Epoch: 319 Train loss: 0.045945 Validation loss: 0.0091864\n",
            "Epoch: 320 Train loss: 0.04295 Validation loss: 0.02296\n",
            "Epoch: 321 Train loss: 0.045819 Validation loss: 0.009148\n",
            "Epoch: 322 Train loss: 0.042834 Validation loss: 0.022879\n",
            "Epoch: 323 Train loss: 0.045692 Validation loss: 0.0091095\n",
            "Epoch: 324 Train loss: 0.042719 Validation loss: 0.022799\n",
            "Epoch: 325 Train loss: 0.045566 Validation loss: 0.0090721\n",
            "Epoch: 326 Train loss: 0.042603 Validation loss: 0.022719\n",
            "Epoch: 327 Train loss: 0.045439 Validation loss: 0.0090348\n",
            "Epoch: 328 Train loss: 0.042488 Validation loss: 0.022639\n",
            "Epoch: 329 Train loss: 0.045312 Validation loss: 0.008998\n",
            "Epoch: 330 Train loss: 0.042373 Validation loss: 0.02256\n",
            "Epoch: 331 Train loss: 0.045185 Validation loss: 0.0089617\n",
            "Epoch: 332 Train loss: 0.042257 Validation loss: 0.022481\n",
            "Epoch: 333 Train loss: 0.045058 Validation loss: 0.0089264\n",
            "Epoch: 334 Train loss: 0.042142 Validation loss: 0.022403\n",
            "Epoch: 335 Train loss: 0.04493 Validation loss: 0.0088909\n",
            "Epoch: 336 Train loss: 0.042027 Validation loss: 0.022325\n",
            "Epoch: 337 Train loss: 0.044803 Validation loss: 0.0088563\n",
            "Epoch: 338 Train loss: 0.041913 Validation loss: 0.022247\n",
            "Epoch: 339 Train loss: 0.044677 Validation loss: 0.0088222\n",
            "Epoch: 340 Train loss: 0.041799 Validation loss: 0.02217\n",
            "Epoch: 341 Train loss: 0.04455 Validation loss: 0.0087881\n",
            "Epoch: 342 Train loss: 0.041684 Validation loss: 0.022093\n",
            "Epoch: 343 Train loss: 0.044423 Validation loss: 0.008755\n",
            "Epoch: 344 Train loss: 0.04157 Validation loss: 0.022017\n",
            "Epoch: 345 Train loss: 0.044296 Validation loss: 0.0087221\n",
            "Epoch: 346 Train loss: 0.041456 Validation loss: 0.021941\n",
            "Epoch: 347 Train loss: 0.04417 Validation loss: 0.0086894\n",
            "Epoch: 348 Train loss: 0.041342 Validation loss: 0.021865\n",
            "Epoch: 349 Train loss: 0.044043 Validation loss: 0.0086573\n",
            "Epoch: 350 Train loss: 0.041229 Validation loss: 0.021789\n",
            "Epoch: 351 Train loss: 0.043917 Validation loss: 0.0086255\n",
            "Epoch: 352 Train loss: 0.041116 Validation loss: 0.021714\n",
            "Epoch: 353 Train loss: 0.043792 Validation loss: 0.0085939\n",
            "Epoch: 354 Train loss: 0.041004 Validation loss: 0.02164\n",
            "Epoch: 355 Train loss: 0.043667 Validation loss: 0.0085628\n",
            "Epoch: 356 Train loss: 0.040891 Validation loss: 0.021566\n",
            "Epoch: 357 Train loss: 0.043541 Validation loss: 0.0085321\n",
            "Epoch: 358 Train loss: 0.040779 Validation loss: 0.021493\n",
            "Epoch: 359 Train loss: 0.043416 Validation loss: 0.008502\n",
            "Epoch: 360 Train loss: 0.040667 Validation loss: 0.02142\n",
            "Epoch: 361 Train loss: 0.043291 Validation loss: 0.008472\n",
            "Epoch: 362 Train loss: 0.040556 Validation loss: 0.021346\n",
            "Epoch: 363 Train loss: 0.043167 Validation loss: 0.0084423\n",
            "Epoch: 364 Train loss: 0.040445 Validation loss: 0.021275\n",
            "Epoch: 365 Train loss: 0.043043 Validation loss: 0.0084133\n",
            "Epoch: 366 Train loss: 0.040334 Validation loss: 0.021203\n",
            "Epoch: 367 Train loss: 0.04292 Validation loss: 0.0083843\n",
            "Epoch: 368 Train loss: 0.040224 Validation loss: 0.021131\n",
            "Epoch: 369 Train loss: 0.042797 Validation loss: 0.0083558\n",
            "Epoch: 370 Train loss: 0.040115 Validation loss: 0.02106\n",
            "Epoch: 371 Train loss: 0.042674 Validation loss: 0.0083275\n",
            "Epoch: 372 Train loss: 0.040005 Validation loss: 0.02099\n",
            "Epoch: 373 Train loss: 0.042552 Validation loss: 0.0082993\n",
            "Epoch: 374 Train loss: 0.039896 Validation loss: 0.02092\n",
            "Epoch: 375 Train loss: 0.04243 Validation loss: 0.0082717\n",
            "Epoch: 376 Train loss: 0.039788 Validation loss: 0.02085\n",
            "Epoch: 377 Train loss: 0.042308 Validation loss: 0.0082442\n",
            "Epoch: 378 Train loss: 0.039679 Validation loss: 0.020781\n",
            "Epoch: 379 Train loss: 0.042187 Validation loss: 0.008217\n",
            "Epoch: 380 Train loss: 0.039572 Validation loss: 0.020712\n",
            "Epoch: 381 Train loss: 0.042066 Validation loss: 0.0081902\n",
            "Epoch: 382 Train loss: 0.039464 Validation loss: 0.020644\n",
            "Epoch: 383 Train loss: 0.041946 Validation loss: 0.0081638\n",
            "Epoch: 384 Train loss: 0.039358 Validation loss: 0.020576\n",
            "Epoch: 385 Train loss: 0.041827 Validation loss: 0.0081371\n",
            "Epoch: 386 Train loss: 0.039251 Validation loss: 0.020509\n",
            "Epoch: 387 Train loss: 0.041708 Validation loss: 0.0081113\n",
            "Epoch: 388 Train loss: 0.039145 Validation loss: 0.020442\n",
            "Epoch: 389 Train loss: 0.041589 Validation loss: 0.0080852\n",
            "Epoch: 390 Train loss: 0.03904 Validation loss: 0.020375\n",
            "Epoch: 391 Train loss: 0.041471 Validation loss: 0.0080598\n",
            "Epoch: 392 Train loss: 0.038935 Validation loss: 0.020309\n",
            "Epoch: 393 Train loss: 0.041353 Validation loss: 0.0080349\n",
            "Epoch: 394 Train loss: 0.03883 Validation loss: 0.020243\n",
            "Epoch: 395 Train loss: 0.041236 Validation loss: 0.0080099\n",
            "Epoch: 396 Train loss: 0.038726 Validation loss: 0.020178\n",
            "Epoch: 397 Train loss: 0.041118 Validation loss: 0.0079854\n",
            "Epoch: 398 Train loss: 0.038622 Validation loss: 0.020113\n",
            "Epoch: 399 Train loss: 0.041002 Validation loss: 0.0079607\n",
            "Epoch: 400 Train loss: 0.038519 Validation loss: 0.020048\n",
            "Epoch: 401 Train loss: 0.040887 Validation loss: 0.0079366\n",
            "Epoch: 402 Train loss: 0.038416 Validation loss: 0.019984\n",
            "Epoch: 403 Train loss: 0.040772 Validation loss: 0.0079127\n",
            "Epoch: 404 Train loss: 0.038314 Validation loss: 0.019921\n",
            "Epoch: 405 Train loss: 0.040657 Validation loss: 0.0078884\n",
            "Epoch: 406 Train loss: 0.038212 Validation loss: 0.019858\n",
            "Epoch: 407 Train loss: 0.040543 Validation loss: 0.007865\n",
            "Epoch: 408 Train loss: 0.03811 Validation loss: 0.019795\n",
            "Epoch: 409 Train loss: 0.040429 Validation loss: 0.0078418\n",
            "Epoch: 410 Train loss: 0.038009 Validation loss: 0.019733\n",
            "Epoch: 411 Train loss: 0.040316 Validation loss: 0.0078186\n",
            "Epoch: 412 Train loss: 0.037909 Validation loss: 0.01967\n",
            "Epoch: 413 Train loss: 0.040203 Validation loss: 0.0077958\n",
            "Epoch: 414 Train loss: 0.037809 Validation loss: 0.019609\n",
            "Epoch: 415 Train loss: 0.040091 Validation loss: 0.0077726\n",
            "Epoch: 416 Train loss: 0.03771 Validation loss: 0.019547\n",
            "Epoch: 417 Train loss: 0.03998 Validation loss: 0.00775\n",
            "Epoch: 418 Train loss: 0.037611 Validation loss: 0.019487\n",
            "Epoch: 419 Train loss: 0.039869 Validation loss: 0.0077281\n",
            "Epoch: 420 Train loss: 0.037512 Validation loss: 0.019426\n",
            "Epoch: 421 Train loss: 0.039758 Validation loss: 0.0077059\n",
            "Epoch: 422 Train loss: 0.037414 Validation loss: 0.019366\n",
            "Epoch: 423 Train loss: 0.039647 Validation loss: 0.0076836\n",
            "Epoch: 424 Train loss: 0.037316 Validation loss: 0.019306\n",
            "Epoch: 425 Train loss: 0.039538 Validation loss: 0.0076624\n",
            "Epoch: 426 Train loss: 0.037218 Validation loss: 0.019247\n",
            "Epoch: 427 Train loss: 0.039429 Validation loss: 0.0076405\n",
            "Epoch: 428 Train loss: 0.037122 Validation loss: 0.019188\n",
            "Epoch: 429 Train loss: 0.03932 Validation loss: 0.007619\n",
            "Epoch: 430 Train loss: 0.037026 Validation loss: 0.01913\n",
            "Epoch: 431 Train loss: 0.039212 Validation loss: 0.0075979\n",
            "Epoch: 432 Train loss: 0.036929 Validation loss: 0.019072\n",
            "Epoch: 433 Train loss: 0.039104 Validation loss: 0.0075768\n",
            "Epoch: 434 Train loss: 0.036834 Validation loss: 0.019013\n",
            "Epoch: 435 Train loss: 0.038998 Validation loss: 0.0075564\n",
            "Epoch: 436 Train loss: 0.036739 Validation loss: 0.018956\n",
            "Epoch: 437 Train loss: 0.038892 Validation loss: 0.0075358\n",
            "Epoch: 438 Train loss: 0.036645 Validation loss: 0.018899\n",
            "Epoch: 439 Train loss: 0.038786 Validation loss: 0.0075151\n",
            "Epoch: 440 Train loss: 0.036551 Validation loss: 0.018843\n",
            "Epoch: 441 Train loss: 0.03868 Validation loss: 0.0074949\n",
            "Epoch: 442 Train loss: 0.036458 Validation loss: 0.018787\n",
            "Epoch: 443 Train loss: 0.038576 Validation loss: 0.0074745\n",
            "Epoch: 444 Train loss: 0.036364 Validation loss: 0.018731\n",
            "Epoch: 445 Train loss: 0.038472 Validation loss: 0.0074548\n",
            "Epoch: 446 Train loss: 0.036272 Validation loss: 0.018675\n",
            "Epoch: 447 Train loss: 0.038368 Validation loss: 0.0074346\n",
            "Epoch: 448 Train loss: 0.03618 Validation loss: 0.01862\n",
            "Epoch: 449 Train loss: 0.038265 Validation loss: 0.0074147\n",
            "Epoch: 450 Train loss: 0.036088 Validation loss: 0.018565\n",
            "Epoch: 451 Train loss: 0.038163 Validation loss: 0.0073951\n",
            "Epoch: 452 Train loss: 0.035997 Validation loss: 0.018511\n",
            "Epoch: 453 Train loss: 0.038061 Validation loss: 0.0073757\n",
            "Epoch: 454 Train loss: 0.035906 Validation loss: 0.018456\n",
            "Epoch: 455 Train loss: 0.037959 Validation loss: 0.0073564\n",
            "Epoch: 456 Train loss: 0.035816 Validation loss: 0.018403\n",
            "Epoch: 457 Train loss: 0.037858 Validation loss: 0.0073377\n",
            "Epoch: 458 Train loss: 0.035726 Validation loss: 0.018349\n",
            "Epoch: 459 Train loss: 0.037757 Validation loss: 0.0073184\n",
            "Epoch: 460 Train loss: 0.035636 Validation loss: 0.018296\n",
            "Epoch: 461 Train loss: 0.037657 Validation loss: 0.0072999\n",
            "Epoch: 462 Train loss: 0.035548 Validation loss: 0.018243\n",
            "Epoch: 463 Train loss: 0.037557 Validation loss: 0.007281\n",
            "Epoch: 464 Train loss: 0.035459 Validation loss: 0.018191\n",
            "Epoch: 465 Train loss: 0.037458 Validation loss: 0.0072625\n",
            "Epoch: 466 Train loss: 0.035371 Validation loss: 0.018138\n",
            "Epoch: 467 Train loss: 0.03736 Validation loss: 0.0072441\n",
            "Epoch: 468 Train loss: 0.035284 Validation loss: 0.018087\n",
            "Epoch: 469 Train loss: 0.037262 Validation loss: 0.007226\n",
            "Epoch: 470 Train loss: 0.035196 Validation loss: 0.018036\n",
            "Epoch: 471 Train loss: 0.037165 Validation loss: 0.0072076\n",
            "Epoch: 472 Train loss: 0.03511 Validation loss: 0.017985\n",
            "Epoch: 473 Train loss: 0.037068 Validation loss: 0.0071898\n",
            "Epoch: 474 Train loss: 0.035023 Validation loss: 0.017934\n",
            "Epoch: 475 Train loss: 0.036972 Validation loss: 0.0071715\n",
            "Epoch: 476 Train loss: 0.034938 Validation loss: 0.017883\n",
            "Epoch: 477 Train loss: 0.036876 Validation loss: 0.0071539\n",
            "Epoch: 478 Train loss: 0.034853 Validation loss: 0.017833\n",
            "Epoch: 479 Train loss: 0.036781 Validation loss: 0.0071361\n",
            "Epoch: 480 Train loss: 0.034768 Validation loss: 0.017783\n",
            "Epoch: 481 Train loss: 0.036687 Validation loss: 0.0071183\n",
            "Epoch: 482 Train loss: 0.034684 Validation loss: 0.017734\n",
            "Epoch: 483 Train loss: 0.036593 Validation loss: 0.007101\n",
            "Epoch: 484 Train loss: 0.0346 Validation loss: 0.017685\n",
            "Epoch: 485 Train loss: 0.036499 Validation loss: 0.0070836\n",
            "Epoch: 486 Train loss: 0.034516 Validation loss: 0.017636\n",
            "Epoch: 487 Train loss: 0.036406 Validation loss: 0.0070663\n",
            "Epoch: 488 Train loss: 0.034433 Validation loss: 0.017587\n",
            "Epoch: 489 Train loss: 0.036313 Validation loss: 0.0070495\n",
            "Epoch: 490 Train loss: 0.03435 Validation loss: 0.01754\n",
            "Epoch: 491 Train loss: 0.036221 Validation loss: 0.007032\n",
            "Epoch: 492 Train loss: 0.034268 Validation loss: 0.017493\n",
            "Epoch: 493 Train loss: 0.03613 Validation loss: 0.0070153\n",
            "Epoch: 494 Train loss: 0.034187 Validation loss: 0.017445\n",
            "Epoch: 495 Train loss: 0.036039 Validation loss: 0.0069985\n",
            "Epoch: 496 Train loss: 0.034105 Validation loss: 0.017398\n",
            "Epoch: 497 Train loss: 0.035948 Validation loss: 0.006982\n",
            "Epoch: 498 Train loss: 0.034025 Validation loss: 0.017351\n",
            "Epoch: 499 Train loss: 0.035859 Validation loss: 0.0069652\n",
            "Epoch: 500 Train loss: 0.033944 Validation loss: 0.017304\n",
            "Epoch: 501 Train loss: 0.035769 Validation loss: 0.0069488\n",
            "Epoch: 502 Train loss: 0.033864 Validation loss: 0.017257\n",
            "Epoch: 503 Train loss: 0.03568 Validation loss: 0.0069322\n",
            "Epoch: 504 Train loss: 0.033785 Validation loss: 0.017212\n",
            "Epoch: 505 Train loss: 0.035592 Validation loss: 0.0069162\n",
            "Epoch: 506 Train loss: 0.033706 Validation loss: 0.017166\n",
            "Epoch: 507 Train loss: 0.035504 Validation loss: 0.0068998\n",
            "Epoch: 508 Train loss: 0.033627 Validation loss: 0.01712\n",
            "Epoch: 509 Train loss: 0.035416 Validation loss: 0.0068837\n",
            "Epoch: 510 Train loss: 0.033549 Validation loss: 0.017075\n",
            "Epoch: 511 Train loss: 0.035329 Validation loss: 0.0068675\n",
            "Epoch: 512 Train loss: 0.033471 Validation loss: 0.01703\n",
            "Epoch: 513 Train loss: 0.035243 Validation loss: 0.0068518\n",
            "Epoch: 514 Train loss: 0.033393 Validation loss: 0.016986\n",
            "Epoch: 515 Train loss: 0.035157 Validation loss: 0.0068357\n",
            "Epoch: 516 Train loss: 0.033316 Validation loss: 0.016941\n",
            "Epoch: 517 Train loss: 0.035072 Validation loss: 0.0068196\n",
            "Epoch: 518 Train loss: 0.03324 Validation loss: 0.016897\n",
            "Epoch: 519 Train loss: 0.034987 Validation loss: 0.006804\n",
            "Epoch: 520 Train loss: 0.033164 Validation loss: 0.016854\n",
            "Epoch: 521 Train loss: 0.034903 Validation loss: 0.0067883\n",
            "Epoch: 522 Train loss: 0.033088 Validation loss: 0.01681\n",
            "Epoch: 523 Train loss: 0.034819 Validation loss: 0.0067726\n",
            "Epoch: 524 Train loss: 0.033013 Validation loss: 0.016767\n",
            "Epoch: 525 Train loss: 0.034736 Validation loss: 0.006757\n",
            "Epoch: 526 Train loss: 0.032939 Validation loss: 0.016724\n",
            "Epoch: 527 Train loss: 0.034654 Validation loss: 0.0067414\n",
            "Epoch: 528 Train loss: 0.032865 Validation loss: 0.016682\n",
            "Epoch: 529 Train loss: 0.034572 Validation loss: 0.0067262\n",
            "Epoch: 530 Train loss: 0.032791 Validation loss: 0.01664\n",
            "Epoch: 531 Train loss: 0.03449 Validation loss: 0.006711\n",
            "Epoch: 532 Train loss: 0.032717 Validation loss: 0.016598\n",
            "Epoch: 533 Train loss: 0.034409 Validation loss: 0.0066957\n",
            "Epoch: 534 Train loss: 0.032644 Validation loss: 0.016556\n",
            "Epoch: 535 Train loss: 0.034328 Validation loss: 0.0066805\n",
            "Epoch: 536 Train loss: 0.032572 Validation loss: 0.016515\n",
            "Epoch: 537 Train loss: 0.034248 Validation loss: 0.0066657\n",
            "Epoch: 538 Train loss: 0.032499 Validation loss: 0.016473\n",
            "Epoch: 539 Train loss: 0.034168 Validation loss: 0.0066505\n",
            "Epoch: 540 Train loss: 0.032427 Validation loss: 0.016433\n",
            "Epoch: 541 Train loss: 0.034088 Validation loss: 0.0066355\n",
            "Epoch: 542 Train loss: 0.032356 Validation loss: 0.016392\n",
            "Epoch: 543 Train loss: 0.03401 Validation loss: 0.0066209\n",
            "Epoch: 544 Train loss: 0.032285 Validation loss: 0.016351\n",
            "Epoch: 545 Train loss: 0.033931 Validation loss: 0.0066058\n",
            "Epoch: 546 Train loss: 0.032214 Validation loss: 0.016312\n",
            "Epoch: 547 Train loss: 0.033854 Validation loss: 0.0065911\n",
            "Epoch: 548 Train loss: 0.032144 Validation loss: 0.016272\n",
            "Epoch: 549 Train loss: 0.033777 Validation loss: 0.0065763\n",
            "Epoch: 550 Train loss: 0.032075 Validation loss: 0.016232\n",
            "Epoch: 551 Train loss: 0.033701 Validation loss: 0.0065619\n",
            "Epoch: 552 Train loss: 0.032006 Validation loss: 0.016193\n",
            "Epoch: 553 Train loss: 0.033625 Validation loss: 0.006547\n",
            "Epoch: 554 Train loss: 0.031937 Validation loss: 0.016154\n",
            "Epoch: 555 Train loss: 0.033549 Validation loss: 0.0065324\n",
            "Epoch: 556 Train loss: 0.031869 Validation loss: 0.016115\n",
            "Epoch: 557 Train loss: 0.033474 Validation loss: 0.0065176\n",
            "Epoch: 558 Train loss: 0.031801 Validation loss: 0.016077\n",
            "Epoch: 559 Train loss: 0.033399 Validation loss: 0.0065033\n",
            "Epoch: 560 Train loss: 0.031733 Validation loss: 0.016038\n",
            "Epoch: 561 Train loss: 0.033325 Validation loss: 0.0064886\n",
            "Epoch: 562 Train loss: 0.031666 Validation loss: 0.016\n",
            "Epoch: 563 Train loss: 0.033252 Validation loss: 0.0064742\n",
            "Epoch: 564 Train loss: 0.0316 Validation loss: 0.015963\n",
            "Epoch: 565 Train loss: 0.033179 Validation loss: 0.00646\n",
            "Epoch: 566 Train loss: 0.031534 Validation loss: 0.015925\n",
            "Epoch: 567 Train loss: 0.033107 Validation loss: 0.0064457\n",
            "Epoch: 568 Train loss: 0.031468 Validation loss: 0.015888\n",
            "Epoch: 569 Train loss: 0.033035 Validation loss: 0.0064312\n",
            "Epoch: 570 Train loss: 0.031403 Validation loss: 0.015851\n",
            "Epoch: 571 Train loss: 0.032963 Validation loss: 0.0064169\n",
            "Epoch: 572 Train loss: 0.031338 Validation loss: 0.015814\n",
            "Epoch: 573 Train loss: 0.032892 Validation loss: 0.0064028\n",
            "Epoch: 574 Train loss: 0.031273 Validation loss: 0.015778\n",
            "Epoch: 575 Train loss: 0.032822 Validation loss: 0.0063886\n",
            "Epoch: 576 Train loss: 0.031209 Validation loss: 0.015742\n",
            "Epoch: 577 Train loss: 0.032752 Validation loss: 0.0063745\n",
            "Epoch: 578 Train loss: 0.031145 Validation loss: 0.015706\n",
            "Epoch: 579 Train loss: 0.032682 Validation loss: 0.0063606\n",
            "Epoch: 580 Train loss: 0.031082 Validation loss: 0.01567\n",
            "Epoch: 581 Train loss: 0.032613 Validation loss: 0.0063462\n",
            "Epoch: 582 Train loss: 0.031019 Validation loss: 0.015635\n",
            "Epoch: 583 Train loss: 0.032545 Validation loss: 0.0063325\n",
            "Epoch: 584 Train loss: 0.030957 Validation loss: 0.0156\n",
            "Epoch: 585 Train loss: 0.032477 Validation loss: 0.0063182\n",
            "Epoch: 586 Train loss: 0.030895 Validation loss: 0.015565\n",
            "Epoch: 587 Train loss: 0.03241 Validation loss: 0.0063042\n",
            "Epoch: 588 Train loss: 0.030833 Validation loss: 0.01553\n",
            "Epoch: 589 Train loss: 0.032343 Validation loss: 0.0062904\n",
            "Epoch: 590 Train loss: 0.030772 Validation loss: 0.015496\n",
            "Epoch: 591 Train loss: 0.032276 Validation loss: 0.0062763\n",
            "Epoch: 592 Train loss: 0.030711 Validation loss: 0.015461\n",
            "Epoch: 593 Train loss: 0.032211 Validation loss: 0.0062626\n",
            "Epoch: 594 Train loss: 0.030651 Validation loss: 0.015428\n",
            "Epoch: 595 Train loss: 0.032145 Validation loss: 0.0062483\n",
            "Epoch: 596 Train loss: 0.030591 Validation loss: 0.015394\n",
            "Epoch: 597 Train loss: 0.032081 Validation loss: 0.0062348\n",
            "Epoch: 598 Train loss: 0.030532 Validation loss: 0.01536\n",
            "Epoch: 599 Train loss: 0.032016 Validation loss: 0.0062207\n",
            "Epoch: 600 Train loss: 0.030473 Validation loss: 0.015327\n",
            "Epoch: 601 Train loss: 0.031953 Validation loss: 0.0062067\n",
            "Epoch: 602 Train loss: 0.030414 Validation loss: 0.015294\n",
            "Epoch: 603 Train loss: 0.031889 Validation loss: 0.0061933\n",
            "Epoch: 604 Train loss: 0.030356 Validation loss: 0.015261\n",
            "Epoch: 605 Train loss: 0.031827 Validation loss: 0.0061793\n",
            "Epoch: 606 Train loss: 0.030298 Validation loss: 0.015229\n",
            "Epoch: 607 Train loss: 0.031764 Validation loss: 0.0061653\n",
            "Epoch: 608 Train loss: 0.030241 Validation loss: 0.015197\n",
            "Epoch: 609 Train loss: 0.031703 Validation loss: 0.0061514\n",
            "Epoch: 610 Train loss: 0.030184 Validation loss: 0.015165\n",
            "Epoch: 611 Train loss: 0.031642 Validation loss: 0.0061379\n",
            "Epoch: 612 Train loss: 0.030128 Validation loss: 0.015133\n",
            "Epoch: 613 Train loss: 0.031581 Validation loss: 0.0061239\n",
            "Epoch: 614 Train loss: 0.030072 Validation loss: 0.015101\n",
            "Epoch: 615 Train loss: 0.031521 Validation loss: 0.0061101\n",
            "Epoch: 616 Train loss: 0.030016 Validation loss: 0.01507\n",
            "Epoch: 617 Train loss: 0.031461 Validation loss: 0.0060962\n",
            "Epoch: 618 Train loss: 0.029961 Validation loss: 0.015039\n",
            "Epoch: 619 Train loss: 0.031402 Validation loss: 0.0060824\n",
            "Epoch: 620 Train loss: 0.029906 Validation loss: 0.015008\n",
            "Epoch: 621 Train loss: 0.031344 Validation loss: 0.0060687\n",
            "Epoch: 622 Train loss: 0.029852 Validation loss: 0.014977\n",
            "Epoch: 623 Train loss: 0.031286 Validation loss: 0.0060551\n",
            "Epoch: 624 Train loss: 0.029798 Validation loss: 0.014947\n",
            "Epoch: 625 Train loss: 0.031228 Validation loss: 0.0060412\n",
            "Epoch: 626 Train loss: 0.029745 Validation loss: 0.014917\n",
            "Epoch: 627 Train loss: 0.031172 Validation loss: 0.0060275\n",
            "Epoch: 628 Train loss: 0.029692 Validation loss: 0.014887\n",
            "Epoch: 629 Train loss: 0.031115 Validation loss: 0.0060136\n",
            "Epoch: 630 Train loss: 0.02964 Validation loss: 0.014858\n",
            "Epoch: 631 Train loss: 0.03106 Validation loss: 0.0059995\n",
            "Epoch: 632 Train loss: 0.029588 Validation loss: 0.014828\n",
            "Epoch: 633 Train loss: 0.031005 Validation loss: 0.0059861\n",
            "Epoch: 634 Train loss: 0.029536 Validation loss: 0.014799\n",
            "Epoch: 635 Train loss: 0.03095 Validation loss: 0.005972\n",
            "Epoch: 636 Train loss: 0.029485 Validation loss: 0.01477\n",
            "Epoch: 637 Train loss: 0.030896 Validation loss: 0.0059581\n",
            "Epoch: 638 Train loss: 0.029434 Validation loss: 0.014742\n",
            "Epoch: 639 Train loss: 0.030842 Validation loss: 0.0059444\n",
            "Epoch: 640 Train loss: 0.029384 Validation loss: 0.014713\n",
            "Epoch: 641 Train loss: 0.030789 Validation loss: 0.0059309\n",
            "Epoch: 642 Train loss: 0.029334 Validation loss: 0.014685\n",
            "Epoch: 643 Train loss: 0.030736 Validation loss: 0.0059172\n",
            "Epoch: 644 Train loss: 0.029284 Validation loss: 0.014657\n",
            "Epoch: 645 Train loss: 0.030683 Validation loss: 0.0059032\n",
            "Epoch: 646 Train loss: 0.029235 Validation loss: 0.014629\n",
            "Epoch: 647 Train loss: 0.030632 Validation loss: 0.0058893\n",
            "Epoch: 648 Train loss: 0.029187 Validation loss: 0.014601\n",
            "Epoch: 649 Train loss: 0.030581 Validation loss: 0.0058754\n",
            "Epoch: 650 Train loss: 0.029139 Validation loss: 0.014574\n",
            "Epoch: 651 Train loss: 0.03053 Validation loss: 0.0058616\n",
            "Epoch: 652 Train loss: 0.029091 Validation loss: 0.014547\n",
            "Epoch: 653 Train loss: 0.03048 Validation loss: 0.0058478\n",
            "Epoch: 654 Train loss: 0.029043 Validation loss: 0.01452\n",
            "Epoch: 655 Train loss: 0.03043 Validation loss: 0.0058339\n",
            "Epoch: 656 Train loss: 0.028996 Validation loss: 0.014493\n",
            "Epoch: 657 Train loss: 0.030381 Validation loss: 0.0058199\n",
            "Epoch: 658 Train loss: 0.02895 Validation loss: 0.014467\n",
            "Epoch: 659 Train loss: 0.030333 Validation loss: 0.0058064\n",
            "Epoch: 660 Train loss: 0.028904 Validation loss: 0.014441\n",
            "Epoch: 661 Train loss: 0.030285 Validation loss: 0.0057924\n",
            "Epoch: 662 Train loss: 0.028858 Validation loss: 0.014414\n",
            "Epoch: 663 Train loss: 0.030237 Validation loss: 0.0057784\n",
            "Epoch: 664 Train loss: 0.028813 Validation loss: 0.014388\n",
            "Epoch: 665 Train loss: 0.03019 Validation loss: 0.0057646\n",
            "Epoch: 666 Train loss: 0.028768 Validation loss: 0.014363\n",
            "Epoch: 667 Train loss: 0.030144 Validation loss: 0.0057508\n",
            "Epoch: 668 Train loss: 0.028724 Validation loss: 0.014337\n",
            "Epoch: 669 Train loss: 0.030098 Validation loss: 0.0057365\n",
            "Epoch: 670 Train loss: 0.02868 Validation loss: 0.014313\n",
            "Epoch: 671 Train loss: 0.030053 Validation loss: 0.0057228\n",
            "Epoch: 672 Train loss: 0.028637 Validation loss: 0.014288\n",
            "Epoch: 673 Train loss: 0.030008 Validation loss: 0.0057088\n",
            "Epoch: 674 Train loss: 0.028594 Validation loss: 0.014263\n",
            "Epoch: 675 Train loss: 0.029964 Validation loss: 0.0056946\n",
            "Epoch: 676 Train loss: 0.028551 Validation loss: 0.014239\n",
            "Epoch: 677 Train loss: 0.029921 Validation loss: 0.0056807\n",
            "Epoch: 678 Train loss: 0.028509 Validation loss: 0.014214\n",
            "Epoch: 679 Train loss: 0.029877 Validation loss: 0.0056664\n",
            "Epoch: 680 Train loss: 0.028467 Validation loss: 0.01419\n",
            "Epoch: 681 Train loss: 0.029835 Validation loss: 0.0056526\n",
            "Epoch: 682 Train loss: 0.028426 Validation loss: 0.014166\n",
            "Epoch: 683 Train loss: 0.029793 Validation loss: 0.0056382\n",
            "Epoch: 684 Train loss: 0.028385 Validation loss: 0.014142\n",
            "Epoch: 685 Train loss: 0.029751 Validation loss: 0.0056241\n",
            "Epoch: 686 Train loss: 0.028345 Validation loss: 0.014119\n",
            "Epoch: 687 Train loss: 0.02971 Validation loss: 0.0056103\n",
            "Epoch: 688 Train loss: 0.028305 Validation loss: 0.014096\n",
            "Epoch: 689 Train loss: 0.029669 Validation loss: 0.0055962\n",
            "Epoch: 690 Train loss: 0.028265 Validation loss: 0.014072\n",
            "Epoch: 691 Train loss: 0.029629 Validation loss: 0.0055823\n",
            "Epoch: 692 Train loss: 0.028226 Validation loss: 0.01405\n",
            "Epoch: 693 Train loss: 0.029589 Validation loss: 0.0055682\n",
            "Epoch: 694 Train loss: 0.028186 Validation loss: 0.014027\n",
            "Epoch: 695 Train loss: 0.029549 Validation loss: 0.0055542\n",
            "Epoch: 696 Train loss: 0.028148 Validation loss: 0.014004\n",
            "Epoch: 697 Train loss: 0.029511 Validation loss: 0.0055398\n",
            "Epoch: 698 Train loss: 0.02811 Validation loss: 0.013983\n",
            "Epoch: 699 Train loss: 0.029472 Validation loss: 0.0055254\n",
            "Epoch: 700 Train loss: 0.028072 Validation loss: 0.01396\n",
            "Epoch: 701 Train loss: 0.029435 Validation loss: 0.0055113\n",
            "Epoch: 702 Train loss: 0.028035 Validation loss: 0.013939\n",
            "Epoch: 703 Train loss: 0.029397 Validation loss: 0.0054969\n",
            "Epoch: 704 Train loss: 0.027998 Validation loss: 0.013917\n",
            "Epoch: 705 Train loss: 0.029361 Validation loss: 0.0054827\n",
            "Epoch: 706 Train loss: 0.027962 Validation loss: 0.013895\n",
            "Epoch: 707 Train loss: 0.029324 Validation loss: 0.0054689\n",
            "Epoch: 708 Train loss: 0.027926 Validation loss: 0.013874\n",
            "Epoch: 709 Train loss: 0.029288 Validation loss: 0.0054546\n",
            "Epoch: 710 Train loss: 0.02789 Validation loss: 0.013853\n",
            "Epoch: 711 Train loss: 0.029252 Validation loss: 0.0054403\n",
            "Epoch: 712 Train loss: 0.027854 Validation loss: 0.013831\n",
            "Epoch: 713 Train loss: 0.029217 Validation loss: 0.005426\n",
            "Epoch: 714 Train loss: 0.027819 Validation loss: 0.013811\n",
            "Epoch: 715 Train loss: 0.029182 Validation loss: 0.005412\n",
            "Epoch: 716 Train loss: 0.027784 Validation loss: 0.01379\n",
            "Epoch: 717 Train loss: 0.029148 Validation loss: 0.0053976\n",
            "Epoch: 718 Train loss: 0.027749 Validation loss: 0.01377\n",
            "Epoch: 719 Train loss: 0.029114 Validation loss: 0.0053834\n",
            "Epoch: 720 Train loss: 0.027716 Validation loss: 0.013749\n",
            "Epoch: 721 Train loss: 0.02908 Validation loss: 0.0053696\n",
            "Epoch: 722 Train loss: 0.027682 Validation loss: 0.013729\n",
            "Epoch: 723 Train loss: 0.029047 Validation loss: 0.0053551\n",
            "Epoch: 724 Train loss: 0.027648 Validation loss: 0.013709\n",
            "Epoch: 725 Train loss: 0.029014 Validation loss: 0.0053411\n",
            "Epoch: 726 Train loss: 0.027615 Validation loss: 0.013689\n",
            "Epoch: 727 Train loss: 0.028982 Validation loss: 0.0053267\n",
            "Epoch: 728 Train loss: 0.027582 Validation loss: 0.013669\n",
            "Epoch: 729 Train loss: 0.02895 Validation loss: 0.0053128\n",
            "Epoch: 730 Train loss: 0.027549 Validation loss: 0.013649\n",
            "Epoch: 731 Train loss: 0.028918 Validation loss: 0.0052987\n",
            "Epoch: 732 Train loss: 0.027517 Validation loss: 0.01363\n",
            "Epoch: 733 Train loss: 0.028886 Validation loss: 0.0052847\n",
            "Epoch: 734 Train loss: 0.027485 Validation loss: 0.013611\n",
            "Epoch: 735 Train loss: 0.028855 Validation loss: 0.0052703\n",
            "Epoch: 736 Train loss: 0.027453 Validation loss: 0.013592\n",
            "Epoch: 737 Train loss: 0.028824 Validation loss: 0.0052561\n",
            "Epoch: 738 Train loss: 0.027422 Validation loss: 0.013573\n",
            "Epoch: 739 Train loss: 0.028794 Validation loss: 0.0052424\n",
            "Epoch: 740 Train loss: 0.02739 Validation loss: 0.013554\n",
            "Epoch: 741 Train loss: 0.028764 Validation loss: 0.0052282\n",
            "Epoch: 742 Train loss: 0.027359 Validation loss: 0.013535\n",
            "Epoch: 743 Train loss: 0.028734 Validation loss: 0.0052143\n",
            "Epoch: 744 Train loss: 0.027329 Validation loss: 0.013516\n",
            "Epoch: 745 Train loss: 0.028704 Validation loss: 0.0052003\n",
            "Epoch: 746 Train loss: 0.027298 Validation loss: 0.013498\n",
            "Epoch: 747 Train loss: 0.028675 Validation loss: 0.0051861\n",
            "Epoch: 748 Train loss: 0.027268 Validation loss: 0.013479\n",
            "Epoch: 749 Train loss: 0.028646 Validation loss: 0.0051722\n",
            "Epoch: 750 Train loss: 0.027238 Validation loss: 0.013461\n",
            "Epoch: 751 Train loss: 0.028617 Validation loss: 0.0051585\n",
            "Epoch: 752 Train loss: 0.027208 Validation loss: 0.013443\n",
            "Epoch: 753 Train loss: 0.028587 Validation loss: 0.0051449\n",
            "Epoch: 754 Train loss: 0.027178 Validation loss: 0.013424\n",
            "Epoch: 755 Train loss: 0.028558 Validation loss: 0.005131\n",
            "Epoch: 756 Train loss: 0.027148 Validation loss: 0.013406\n",
            "Epoch: 757 Train loss: 0.028529 Validation loss: 0.0051175\n",
            "Epoch: 758 Train loss: 0.027118 Validation loss: 0.013388\n",
            "Epoch: 759 Train loss: 0.028501 Validation loss: 0.0051037\n",
            "Epoch: 760 Train loss: 0.027088 Validation loss: 0.01337\n",
            "Epoch: 761 Train loss: 0.028472 Validation loss: 0.0050903\n",
            "Epoch: 762 Train loss: 0.027059 Validation loss: 0.013352\n",
            "Epoch: 763 Train loss: 0.028444 Validation loss: 0.0050767\n",
            "Epoch: 764 Train loss: 0.02703 Validation loss: 0.013335\n",
            "Epoch: 765 Train loss: 0.028416 Validation loss: 0.005063\n",
            "Epoch: 766 Train loss: 0.027001 Validation loss: 0.013317\n",
            "Epoch: 767 Train loss: 0.028388 Validation loss: 0.0050495\n",
            "Epoch: 768 Train loss: 0.026971 Validation loss: 0.013299\n",
            "Epoch: 769 Train loss: 0.028359 Validation loss: 0.0050361\n",
            "Epoch: 770 Train loss: 0.026942 Validation loss: 0.013281\n",
            "Epoch: 771 Train loss: 0.028331 Validation loss: 0.0050231\n",
            "Epoch: 772 Train loss: 0.026913 Validation loss: 0.013264\n",
            "Epoch: 773 Train loss: 0.028303 Validation loss: 0.0050098\n",
            "Epoch: 774 Train loss: 0.026884 Validation loss: 0.013246\n",
            "Epoch: 775 Train loss: 0.028275 Validation loss: 0.0049967\n",
            "Epoch: 776 Train loss: 0.026856 Validation loss: 0.013229\n",
            "Epoch: 777 Train loss: 0.028247 Validation loss: 0.0049834\n",
            "Epoch: 778 Train loss: 0.026827 Validation loss: 0.013211\n",
            "Epoch: 779 Train loss: 0.02822 Validation loss: 0.0049704\n",
            "Epoch: 780 Train loss: 0.026798 Validation loss: 0.013194\n",
            "Epoch: 781 Train loss: 0.028192 Validation loss: 0.0049577\n",
            "Epoch: 782 Train loss: 0.026769 Validation loss: 0.013177\n",
            "Epoch: 783 Train loss: 0.028163 Validation loss: 0.0049449\n",
            "Epoch: 784 Train loss: 0.02674 Validation loss: 0.013159\n",
            "Epoch: 785 Train loss: 0.028135 Validation loss: 0.004932\n",
            "Epoch: 786 Train loss: 0.026712 Validation loss: 0.013142\n",
            "Epoch: 787 Train loss: 0.028107 Validation loss: 0.0049193\n",
            "Epoch: 788 Train loss: 0.026683 Validation loss: 0.013125\n",
            "Epoch: 789 Train loss: 0.028079 Validation loss: 0.0049068\n",
            "Epoch: 790 Train loss: 0.026654 Validation loss: 0.013108\n",
            "Epoch: 791 Train loss: 0.02805 Validation loss: 0.0048944\n",
            "Epoch: 792 Train loss: 0.026625 Validation loss: 0.01309\n",
            "Epoch: 793 Train loss: 0.028022 Validation loss: 0.0048822\n",
            "Epoch: 794 Train loss: 0.026595 Validation loss: 0.013073\n",
            "Epoch: 795 Train loss: 0.027993 Validation loss: 0.0048701\n",
            "Epoch: 796 Train loss: 0.026566 Validation loss: 0.013055\n",
            "Epoch: 797 Train loss: 0.027964 Validation loss: 0.0048577\n",
            "Epoch: 798 Train loss: 0.026537 Validation loss: 0.013038\n",
            "Epoch: 799 Train loss: 0.027935 Validation loss: 0.0048458\n",
            "Epoch: 800 Train loss: 0.026507 Validation loss: 0.013021\n",
            "Epoch: 801 Train loss: 0.027905 Validation loss: 0.0048338\n",
            "Epoch: 802 Train loss: 0.026477 Validation loss: 0.013003\n",
            "Epoch: 803 Train loss: 0.027876 Validation loss: 0.0048219\n",
            "Epoch: 804 Train loss: 0.026448 Validation loss: 0.012986\n",
            "Epoch: 805 Train loss: 0.027846 Validation loss: 0.0048102\n",
            "Epoch: 806 Train loss: 0.026418 Validation loss: 0.012969\n",
            "Epoch: 807 Train loss: 0.027816 Validation loss: 0.0047984\n",
            "Epoch: 808 Train loss: 0.026388 Validation loss: 0.012952\n",
            "Epoch: 809 Train loss: 0.027787 Validation loss: 0.004787\n",
            "Epoch: 810 Train loss: 0.026358 Validation loss: 0.012934\n",
            "Epoch: 811 Train loss: 0.027756 Validation loss: 0.0047756\n",
            "Epoch: 812 Train loss: 0.026328 Validation loss: 0.012917\n",
            "Epoch: 813 Train loss: 0.027726 Validation loss: 0.0047644\n",
            "Epoch: 814 Train loss: 0.026298 Validation loss: 0.012899\n",
            "Epoch: 815 Train loss: 0.027695 Validation loss: 0.0047529\n",
            "Epoch: 816 Train loss: 0.026268 Validation loss: 0.012882\n",
            "Epoch: 817 Train loss: 0.027665 Validation loss: 0.0047419\n",
            "Epoch: 818 Train loss: 0.026237 Validation loss: 0.012865\n",
            "Epoch: 819 Train loss: 0.027634 Validation loss: 0.0047308\n",
            "Epoch: 820 Train loss: 0.026207 Validation loss: 0.012847\n",
            "Epoch: 821 Train loss: 0.027602 Validation loss: 0.0047201\n",
            "Epoch: 822 Train loss: 0.026176 Validation loss: 0.012829\n",
            "Epoch: 823 Train loss: 0.027571 Validation loss: 0.0047093\n",
            "Epoch: 824 Train loss: 0.026144 Validation loss: 0.012812\n",
            "Epoch: 825 Train loss: 0.027539 Validation loss: 0.0046987\n",
            "Epoch: 826 Train loss: 0.026113 Validation loss: 0.012795\n",
            "Epoch: 827 Train loss: 0.027506 Validation loss: 0.0046881\n",
            "Epoch: 828 Train loss: 0.026081 Validation loss: 0.012777\n",
            "Epoch: 829 Train loss: 0.027474 Validation loss: 0.0046778\n",
            "Epoch: 830 Train loss: 0.02605 Validation loss: 0.012759\n",
            "Epoch: 831 Train loss: 0.027441 Validation loss: 0.0046677\n",
            "Epoch: 832 Train loss: 0.026018 Validation loss: 0.012742\n",
            "Epoch: 833 Train loss: 0.027408 Validation loss: 0.0046573\n",
            "Epoch: 834 Train loss: 0.025986 Validation loss: 0.012724\n",
            "Epoch: 835 Train loss: 0.027374 Validation loss: 0.0046474\n",
            "Epoch: 836 Train loss: 0.025954 Validation loss: 0.012706\n",
            "Epoch: 837 Train loss: 0.027341 Validation loss: 0.0046374\n",
            "Epoch: 838 Train loss: 0.025922 Validation loss: 0.012689\n",
            "Epoch: 839 Train loss: 0.027308 Validation loss: 0.0046274\n",
            "Epoch: 840 Train loss: 0.025889 Validation loss: 0.012671\n",
            "Epoch: 841 Train loss: 0.027274 Validation loss: 0.0046177\n",
            "Epoch: 842 Train loss: 0.025857 Validation loss: 0.012653\n",
            "Epoch: 843 Train loss: 0.02724 Validation loss: 0.004608\n",
            "Epoch: 844 Train loss: 0.025824 Validation loss: 0.012635\n",
            "Epoch: 845 Train loss: 0.027205 Validation loss: 0.0045987\n",
            "Epoch: 846 Train loss: 0.025791 Validation loss: 0.012617\n",
            "Epoch: 847 Train loss: 0.02717 Validation loss: 0.0045894\n",
            "Epoch: 848 Train loss: 0.025757 Validation loss: 0.0126\n",
            "Epoch: 849 Train loss: 0.027135 Validation loss: 0.0045801\n",
            "Epoch: 850 Train loss: 0.025724 Validation loss: 0.012582\n",
            "Epoch: 851 Train loss: 0.027099 Validation loss: 0.0045709\n",
            "Epoch: 852 Train loss: 0.02569 Validation loss: 0.012564\n",
            "Epoch: 853 Train loss: 0.027063 Validation loss: 0.0045621\n",
            "Epoch: 854 Train loss: 0.025656 Validation loss: 0.012546\n",
            "Epoch: 855 Train loss: 0.027027 Validation loss: 0.0045532\n",
            "Epoch: 856 Train loss: 0.025622 Validation loss: 0.012528\n",
            "Epoch: 857 Train loss: 0.02699 Validation loss: 0.0045446\n",
            "Epoch: 858 Train loss: 0.025587 Validation loss: 0.01251\n",
            "Epoch: 859 Train loss: 0.026953 Validation loss: 0.0045357\n",
            "Epoch: 860 Train loss: 0.025553 Validation loss: 0.012491\n",
            "Epoch: 861 Train loss: 0.026916 Validation loss: 0.0045272\n",
            "Epoch: 862 Train loss: 0.025518 Validation loss: 0.012474\n",
            "Epoch: 863 Train loss: 0.026879 Validation loss: 0.0045186\n",
            "Epoch: 864 Train loss: 0.025483 Validation loss: 0.012456\n",
            "Epoch: 865 Train loss: 0.026842 Validation loss: 0.0045103\n",
            "Epoch: 866 Train loss: 0.025448 Validation loss: 0.012437\n",
            "Epoch: 867 Train loss: 0.026805 Validation loss: 0.0045021\n",
            "Epoch: 868 Train loss: 0.025413 Validation loss: 0.012419\n",
            "Epoch: 869 Train loss: 0.026767 Validation loss: 0.0044941\n",
            "Epoch: 870 Train loss: 0.025378 Validation loss: 0.012402\n",
            "Epoch: 871 Train loss: 0.026729 Validation loss: 0.0044859\n",
            "Epoch: 872 Train loss: 0.025343 Validation loss: 0.012383\n",
            "Epoch: 873 Train loss: 0.026691 Validation loss: 0.0044779\n",
            "Epoch: 874 Train loss: 0.025307 Validation loss: 0.012365\n",
            "Epoch: 875 Train loss: 0.026652 Validation loss: 0.0044701\n",
            "Epoch: 876 Train loss: 0.025271 Validation loss: 0.012347\n",
            "Epoch: 877 Train loss: 0.026613 Validation loss: 0.0044621\n",
            "Epoch: 878 Train loss: 0.025236 Validation loss: 0.012328\n",
            "Epoch: 879 Train loss: 0.026575 Validation loss: 0.0044543\n",
            "Epoch: 880 Train loss: 0.0252 Validation loss: 0.012311\n",
            "Epoch: 881 Train loss: 0.026536 Validation loss: 0.004447\n",
            "Epoch: 882 Train loss: 0.025163 Validation loss: 0.012292\n",
            "Epoch: 883 Train loss: 0.026496 Validation loss: 0.0044393\n",
            "Epoch: 884 Train loss: 0.025127 Validation loss: 0.012274\n",
            "Epoch: 885 Train loss: 0.026457 Validation loss: 0.0044317\n",
            "Epoch: 886 Train loss: 0.025091 Validation loss: 0.012255\n",
            "Epoch: 887 Train loss: 0.026417 Validation loss: 0.0044245\n",
            "Epoch: 888 Train loss: 0.025054 Validation loss: 0.012237\n",
            "Epoch: 889 Train loss: 0.026377 Validation loss: 0.0044178\n",
            "Epoch: 890 Train loss: 0.025018 Validation loss: 0.012219\n",
            "Epoch: 891 Train loss: 0.026337 Validation loss: 0.0044105\n",
            "Epoch: 892 Train loss: 0.024981 Validation loss: 0.012201\n",
            "Epoch: 893 Train loss: 0.026297 Validation loss: 0.0044034\n",
            "Epoch: 894 Train loss: 0.024944 Validation loss: 0.012183\n",
            "Epoch: 895 Train loss: 0.026256 Validation loss: 0.0043967\n",
            "Epoch: 896 Train loss: 0.024907 Validation loss: 0.012165\n",
            "Epoch: 897 Train loss: 0.026216 Validation loss: 0.0043899\n",
            "Epoch: 898 Train loss: 0.02487 Validation loss: 0.012147\n",
            "Epoch: 899 Train loss: 0.026175 Validation loss: 0.0043832\n",
            "Epoch: 900 Train loss: 0.024832 Validation loss: 0.012128\n",
            "Epoch: 901 Train loss: 0.026134 Validation loss: 0.0043768\n",
            "Epoch: 902 Train loss: 0.024795 Validation loss: 0.01211\n",
            "Epoch: 903 Train loss: 0.026093 Validation loss: 0.0043701\n",
            "Epoch: 904 Train loss: 0.024758 Validation loss: 0.012092\n",
            "Epoch: 905 Train loss: 0.026052 Validation loss: 0.0043635\n",
            "Epoch: 906 Train loss: 0.02472 Validation loss: 0.012074\n",
            "Epoch: 907 Train loss: 0.026011 Validation loss: 0.0043567\n",
            "Epoch: 908 Train loss: 0.024683 Validation loss: 0.012056\n",
            "Epoch: 909 Train loss: 0.02597 Validation loss: 0.0043508\n",
            "Epoch: 910 Train loss: 0.024645 Validation loss: 0.012038\n",
            "Epoch: 911 Train loss: 0.025928 Validation loss: 0.0043446\n",
            "Epoch: 912 Train loss: 0.024607 Validation loss: 0.01202\n",
            "Epoch: 913 Train loss: 0.025886 Validation loss: 0.0043385\n",
            "Epoch: 914 Train loss: 0.024569 Validation loss: 0.012002\n",
            "Epoch: 915 Train loss: 0.025844 Validation loss: 0.0043322\n",
            "Epoch: 916 Train loss: 0.024532 Validation loss: 0.011984\n",
            "Epoch: 917 Train loss: 0.025803 Validation loss: 0.0043262\n",
            "Epoch: 918 Train loss: 0.024494 Validation loss: 0.011965\n",
            "Epoch: 919 Train loss: 0.025761 Validation loss: 0.0043205\n",
            "Epoch: 920 Train loss: 0.024456 Validation loss: 0.011948\n",
            "Epoch: 921 Train loss: 0.025719 Validation loss: 0.0043144\n",
            "Epoch: 922 Train loss: 0.024418 Validation loss: 0.01193\n",
            "Epoch: 923 Train loss: 0.025677 Validation loss: 0.0043088\n",
            "Epoch: 924 Train loss: 0.02438 Validation loss: 0.011912\n",
            "Epoch: 925 Train loss: 0.025635 Validation loss: 0.0043029\n",
            "Epoch: 926 Train loss: 0.024342 Validation loss: 0.011893\n",
            "Epoch: 927 Train loss: 0.025592 Validation loss: 0.0042971\n",
            "Epoch: 928 Train loss: 0.024303 Validation loss: 0.011876\n",
            "Epoch: 929 Train loss: 0.02555 Validation loss: 0.0042918\n",
            "Epoch: 930 Train loss: 0.024265 Validation loss: 0.011858\n",
            "Epoch: 931 Train loss: 0.025508 Validation loss: 0.0042859\n",
            "Epoch: 932 Train loss: 0.024227 Validation loss: 0.01184\n",
            "Epoch: 933 Train loss: 0.025465 Validation loss: 0.0042807\n",
            "Epoch: 934 Train loss: 0.024188 Validation loss: 0.011822\n",
            "Epoch: 935 Train loss: 0.025422 Validation loss: 0.0042754\n",
            "Epoch: 936 Train loss: 0.02415 Validation loss: 0.011804\n",
            "Epoch: 937 Train loss: 0.02538 Validation loss: 0.0042701\n",
            "Epoch: 938 Train loss: 0.024112 Validation loss: 0.011787\n",
            "Epoch: 939 Train loss: 0.025337 Validation loss: 0.0042651\n",
            "Epoch: 940 Train loss: 0.024073 Validation loss: 0.011769\n",
            "Epoch: 941 Train loss: 0.025294 Validation loss: 0.0042601\n",
            "Epoch: 942 Train loss: 0.024034 Validation loss: 0.011751\n",
            "Epoch: 943 Train loss: 0.025251 Validation loss: 0.004255\n",
            "Epoch: 944 Train loss: 0.023996 Validation loss: 0.011733\n",
            "Epoch: 945 Train loss: 0.025209 Validation loss: 0.0042499\n",
            "Epoch: 946 Train loss: 0.023957 Validation loss: 0.011716\n",
            "Epoch: 947 Train loss: 0.025166 Validation loss: 0.0042448\n",
            "Epoch: 948 Train loss: 0.023919 Validation loss: 0.011698\n",
            "Epoch: 949 Train loss: 0.025123 Validation loss: 0.0042398\n",
            "Epoch: 950 Train loss: 0.023881 Validation loss: 0.01168\n",
            "Epoch: 951 Train loss: 0.02508 Validation loss: 0.0042351\n",
            "Epoch: 952 Train loss: 0.023842 Validation loss: 0.011663\n",
            "Epoch: 953 Train loss: 0.025038 Validation loss: 0.0042302\n",
            "Epoch: 954 Train loss: 0.023804 Validation loss: 0.011646\n",
            "Epoch: 955 Train loss: 0.024995 Validation loss: 0.0042254\n",
            "Epoch: 956 Train loss: 0.023765 Validation loss: 0.011628\n",
            "Epoch: 957 Train loss: 0.024952 Validation loss: 0.0042208\n",
            "Epoch: 958 Train loss: 0.023727 Validation loss: 0.011611\n",
            "Epoch: 959 Train loss: 0.024909 Validation loss: 0.0042161\n",
            "Epoch: 960 Train loss: 0.023688 Validation loss: 0.011594\n",
            "Epoch: 961 Train loss: 0.024866 Validation loss: 0.0042116\n",
            "Epoch: 962 Train loss: 0.02365 Validation loss: 0.011576\n",
            "Epoch: 963 Train loss: 0.024823 Validation loss: 0.0042072\n",
            "Epoch: 964 Train loss: 0.023611 Validation loss: 0.011559\n",
            "Epoch: 965 Train loss: 0.024781 Validation loss: 0.0042028\n",
            "Epoch: 966 Train loss: 0.023573 Validation loss: 0.011542\n",
            "Epoch: 967 Train loss: 0.024738 Validation loss: 0.0041983\n",
            "Epoch: 968 Train loss: 0.023535 Validation loss: 0.011524\n",
            "Epoch: 969 Train loss: 0.024695 Validation loss: 0.0041942\n",
            "Epoch: 970 Train loss: 0.023496 Validation loss: 0.011507\n",
            "Epoch: 971 Train loss: 0.024652 Validation loss: 0.0041898\n",
            "Epoch: 972 Train loss: 0.023458 Validation loss: 0.01149\n",
            "Epoch: 973 Train loss: 0.024609 Validation loss: 0.0041856\n",
            "Epoch: 974 Train loss: 0.023419 Validation loss: 0.011473\n",
            "Epoch: 975 Train loss: 0.024566 Validation loss: 0.0041814\n",
            "Epoch: 976 Train loss: 0.023381 Validation loss: 0.011456\n",
            "Epoch: 977 Train loss: 0.024523 Validation loss: 0.0041772\n",
            "Epoch: 978 Train loss: 0.023342 Validation loss: 0.011439\n",
            "Epoch: 979 Train loss: 0.024481 Validation loss: 0.0041733\n",
            "Epoch: 980 Train loss: 0.023304 Validation loss: 0.011423\n",
            "Epoch: 981 Train loss: 0.024438 Validation loss: 0.004169\n",
            "Epoch: 982 Train loss: 0.023266 Validation loss: 0.011406\n",
            "Epoch: 983 Train loss: 0.024395 Validation loss: 0.0041649\n",
            "Epoch: 984 Train loss: 0.023228 Validation loss: 0.011389\n",
            "Epoch: 985 Train loss: 0.024353 Validation loss: 0.0041612\n",
            "Epoch: 986 Train loss: 0.02319 Validation loss: 0.011372\n",
            "Epoch: 987 Train loss: 0.02431 Validation loss: 0.0041573\n",
            "Epoch: 988 Train loss: 0.023152 Validation loss: 0.011356\n",
            "Epoch: 989 Train loss: 0.024268 Validation loss: 0.0041535\n",
            "Epoch: 990 Train loss: 0.023114 Validation loss: 0.011339\n",
            "Epoch: 991 Train loss: 0.024225 Validation loss: 0.0041494\n",
            "Epoch: 992 Train loss: 0.023076 Validation loss: 0.011322\n",
            "Epoch: 993 Train loss: 0.024183 Validation loss: 0.0041458\n",
            "Epoch: 994 Train loss: 0.023038 Validation loss: 0.011306\n",
            "Epoch: 995 Train loss: 0.02414 Validation loss: 0.004142\n",
            "Epoch: 996 Train loss: 0.023 Validation loss: 0.01129\n",
            "Epoch: 997 Train loss: 0.024098 Validation loss: 0.0041386\n",
            "Epoch: 998 Train loss: 0.022962 Validation loss: 0.011273\n",
            "Epoch: 999 Train loss: 0.024055 Validation loss: 0.0041348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkaUDmA8Ps6w",
        "colab_type": "text"
      },
      "source": [
        "Predicting with this model shows overfitting. For recognizing overfitting a comparison of the validation and training loss is very useful. If the training loss decreases during training while the validation loss consistently increases, the model you are training is probably overfitting. Plotting the models prediction and the target also shows that there is a significant discrepancy between the target and the prediction of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Sq-9xhWPxI4",
        "colab_type": "code",
        "outputId": "fbe6a225-c89f-4ce7-db76-a424cf524f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "y_pred = big_mdl(x)# Predict on x with \"big_mdl\"\n",
        "plt.scatter(x_train_overfit, y_train_overfit)\n",
        "plt.plot(x, y_true)\n",
        "plt.plot(x, y_pred.numpy())\n",
        "plt.legend([\"Target\", \"Prediction\", \"Training samples\"])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBGKcv8FgMEQgxvkbDAZDBPJ/bnjcuDoHNscAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ36J2EN85b9",
        "colab_type": "text"
      },
      "source": [
        "In order to implement a regularization we need to modify the loss function. Since the loss function in this exercise is computed during the training step, we define a new training step with a regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLbcWwlt9Jwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" In order to avoid overfitting we implement a training step that also includes a regularization on the weights of our big model. For this we use the Frobenius/squared l2-norm of each weight matrix/vector. \n",
        "Hint: Use the tf.reduce_sum() function on a list of individual regularization terms for each matrix/vector of the network.\"\"\"\n",
        "\n",
        "def Cal_Reg_Loss(ModelParam, Norm):\n",
        "  sum = 0\n",
        "  for i in ModelParam:\n",
        "    sum += tf.reduce_sum(tf.square(i))\n",
        "  return sum\n",
        "  \n",
        "\n",
        "def regularized_train_step(model, optimizer, x, y, lmbd):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "        y_pred = model(x)# Compute a prediction with \"model\" on the input \"x\"\n",
        "        loss_val = tf.reduce_mean(tf.square(y-y_pred))# Compute the Mean Squared Error (MSE) for the prediction \"y_pred\" and the targets \"y\"\n",
        "        regul_val = Cal_Reg_Loss(model.trainable_variables,2)# Compute the regularization based on the list \"model.trainable_variables\"\n",
        "        total_loss = loss_val + (regul_val * lmbd) # Add the loss with a the regularization term weighted by \"lmbd\"\n",
        "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExK4FIw89s0M",
        "colab_type": "text"
      },
      "source": [
        "We can now set the strength of the regularization and retrain the big model with a regularization. We create another instance of the big model in order to compare the big model with and without regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PUUBGi496Vt",
        "colab_type": "code",
        "outputId": "94343e68-5d4d-4104-9453-3123492c4f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\" Implement the training for the bigger model with the regularized_train_step function. Note: We are plotting the MSE loss without the regularization in order to compare it with the unregularized model. \"\"\"\n",
        "\n",
        "lmbd = 0.005\n",
        "\n",
        "big_reg_mdl = MyBigModel()\n",
        "big_opt = tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "epoch = 0\n",
        "train_iters = 0\n",
        "train_loss = 0.0\n",
        "for x_t, y_t in train_overfit_ds:\n",
        "    train_loss += regularized_train_step(big_reg_mdl,big_opt,x_t,y_t,lmbd)# Perform a regularized training step with the model \"big_mdl\" and the optimizer \"big_opt\" on the inputs \"x_t\" and the corresponding targets \"y_t\" with the regularization parameter being \"lmbd\"\n",
        "    train_iters += 1\n",
        "    if(( N_train_samples_overfit / batch_size) == train_iters ):\n",
        "        for x_v, y_v in validation_ds:\n",
        "            y_pred = big_reg_mdl(x_v)# Compute a prediction with \"big_mdl\" on the input \"x_v\"\n",
        "            validation_loss =tf.reduce_mean(tf.square(y_v-y_pred)) # Compute the Mean Squared Error (MSE) for the prediction \"y_pred\" and the targets \"y_v\"\n",
        "        print(\"Epoch: {} Train loss: {:.5} Validation loss: {:.5}\".format(epoch, train_loss/train_iters, validation_loss))\n",
        "        train_iters = 0\n",
        "        train_loss = 0.0\n",
        "        train_reg = 0.0\n",
        "        epoch += 1\n",
        "    if (epoch == N_epochs):\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Train loss: 22.051 Validation loss: 5.1374\n",
            "Epoch: 1 Train loss: 13.141 Validation loss: 0.49444\n",
            "Epoch: 2 Train loss: 17.351 Validation loss: 5.1759\n",
            "Epoch: 3 Train loss: 18.742 Validation loss: 0.19472\n",
            "Epoch: 4 Train loss: 4.9109 Validation loss: 1.8674\n",
            "Epoch: 5 Train loss: 4.8436 Validation loss: 0.2585\n",
            "Epoch: 6 Train loss: 1.0792 Validation loss: 0.69893\n",
            "Epoch: 7 Train loss: 0.37226 Validation loss: 0.34723\n",
            "Epoch: 8 Train loss: 0.27159 Validation loss: 0.38142\n",
            "Epoch: 9 Train loss: 0.22903 Validation loss: 0.27265\n",
            "Epoch: 10 Train loss: 0.20497 Validation loss: 0.26271\n",
            "Epoch: 11 Train loss: 0.18867 Validation loss: 0.21312\n",
            "Epoch: 12 Train loss: 0.17608 Validation loss: 0.19554\n",
            "Epoch: 13 Train loss: 0.16569 Validation loss: 0.16816\n",
            "Epoch: 14 Train loss: 0.15673 Validation loss: 0.15236\n",
            "Epoch: 15 Train loss: 0.14881 Validation loss: 0.13511\n",
            "Epoch: 16 Train loss: 0.14166 Validation loss: 0.12266\n",
            "Epoch: 17 Train loss: 0.13513 Validation loss: 0.11084\n",
            "Epoch: 18 Train loss: 0.12908 Validation loss: 0.10137\n",
            "Epoch: 19 Train loss: 0.12343 Validation loss: 0.092843\n",
            "Epoch: 20 Train loss: 0.11811 Validation loss: 0.08567\n",
            "Epoch: 21 Train loss: 0.11307 Validation loss: 0.079322\n",
            "Epoch: 22 Train loss: 0.10828 Validation loss: 0.073845\n",
            "Epoch: 23 Train loss: 0.1037 Validation loss: 0.069014\n",
            "Epoch: 24 Train loss: 0.099318 Validation loss: 0.064788\n",
            "Epoch: 25 Train loss: 0.095103 Validation loss: 0.061052\n",
            "Epoch: 26 Train loss: 0.091042 Validation loss: 0.057754\n",
            "Epoch: 27 Train loss: 0.087124 Validation loss: 0.054827\n",
            "Epoch: 28 Train loss: 0.083335 Validation loss: 0.052224\n",
            "Epoch: 29 Train loss: 0.079666 Validation loss: 0.049905\n",
            "Epoch: 30 Train loss: 0.076108 Validation loss: 0.047833\n",
            "Epoch: 31 Train loss: 0.072657 Validation loss: 0.045978\n",
            "Epoch: 32 Train loss: 0.06931 Validation loss: 0.044312\n",
            "Epoch: 33 Train loss: 0.066065 Validation loss: 0.042812\n",
            "Epoch: 34 Train loss: 0.062926 Validation loss: 0.041458\n",
            "Epoch: 35 Train loss: 0.059897 Validation loss: 0.040232\n",
            "Epoch: 36 Train loss: 0.056987 Validation loss: 0.039117\n",
            "Epoch: 37 Train loss: 0.054205 Validation loss: 0.038098\n",
            "Epoch: 38 Train loss: 0.051561 Validation loss: 0.037162\n",
            "Epoch: 39 Train loss: 0.049063 Validation loss: 0.036298\n",
            "Epoch: 40 Train loss: 0.046719 Validation loss: 0.035496\n",
            "Epoch: 41 Train loss: 0.044535 Validation loss: 0.034746\n",
            "Epoch: 42 Train loss: 0.042512 Validation loss: 0.03404\n",
            "Epoch: 43 Train loss: 0.040649 Validation loss: 0.033373\n",
            "Epoch: 44 Train loss: 0.038942 Validation loss: 0.032739\n",
            "Epoch: 45 Train loss: 0.037384 Validation loss: 0.032136\n",
            "Epoch: 46 Train loss: 0.035967 Validation loss: 0.03156\n",
            "Epoch: 47 Train loss: 0.034682 Validation loss: 0.031008\n",
            "Epoch: 48 Train loss: 0.033518 Validation loss: 0.03048\n",
            "Epoch: 49 Train loss: 0.032466 Validation loss: 0.029974\n",
            "Epoch: 50 Train loss: 0.031514 Validation loss: 0.02949\n",
            "Epoch: 51 Train loss: 0.030654 Validation loss: 0.029027\n",
            "Epoch: 52 Train loss: 0.029877 Validation loss: 0.028583\n",
            "Epoch: 53 Train loss: 0.029173 Validation loss: 0.02816\n",
            "Epoch: 54 Train loss: 0.028537 Validation loss: 0.027756\n",
            "Epoch: 55 Train loss: 0.027959 Validation loss: 0.027371\n",
            "Epoch: 56 Train loss: 0.027436 Validation loss: 0.027004\n",
            "Epoch: 57 Train loss: 0.02696 Validation loss: 0.026655\n",
            "Epoch: 58 Train loss: 0.026526 Validation loss: 0.026323\n",
            "Epoch: 59 Train loss: 0.026131 Validation loss: 0.026007\n",
            "Epoch: 60 Train loss: 0.02577 Validation loss: 0.025707\n",
            "Epoch: 61 Train loss: 0.02544 Validation loss: 0.025422\n",
            "Epoch: 62 Train loss: 0.025137 Validation loss: 0.025152\n",
            "Epoch: 63 Train loss: 0.024858 Validation loss: 0.024894\n",
            "Epoch: 64 Train loss: 0.024601 Validation loss: 0.024651\n",
            "Epoch: 65 Train loss: 0.024364 Validation loss: 0.024419\n",
            "Epoch: 66 Train loss: 0.024144 Validation loss: 0.024199\n",
            "Epoch: 67 Train loss: 0.023941 Validation loss: 0.02399\n",
            "Epoch: 68 Train loss: 0.023751 Validation loss: 0.023791\n",
            "Epoch: 69 Train loss: 0.023574 Validation loss: 0.023602\n",
            "Epoch: 70 Train loss: 0.023409 Validation loss: 0.023422\n",
            "Epoch: 71 Train loss: 0.023254 Validation loss: 0.023251\n",
            "Epoch: 72 Train loss: 0.023108 Validation loss: 0.023088\n",
            "Epoch: 73 Train loss: 0.022971 Validation loss: 0.022931\n",
            "Epoch: 74 Train loss: 0.022842 Validation loss: 0.022783\n",
            "Epoch: 75 Train loss: 0.022719 Validation loss: 0.02264\n",
            "Epoch: 76 Train loss: 0.022603 Validation loss: 0.022504\n",
            "Epoch: 77 Train loss: 0.022493 Validation loss: 0.022374\n",
            "Epoch: 78 Train loss: 0.022388 Validation loss: 0.022249\n",
            "Epoch: 79 Train loss: 0.022287 Validation loss: 0.022129\n",
            "Epoch: 80 Train loss: 0.022191 Validation loss: 0.022014\n",
            "Epoch: 81 Train loss: 0.022099 Validation loss: 0.021903\n",
            "Epoch: 82 Train loss: 0.022011 Validation loss: 0.021796\n",
            "Epoch: 83 Train loss: 0.021926 Validation loss: 0.021692\n",
            "Epoch: 84 Train loss: 0.021845 Validation loss: 0.021593\n",
            "Epoch: 85 Train loss: 0.021766 Validation loss: 0.021497\n",
            "Epoch: 86 Train loss: 0.02169 Validation loss: 0.021404\n",
            "Epoch: 87 Train loss: 0.021616 Validation loss: 0.021313\n",
            "Epoch: 88 Train loss: 0.021545 Validation loss: 0.021225\n",
            "Epoch: 89 Train loss: 0.021476 Validation loss: 0.021141\n",
            "Epoch: 90 Train loss: 0.021409 Validation loss: 0.021058\n",
            "Epoch: 91 Train loss: 0.021344 Validation loss: 0.020978\n",
            "Epoch: 92 Train loss: 0.021281 Validation loss: 0.0209\n",
            "Epoch: 93 Train loss: 0.021219 Validation loss: 0.020823\n",
            "Epoch: 94 Train loss: 0.021159 Validation loss: 0.020749\n",
            "Epoch: 95 Train loss: 0.021101 Validation loss: 0.020676\n",
            "Epoch: 96 Train loss: 0.021044 Validation loss: 0.020605\n",
            "Epoch: 97 Train loss: 0.020988 Validation loss: 0.020536\n",
            "Epoch: 98 Train loss: 0.020933 Validation loss: 0.020468\n",
            "Epoch: 99 Train loss: 0.02088 Validation loss: 0.020401\n",
            "Epoch: 100 Train loss: 0.020828 Validation loss: 0.020336\n",
            "Epoch: 101 Train loss: 0.020777 Validation loss: 0.020272\n",
            "Epoch: 102 Train loss: 0.020727 Validation loss: 0.02021\n",
            "Epoch: 103 Train loss: 0.020678 Validation loss: 0.020148\n",
            "Epoch: 104 Train loss: 0.02063 Validation loss: 0.020088\n",
            "Epoch: 105 Train loss: 0.020583 Validation loss: 0.020029\n",
            "Epoch: 106 Train loss: 0.020537 Validation loss: 0.019971\n",
            "Epoch: 107 Train loss: 0.020491 Validation loss: 0.019914\n",
            "Epoch: 108 Train loss: 0.020447 Validation loss: 0.019857\n",
            "Epoch: 109 Train loss: 0.020403 Validation loss: 0.019802\n",
            "Epoch: 110 Train loss: 0.020361 Validation loss: 0.019747\n",
            "Epoch: 111 Train loss: 0.020318 Validation loss: 0.019693\n",
            "Epoch: 112 Train loss: 0.020277 Validation loss: 0.01964\n",
            "Epoch: 113 Train loss: 0.020236 Validation loss: 0.019588\n",
            "Epoch: 114 Train loss: 0.020196 Validation loss: 0.019537\n",
            "Epoch: 115 Train loss: 0.020157 Validation loss: 0.019486\n",
            "Epoch: 116 Train loss: 0.020119 Validation loss: 0.019436\n",
            "Epoch: 117 Train loss: 0.020081 Validation loss: 0.019387\n",
            "Epoch: 118 Train loss: 0.020043 Validation loss: 0.019338\n",
            "Epoch: 119 Train loss: 0.020007 Validation loss: 0.019291\n",
            "Epoch: 120 Train loss: 0.019971 Validation loss: 0.019243\n",
            "Epoch: 121 Train loss: 0.019935 Validation loss: 0.019197\n",
            "Epoch: 122 Train loss: 0.0199 Validation loss: 0.019151\n",
            "Epoch: 123 Train loss: 0.019866 Validation loss: 0.019106\n",
            "Epoch: 124 Train loss: 0.019832 Validation loss: 0.019061\n",
            "Epoch: 125 Train loss: 0.019799 Validation loss: 0.019017\n",
            "Epoch: 126 Train loss: 0.019766 Validation loss: 0.018973\n",
            "Epoch: 127 Train loss: 0.019734 Validation loss: 0.01893\n",
            "Epoch: 128 Train loss: 0.019702 Validation loss: 0.018888\n",
            "Epoch: 129 Train loss: 0.019671 Validation loss: 0.018846\n",
            "Epoch: 130 Train loss: 0.01964 Validation loss: 0.018805\n",
            "Epoch: 131 Train loss: 0.01961 Validation loss: 0.018764\n",
            "Epoch: 132 Train loss: 0.01958 Validation loss: 0.018723\n",
            "Epoch: 133 Train loss: 0.01955 Validation loss: 0.018684\n",
            "Epoch: 134 Train loss: 0.019522 Validation loss: 0.018644\n",
            "Epoch: 135 Train loss: 0.019493 Validation loss: 0.018605\n",
            "Epoch: 136 Train loss: 0.019465 Validation loss: 0.018568\n",
            "Epoch: 137 Train loss: 0.019438 Validation loss: 0.018529\n",
            "Epoch: 138 Train loss: 0.019411 Validation loss: 0.018492\n",
            "Epoch: 139 Train loss: 0.019384 Validation loss: 0.018455\n",
            "Epoch: 140 Train loss: 0.019358 Validation loss: 0.018418\n",
            "Epoch: 141 Train loss: 0.019332 Validation loss: 0.018383\n",
            "Epoch: 142 Train loss: 0.019307 Validation loss: 0.018347\n",
            "Epoch: 143 Train loss: 0.019281 Validation loss: 0.018312\n",
            "Epoch: 144 Train loss: 0.019257 Validation loss: 0.018277\n",
            "Epoch: 145 Train loss: 0.019233 Validation loss: 0.018243\n",
            "Epoch: 146 Train loss: 0.019209 Validation loss: 0.018209\n",
            "Epoch: 147 Train loss: 0.019185 Validation loss: 0.018176\n",
            "Epoch: 148 Train loss: 0.019162 Validation loss: 0.018143\n",
            "Epoch: 149 Train loss: 0.019139 Validation loss: 0.01811\n",
            "Epoch: 150 Train loss: 0.019117 Validation loss: 0.018078\n",
            "Epoch: 151 Train loss: 0.019095 Validation loss: 0.018046\n",
            "Epoch: 152 Train loss: 0.019073 Validation loss: 0.018015\n",
            "Epoch: 153 Train loss: 0.019052 Validation loss: 0.017984\n",
            "Epoch: 154 Train loss: 0.019031 Validation loss: 0.017953\n",
            "Epoch: 155 Train loss: 0.01901 Validation loss: 0.017923\n",
            "Epoch: 156 Train loss: 0.018989 Validation loss: 0.017893\n",
            "Epoch: 157 Train loss: 0.018969 Validation loss: 0.017863\n",
            "Epoch: 158 Train loss: 0.01895 Validation loss: 0.017834\n",
            "Epoch: 159 Train loss: 0.01893 Validation loss: 0.017806\n",
            "Epoch: 160 Train loss: 0.018911 Validation loss: 0.017777\n",
            "Epoch: 161 Train loss: 0.018892 Validation loss: 0.017749\n",
            "Epoch: 162 Train loss: 0.018874 Validation loss: 0.017721\n",
            "Epoch: 163 Train loss: 0.018855 Validation loss: 0.017694\n",
            "Epoch: 164 Train loss: 0.018837 Validation loss: 0.017667\n",
            "Epoch: 165 Train loss: 0.01882 Validation loss: 0.01764\n",
            "Epoch: 166 Train loss: 0.018802 Validation loss: 0.017614\n",
            "Epoch: 167 Train loss: 0.018785 Validation loss: 0.017588\n",
            "Epoch: 168 Train loss: 0.018768 Validation loss: 0.017562\n",
            "Epoch: 169 Train loss: 0.018751 Validation loss: 0.017537\n",
            "Epoch: 170 Train loss: 0.018735 Validation loss: 0.017512\n",
            "Epoch: 171 Train loss: 0.018719 Validation loss: 0.017487\n",
            "Epoch: 172 Train loss: 0.018703 Validation loss: 0.017463\n",
            "Epoch: 173 Train loss: 0.018687 Validation loss: 0.017438\n",
            "Epoch: 174 Train loss: 0.018672 Validation loss: 0.017414\n",
            "Epoch: 175 Train loss: 0.018657 Validation loss: 0.017391\n",
            "Epoch: 176 Train loss: 0.018642 Validation loss: 0.017368\n",
            "Epoch: 177 Train loss: 0.018627 Validation loss: 0.017345\n",
            "Epoch: 178 Train loss: 0.018613 Validation loss: 0.017322\n",
            "Epoch: 179 Train loss: 0.018599 Validation loss: 0.0173\n",
            "Epoch: 180 Train loss: 0.018585 Validation loss: 0.017278\n",
            "Epoch: 181 Train loss: 0.018571 Validation loss: 0.017256\n",
            "Epoch: 182 Train loss: 0.018557 Validation loss: 0.017234\n",
            "Epoch: 183 Train loss: 0.018544 Validation loss: 0.017213\n",
            "Epoch: 184 Train loss: 0.01853 Validation loss: 0.017192\n",
            "Epoch: 185 Train loss: 0.018517 Validation loss: 0.017171\n",
            "Epoch: 186 Train loss: 0.018505 Validation loss: 0.017151\n",
            "Epoch: 187 Train loss: 0.018492 Validation loss: 0.017131\n",
            "Epoch: 188 Train loss: 0.01848 Validation loss: 0.017111\n",
            "Epoch: 189 Train loss: 0.018467 Validation loss: 0.017091\n",
            "Epoch: 190 Train loss: 0.018455 Validation loss: 0.017072\n",
            "Epoch: 191 Train loss: 0.018443 Validation loss: 0.017053\n",
            "Epoch: 192 Train loss: 0.018432 Validation loss: 0.017033\n",
            "Epoch: 193 Train loss: 0.01842 Validation loss: 0.017015\n",
            "Epoch: 194 Train loss: 0.018409 Validation loss: 0.016996\n",
            "Epoch: 195 Train loss: 0.018397 Validation loss: 0.016978\n",
            "Epoch: 196 Train loss: 0.018386 Validation loss: 0.01696\n",
            "Epoch: 197 Train loss: 0.018375 Validation loss: 0.016942\n",
            "Epoch: 198 Train loss: 0.018365 Validation loss: 0.016925\n",
            "Epoch: 199 Train loss: 0.018354 Validation loss: 0.016908\n",
            "Epoch: 200 Train loss: 0.018344 Validation loss: 0.016891\n",
            "Epoch: 201 Train loss: 0.018333 Validation loss: 0.016874\n",
            "Epoch: 202 Train loss: 0.018323 Validation loss: 0.016857\n",
            "Epoch: 203 Train loss: 0.018313 Validation loss: 0.016841\n",
            "Epoch: 204 Train loss: 0.018303 Validation loss: 0.016825\n",
            "Epoch: 205 Train loss: 0.018293 Validation loss: 0.016809\n",
            "Epoch: 206 Train loss: 0.018284 Validation loss: 0.016793\n",
            "Epoch: 207 Train loss: 0.018274 Validation loss: 0.016778\n",
            "Epoch: 208 Train loss: 0.018265 Validation loss: 0.016762\n",
            "Epoch: 209 Train loss: 0.018256 Validation loss: 0.016747\n",
            "Epoch: 210 Train loss: 0.018247 Validation loss: 0.016732\n",
            "Epoch: 211 Train loss: 0.018238 Validation loss: 0.016718\n",
            "Epoch: 212 Train loss: 0.018229 Validation loss: 0.016703\n",
            "Epoch: 213 Train loss: 0.01822 Validation loss: 0.016689\n",
            "Epoch: 214 Train loss: 0.018211 Validation loss: 0.016675\n",
            "Epoch: 215 Train loss: 0.018203 Validation loss: 0.016661\n",
            "Epoch: 216 Train loss: 0.018194 Validation loss: 0.016647\n",
            "Epoch: 217 Train loss: 0.018186 Validation loss: 0.016634\n",
            "Epoch: 218 Train loss: 0.018178 Validation loss: 0.016621\n",
            "Epoch: 219 Train loss: 0.01817 Validation loss: 0.016608\n",
            "Epoch: 220 Train loss: 0.018162 Validation loss: 0.016595\n",
            "Epoch: 221 Train loss: 0.018154 Validation loss: 0.016582\n",
            "Epoch: 222 Train loss: 0.018146 Validation loss: 0.01657\n",
            "Epoch: 223 Train loss: 0.018138 Validation loss: 0.016558\n",
            "Epoch: 224 Train loss: 0.018131 Validation loss: 0.016545\n",
            "Epoch: 225 Train loss: 0.018123 Validation loss: 0.016533\n",
            "Epoch: 226 Train loss: 0.018116 Validation loss: 0.016521\n",
            "Epoch: 227 Train loss: 0.018108 Validation loss: 0.01651\n",
            "Epoch: 228 Train loss: 0.018101 Validation loss: 0.016498\n",
            "Epoch: 229 Train loss: 0.018094 Validation loss: 0.016487\n",
            "Epoch: 230 Train loss: 0.018087 Validation loss: 0.016476\n",
            "Epoch: 231 Train loss: 0.01808 Validation loss: 0.016465\n",
            "Epoch: 232 Train loss: 0.018073 Validation loss: 0.016454\n",
            "Epoch: 233 Train loss: 0.018066 Validation loss: 0.016443\n",
            "Epoch: 234 Train loss: 0.018059 Validation loss: 0.016433\n",
            "Epoch: 235 Train loss: 0.018053 Validation loss: 0.016422\n",
            "Epoch: 236 Train loss: 0.018046 Validation loss: 0.016412\n",
            "Epoch: 237 Train loss: 0.01804 Validation loss: 0.016402\n",
            "Epoch: 238 Train loss: 0.018033 Validation loss: 0.016392\n",
            "Epoch: 239 Train loss: 0.018027 Validation loss: 0.016383\n",
            "Epoch: 240 Train loss: 0.01802 Validation loss: 0.016373\n",
            "Epoch: 241 Train loss: 0.018014 Validation loss: 0.016364\n",
            "Epoch: 242 Train loss: 0.018008 Validation loss: 0.016354\n",
            "Epoch: 243 Train loss: 0.018002 Validation loss: 0.016345\n",
            "Epoch: 244 Train loss: 0.017996 Validation loss: 0.016336\n",
            "Epoch: 245 Train loss: 0.01799 Validation loss: 0.016327\n",
            "Epoch: 246 Train loss: 0.017984 Validation loss: 0.016319\n",
            "Epoch: 247 Train loss: 0.017978 Validation loss: 0.01631\n",
            "Epoch: 248 Train loss: 0.017972 Validation loss: 0.016302\n",
            "Epoch: 249 Train loss: 0.017966 Validation loss: 0.016293\n",
            "Epoch: 250 Train loss: 0.01796 Validation loss: 0.016286\n",
            "Epoch: 251 Train loss: 0.017955 Validation loss: 0.016277\n",
            "Epoch: 252 Train loss: 0.017949 Validation loss: 0.01627\n",
            "Epoch: 253 Train loss: 0.017944 Validation loss: 0.016262\n",
            "Epoch: 254 Train loss: 0.017938 Validation loss: 0.016254\n",
            "Epoch: 255 Train loss: 0.017933 Validation loss: 0.016246\n",
            "Epoch: 256 Train loss: 0.017927 Validation loss: 0.016239\n",
            "Epoch: 257 Train loss: 0.017922 Validation loss: 0.016232\n",
            "Epoch: 258 Train loss: 0.017917 Validation loss: 0.016225\n",
            "Epoch: 259 Train loss: 0.017911 Validation loss: 0.016218\n",
            "Epoch: 260 Train loss: 0.017906 Validation loss: 0.016211\n",
            "Epoch: 261 Train loss: 0.017901 Validation loss: 0.016204\n",
            "Epoch: 262 Train loss: 0.017896 Validation loss: 0.016198\n",
            "Epoch: 263 Train loss: 0.017891 Validation loss: 0.016191\n",
            "Epoch: 264 Train loss: 0.017886 Validation loss: 0.016185\n",
            "Epoch: 265 Train loss: 0.017881 Validation loss: 0.016178\n",
            "Epoch: 266 Train loss: 0.017876 Validation loss: 0.016172\n",
            "Epoch: 267 Train loss: 0.017871 Validation loss: 0.016166\n",
            "Epoch: 268 Train loss: 0.017866 Validation loss: 0.01616\n",
            "Epoch: 269 Train loss: 0.017861 Validation loss: 0.016154\n",
            "Epoch: 270 Train loss: 0.017856 Validation loss: 0.016149\n",
            "Epoch: 271 Train loss: 0.017852 Validation loss: 0.016143\n",
            "Epoch: 272 Train loss: 0.017847 Validation loss: 0.016137\n",
            "Epoch: 273 Train loss: 0.017842 Validation loss: 0.016132\n",
            "Epoch: 274 Train loss: 0.017838 Validation loss: 0.016126\n",
            "Epoch: 275 Train loss: 0.017833 Validation loss: 0.016121\n",
            "Epoch: 276 Train loss: 0.017829 Validation loss: 0.016116\n",
            "Epoch: 277 Train loss: 0.017824 Validation loss: 0.016111\n",
            "Epoch: 278 Train loss: 0.01782 Validation loss: 0.016106\n",
            "Epoch: 279 Train loss: 0.017815 Validation loss: 0.016101\n",
            "Epoch: 280 Train loss: 0.017811 Validation loss: 0.016096\n",
            "Epoch: 281 Train loss: 0.017806 Validation loss: 0.016091\n",
            "Epoch: 282 Train loss: 0.017802 Validation loss: 0.016087\n",
            "Epoch: 283 Train loss: 0.017798 Validation loss: 0.016083\n",
            "Epoch: 284 Train loss: 0.017793 Validation loss: 0.016078\n",
            "Epoch: 285 Train loss: 0.017789 Validation loss: 0.016074\n",
            "Epoch: 286 Train loss: 0.017785 Validation loss: 0.01607\n",
            "Epoch: 287 Train loss: 0.01778 Validation loss: 0.016066\n",
            "Epoch: 288 Train loss: 0.017776 Validation loss: 0.016061\n",
            "Epoch: 289 Train loss: 0.017772 Validation loss: 0.016057\n",
            "Epoch: 290 Train loss: 0.017768 Validation loss: 0.016054\n",
            "Epoch: 291 Train loss: 0.017764 Validation loss: 0.01605\n",
            "Epoch: 292 Train loss: 0.01776 Validation loss: 0.016046\n",
            "Epoch: 293 Train loss: 0.017756 Validation loss: 0.016042\n",
            "Epoch: 294 Train loss: 0.017752 Validation loss: 0.016039\n",
            "Epoch: 295 Train loss: 0.017748 Validation loss: 0.016036\n",
            "Epoch: 296 Train loss: 0.017744 Validation loss: 0.016032\n",
            "Epoch: 297 Train loss: 0.01774 Validation loss: 0.016029\n",
            "Epoch: 298 Train loss: 0.017736 Validation loss: 0.016025\n",
            "Epoch: 299 Train loss: 0.017732 Validation loss: 0.016022\n",
            "Epoch: 300 Train loss: 0.017728 Validation loss: 0.016019\n",
            "Epoch: 301 Train loss: 0.017724 Validation loss: 0.016016\n",
            "Epoch: 302 Train loss: 0.01772 Validation loss: 0.016013\n",
            "Epoch: 303 Train loss: 0.017716 Validation loss: 0.01601\n",
            "Epoch: 304 Train loss: 0.017713 Validation loss: 0.016008\n",
            "Epoch: 305 Train loss: 0.017709 Validation loss: 0.016005\n",
            "Epoch: 306 Train loss: 0.017705 Validation loss: 0.016003\n",
            "Epoch: 307 Train loss: 0.017701 Validation loss: 0.016\n",
            "Epoch: 308 Train loss: 0.017697 Validation loss: 0.015997\n",
            "Epoch: 309 Train loss: 0.017694 Validation loss: 0.015995\n",
            "Epoch: 310 Train loss: 0.01769 Validation loss: 0.015992\n",
            "Epoch: 311 Train loss: 0.017686 Validation loss: 0.01599\n",
            "Epoch: 312 Train loss: 0.017683 Validation loss: 0.015988\n",
            "Epoch: 313 Train loss: 0.017679 Validation loss: 0.015985\n",
            "Epoch: 314 Train loss: 0.017675 Validation loss: 0.015983\n",
            "Epoch: 315 Train loss: 0.017672 Validation loss: 0.015981\n",
            "Epoch: 316 Train loss: 0.017668 Validation loss: 0.015979\n",
            "Epoch: 317 Train loss: 0.017665 Validation loss: 0.015977\n",
            "Epoch: 318 Train loss: 0.017661 Validation loss: 0.015975\n",
            "Epoch: 319 Train loss: 0.017658 Validation loss: 0.015974\n",
            "Epoch: 320 Train loss: 0.017654 Validation loss: 0.015972\n",
            "Epoch: 321 Train loss: 0.017651 Validation loss: 0.01597\n",
            "Epoch: 322 Train loss: 0.017647 Validation loss: 0.015968\n",
            "Epoch: 323 Train loss: 0.017644 Validation loss: 0.015966\n",
            "Epoch: 324 Train loss: 0.01764 Validation loss: 0.015965\n",
            "Epoch: 325 Train loss: 0.017637 Validation loss: 0.015963\n",
            "Epoch: 326 Train loss: 0.017633 Validation loss: 0.015962\n",
            "Epoch: 327 Train loss: 0.01763 Validation loss: 0.01596\n",
            "Epoch: 328 Train loss: 0.017626 Validation loss: 0.015959\n",
            "Epoch: 329 Train loss: 0.017623 Validation loss: 0.015958\n",
            "Epoch: 330 Train loss: 0.01762 Validation loss: 0.015956\n",
            "Epoch: 331 Train loss: 0.017616 Validation loss: 0.015955\n",
            "Epoch: 332 Train loss: 0.017613 Validation loss: 0.015954\n",
            "Epoch: 333 Train loss: 0.017609 Validation loss: 0.015952\n",
            "Epoch: 334 Train loss: 0.017606 Validation loss: 0.015952\n",
            "Epoch: 335 Train loss: 0.017603 Validation loss: 0.01595\n",
            "Epoch: 336 Train loss: 0.0176 Validation loss: 0.01595\n",
            "Epoch: 337 Train loss: 0.017596 Validation loss: 0.015949\n",
            "Epoch: 338 Train loss: 0.017593 Validation loss: 0.015948\n",
            "Epoch: 339 Train loss: 0.01759 Validation loss: 0.015947\n",
            "Epoch: 340 Train loss: 0.017586 Validation loss: 0.015946\n",
            "Epoch: 341 Train loss: 0.017583 Validation loss: 0.015945\n",
            "Epoch: 342 Train loss: 0.01758 Validation loss: 0.015945\n",
            "Epoch: 343 Train loss: 0.017577 Validation loss: 0.015944\n",
            "Epoch: 344 Train loss: 0.017574 Validation loss: 0.015943\n",
            "Epoch: 345 Train loss: 0.01757 Validation loss: 0.015943\n",
            "Epoch: 346 Train loss: 0.017567 Validation loss: 0.015942\n",
            "Epoch: 347 Train loss: 0.017564 Validation loss: 0.015941\n",
            "Epoch: 348 Train loss: 0.017561 Validation loss: 0.015941\n",
            "Epoch: 349 Train loss: 0.017558 Validation loss: 0.01594\n",
            "Epoch: 350 Train loss: 0.017554 Validation loss: 0.01594\n",
            "Epoch: 351 Train loss: 0.017551 Validation loss: 0.01594\n",
            "Epoch: 352 Train loss: 0.017548 Validation loss: 0.01594\n",
            "Epoch: 353 Train loss: 0.017545 Validation loss: 0.015939\n",
            "Epoch: 354 Train loss: 0.017542 Validation loss: 0.015939\n",
            "Epoch: 355 Train loss: 0.017539 Validation loss: 0.015939\n",
            "Epoch: 356 Train loss: 0.017536 Validation loss: 0.015938\n",
            "Epoch: 357 Train loss: 0.017533 Validation loss: 0.015938\n",
            "Epoch: 358 Train loss: 0.01753 Validation loss: 0.015938\n",
            "Epoch: 359 Train loss: 0.017526 Validation loss: 0.015938\n",
            "Epoch: 360 Train loss: 0.017523 Validation loss: 0.015938\n",
            "Epoch: 361 Train loss: 0.01752 Validation loss: 0.015938\n",
            "Epoch: 362 Train loss: 0.017517 Validation loss: 0.015938\n",
            "Epoch: 363 Train loss: 0.017514 Validation loss: 0.015938\n",
            "Epoch: 364 Train loss: 0.017511 Validation loss: 0.015938\n",
            "Epoch: 365 Train loss: 0.017508 Validation loss: 0.015938\n",
            "Epoch: 366 Train loss: 0.017505 Validation loss: 0.015938\n",
            "Epoch: 367 Train loss: 0.017502 Validation loss: 0.015939\n",
            "Epoch: 368 Train loss: 0.017499 Validation loss: 0.015938\n",
            "Epoch: 369 Train loss: 0.017496 Validation loss: 0.015938\n",
            "Epoch: 370 Train loss: 0.017493 Validation loss: 0.015939\n",
            "Epoch: 371 Train loss: 0.01749 Validation loss: 0.015939\n",
            "Epoch: 372 Train loss: 0.017487 Validation loss: 0.015939\n",
            "Epoch: 373 Train loss: 0.017484 Validation loss: 0.015939\n",
            "Epoch: 374 Train loss: 0.017481 Validation loss: 0.01594\n",
            "Epoch: 375 Train loss: 0.017478 Validation loss: 0.01594\n",
            "Epoch: 376 Train loss: 0.017475 Validation loss: 0.01594\n",
            "Epoch: 377 Train loss: 0.017473 Validation loss: 0.015941\n",
            "Epoch: 378 Train loss: 0.01747 Validation loss: 0.015942\n",
            "Epoch: 379 Train loss: 0.017467 Validation loss: 0.015942\n",
            "Epoch: 380 Train loss: 0.017464 Validation loss: 0.015942\n",
            "Epoch: 381 Train loss: 0.017461 Validation loss: 0.015943\n",
            "Epoch: 382 Train loss: 0.017458 Validation loss: 0.015943\n",
            "Epoch: 383 Train loss: 0.017455 Validation loss: 0.015944\n",
            "Epoch: 384 Train loss: 0.017452 Validation loss: 0.015945\n",
            "Epoch: 385 Train loss: 0.017449 Validation loss: 0.015945\n",
            "Epoch: 386 Train loss: 0.017446 Validation loss: 0.015946\n",
            "Epoch: 387 Train loss: 0.017444 Validation loss: 0.015946\n",
            "Epoch: 388 Train loss: 0.017441 Validation loss: 0.015947\n",
            "Epoch: 389 Train loss: 0.017438 Validation loss: 0.015948\n",
            "Epoch: 390 Train loss: 0.017435 Validation loss: 0.015949\n",
            "Epoch: 391 Train loss: 0.017432 Validation loss: 0.015949\n",
            "Epoch: 392 Train loss: 0.017429 Validation loss: 0.01595\n",
            "Epoch: 393 Train loss: 0.017426 Validation loss: 0.015951\n",
            "Epoch: 394 Train loss: 0.017424 Validation loss: 0.015952\n",
            "Epoch: 395 Train loss: 0.017421 Validation loss: 0.015953\n",
            "Epoch: 396 Train loss: 0.017418 Validation loss: 0.015954\n",
            "Epoch: 397 Train loss: 0.017415 Validation loss: 0.015954\n",
            "Epoch: 398 Train loss: 0.017412 Validation loss: 0.015955\n",
            "Epoch: 399 Train loss: 0.01741 Validation loss: 0.015956\n",
            "Epoch: 400 Train loss: 0.017407 Validation loss: 0.015957\n",
            "Epoch: 401 Train loss: 0.017404 Validation loss: 0.015958\n",
            "Epoch: 402 Train loss: 0.017401 Validation loss: 0.015959\n",
            "Epoch: 403 Train loss: 0.017399 Validation loss: 0.01596\n",
            "Epoch: 404 Train loss: 0.017396 Validation loss: 0.015961\n",
            "Epoch: 405 Train loss: 0.017393 Validation loss: 0.015962\n",
            "Epoch: 406 Train loss: 0.01739 Validation loss: 0.015963\n",
            "Epoch: 407 Train loss: 0.017387 Validation loss: 0.015964\n",
            "Epoch: 408 Train loss: 0.017385 Validation loss: 0.015965\n",
            "Epoch: 409 Train loss: 0.017382 Validation loss: 0.015966\n",
            "Epoch: 410 Train loss: 0.017379 Validation loss: 0.015967\n",
            "Epoch: 411 Train loss: 0.017377 Validation loss: 0.015968\n",
            "Epoch: 412 Train loss: 0.017374 Validation loss: 0.015969\n",
            "Epoch: 413 Train loss: 0.017371 Validation loss: 0.01597\n",
            "Epoch: 414 Train loss: 0.017368 Validation loss: 0.015971\n",
            "Epoch: 415 Train loss: 0.017366 Validation loss: 0.015972\n",
            "Epoch: 416 Train loss: 0.017363 Validation loss: 0.015974\n",
            "Epoch: 417 Train loss: 0.01736 Validation loss: 0.015975\n",
            "Epoch: 418 Train loss: 0.017358 Validation loss: 0.015976\n",
            "Epoch: 419 Train loss: 0.017355 Validation loss: 0.015977\n",
            "Epoch: 420 Train loss: 0.017352 Validation loss: 0.015978\n",
            "Epoch: 421 Train loss: 0.017349 Validation loss: 0.015979\n",
            "Epoch: 422 Train loss: 0.017347 Validation loss: 0.015981\n",
            "Epoch: 423 Train loss: 0.017344 Validation loss: 0.015982\n",
            "Epoch: 424 Train loss: 0.017341 Validation loss: 0.015983\n",
            "Epoch: 425 Train loss: 0.017339 Validation loss: 0.015984\n",
            "Epoch: 426 Train loss: 0.017336 Validation loss: 0.015986\n",
            "Epoch: 427 Train loss: 0.017333 Validation loss: 0.015987\n",
            "Epoch: 428 Train loss: 0.017331 Validation loss: 0.015989\n",
            "Epoch: 429 Train loss: 0.017328 Validation loss: 0.01599\n",
            "Epoch: 430 Train loss: 0.017325 Validation loss: 0.015991\n",
            "Epoch: 431 Train loss: 0.017323 Validation loss: 0.015993\n",
            "Epoch: 432 Train loss: 0.01732 Validation loss: 0.015994\n",
            "Epoch: 433 Train loss: 0.017318 Validation loss: 0.015996\n",
            "Epoch: 434 Train loss: 0.017315 Validation loss: 0.015997\n",
            "Epoch: 435 Train loss: 0.017312 Validation loss: 0.015998\n",
            "Epoch: 436 Train loss: 0.01731 Validation loss: 0.015999\n",
            "Epoch: 437 Train loss: 0.017307 Validation loss: 0.016001\n",
            "Epoch: 438 Train loss: 0.017304 Validation loss: 0.016002\n",
            "Epoch: 439 Train loss: 0.017302 Validation loss: 0.016004\n",
            "Epoch: 440 Train loss: 0.017299 Validation loss: 0.016005\n",
            "Epoch: 441 Train loss: 0.017297 Validation loss: 0.016006\n",
            "Epoch: 442 Train loss: 0.017294 Validation loss: 0.016008\n",
            "Epoch: 443 Train loss: 0.017291 Validation loss: 0.016009\n",
            "Epoch: 444 Train loss: 0.017289 Validation loss: 0.016011\n",
            "Epoch: 445 Train loss: 0.017286 Validation loss: 0.016012\n",
            "Epoch: 446 Train loss: 0.017284 Validation loss: 0.016014\n",
            "Epoch: 447 Train loss: 0.017281 Validation loss: 0.016016\n",
            "Epoch: 448 Train loss: 0.017278 Validation loss: 0.016017\n",
            "Epoch: 449 Train loss: 0.017276 Validation loss: 0.016019\n",
            "Epoch: 450 Train loss: 0.017273 Validation loss: 0.01602\n",
            "Epoch: 451 Train loss: 0.017271 Validation loss: 0.016022\n",
            "Epoch: 452 Train loss: 0.017268 Validation loss: 0.016023\n",
            "Epoch: 453 Train loss: 0.017266 Validation loss: 0.016025\n",
            "Epoch: 454 Train loss: 0.017263 Validation loss: 0.016026\n",
            "Epoch: 455 Train loss: 0.017261 Validation loss: 0.016028\n",
            "Epoch: 456 Train loss: 0.017258 Validation loss: 0.016029\n",
            "Epoch: 457 Train loss: 0.017256 Validation loss: 0.016031\n",
            "Epoch: 458 Train loss: 0.017253 Validation loss: 0.016033\n",
            "Epoch: 459 Train loss: 0.01725 Validation loss: 0.016034\n",
            "Epoch: 460 Train loss: 0.017248 Validation loss: 0.016036\n",
            "Epoch: 461 Train loss: 0.017245 Validation loss: 0.016038\n",
            "Epoch: 462 Train loss: 0.017243 Validation loss: 0.016039\n",
            "Epoch: 463 Train loss: 0.01724 Validation loss: 0.016041\n",
            "Epoch: 464 Train loss: 0.017238 Validation loss: 0.016042\n",
            "Epoch: 465 Train loss: 0.017235 Validation loss: 0.016044\n",
            "Epoch: 466 Train loss: 0.017233 Validation loss: 0.016046\n",
            "Epoch: 467 Train loss: 0.01723 Validation loss: 0.016047\n",
            "Epoch: 468 Train loss: 0.017228 Validation loss: 0.016049\n",
            "Epoch: 469 Train loss: 0.017225 Validation loss: 0.01605\n",
            "Epoch: 470 Train loss: 0.017223 Validation loss: 0.016052\n",
            "Epoch: 471 Train loss: 0.01722 Validation loss: 0.016054\n",
            "Epoch: 472 Train loss: 0.017218 Validation loss: 0.016055\n",
            "Epoch: 473 Train loss: 0.017215 Validation loss: 0.016057\n",
            "Epoch: 474 Train loss: 0.017213 Validation loss: 0.016059\n",
            "Epoch: 475 Train loss: 0.01721 Validation loss: 0.016061\n",
            "Epoch: 476 Train loss: 0.017208 Validation loss: 0.016062\n",
            "Epoch: 477 Train loss: 0.017205 Validation loss: 0.016064\n",
            "Epoch: 478 Train loss: 0.017203 Validation loss: 0.016066\n",
            "Epoch: 479 Train loss: 0.0172 Validation loss: 0.016067\n",
            "Epoch: 480 Train loss: 0.017198 Validation loss: 0.016069\n",
            "Epoch: 481 Train loss: 0.017195 Validation loss: 0.016071\n",
            "Epoch: 482 Train loss: 0.017193 Validation loss: 0.016072\n",
            "Epoch: 483 Train loss: 0.017191 Validation loss: 0.016074\n",
            "Epoch: 484 Train loss: 0.017188 Validation loss: 0.016076\n",
            "Epoch: 485 Train loss: 0.017186 Validation loss: 0.016078\n",
            "Epoch: 486 Train loss: 0.017183 Validation loss: 0.016079\n",
            "Epoch: 487 Train loss: 0.017181 Validation loss: 0.016081\n",
            "Epoch: 488 Train loss: 0.017178 Validation loss: 0.016083\n",
            "Epoch: 489 Train loss: 0.017176 Validation loss: 0.016084\n",
            "Epoch: 490 Train loss: 0.017173 Validation loss: 0.016086\n",
            "Epoch: 491 Train loss: 0.017171 Validation loss: 0.016088\n",
            "Epoch: 492 Train loss: 0.017169 Validation loss: 0.01609\n",
            "Epoch: 493 Train loss: 0.017166 Validation loss: 0.016092\n",
            "Epoch: 494 Train loss: 0.017164 Validation loss: 0.016094\n",
            "Epoch: 495 Train loss: 0.017161 Validation loss: 0.016095\n",
            "Epoch: 496 Train loss: 0.017159 Validation loss: 0.016097\n",
            "Epoch: 497 Train loss: 0.017156 Validation loss: 0.016099\n",
            "Epoch: 498 Train loss: 0.017154 Validation loss: 0.016101\n",
            "Epoch: 499 Train loss: 0.017152 Validation loss: 0.016102\n",
            "Epoch: 500 Train loss: 0.017149 Validation loss: 0.016104\n",
            "Epoch: 501 Train loss: 0.017147 Validation loss: 0.016106\n",
            "Epoch: 502 Train loss: 0.017144 Validation loss: 0.016108\n",
            "Epoch: 503 Train loss: 0.017142 Validation loss: 0.01611\n",
            "Epoch: 504 Train loss: 0.01714 Validation loss: 0.016111\n",
            "Epoch: 505 Train loss: 0.017137 Validation loss: 0.016113\n",
            "Epoch: 506 Train loss: 0.017135 Validation loss: 0.016115\n",
            "Epoch: 507 Train loss: 0.017132 Validation loss: 0.016117\n",
            "Epoch: 508 Train loss: 0.01713 Validation loss: 0.016119\n",
            "Epoch: 509 Train loss: 0.017128 Validation loss: 0.016121\n",
            "Epoch: 510 Train loss: 0.017125 Validation loss: 0.016122\n",
            "Epoch: 511 Train loss: 0.017123 Validation loss: 0.016124\n",
            "Epoch: 512 Train loss: 0.017121 Validation loss: 0.016126\n",
            "Epoch: 513 Train loss: 0.017118 Validation loss: 0.016128\n",
            "Epoch: 514 Train loss: 0.017116 Validation loss: 0.01613\n",
            "Epoch: 515 Train loss: 0.017113 Validation loss: 0.016131\n",
            "Epoch: 516 Train loss: 0.017111 Validation loss: 0.016133\n",
            "Epoch: 517 Train loss: 0.017109 Validation loss: 0.016135\n",
            "Epoch: 518 Train loss: 0.017106 Validation loss: 0.016137\n",
            "Epoch: 519 Train loss: 0.017104 Validation loss: 0.016139\n",
            "Epoch: 520 Train loss: 0.017102 Validation loss: 0.016141\n",
            "Epoch: 521 Train loss: 0.017099 Validation loss: 0.016142\n",
            "Epoch: 522 Train loss: 0.017097 Validation loss: 0.016144\n",
            "Epoch: 523 Train loss: 0.017095 Validation loss: 0.016146\n",
            "Epoch: 524 Train loss: 0.017092 Validation loss: 0.016148\n",
            "Epoch: 525 Train loss: 0.01709 Validation loss: 0.01615\n",
            "Epoch: 526 Train loss: 0.017088 Validation loss: 0.016151\n",
            "Epoch: 527 Train loss: 0.017085 Validation loss: 0.016153\n",
            "Epoch: 528 Train loss: 0.017083 Validation loss: 0.016155\n",
            "Epoch: 529 Train loss: 0.017081 Validation loss: 0.016157\n",
            "Epoch: 530 Train loss: 0.017078 Validation loss: 0.016159\n",
            "Epoch: 531 Train loss: 0.017076 Validation loss: 0.016161\n",
            "Epoch: 532 Train loss: 0.017074 Validation loss: 0.016162\n",
            "Epoch: 533 Train loss: 0.017071 Validation loss: 0.016164\n",
            "Epoch: 534 Train loss: 0.017069 Validation loss: 0.016166\n",
            "Epoch: 535 Train loss: 0.017067 Validation loss: 0.016168\n",
            "Epoch: 536 Train loss: 0.017064 Validation loss: 0.01617\n",
            "Epoch: 537 Train loss: 0.017062 Validation loss: 0.016172\n",
            "Epoch: 538 Train loss: 0.01706 Validation loss: 0.016174\n",
            "Epoch: 539 Train loss: 0.017057 Validation loss: 0.016175\n",
            "Epoch: 540 Train loss: 0.017055 Validation loss: 0.016177\n",
            "Epoch: 541 Train loss: 0.017053 Validation loss: 0.016179\n",
            "Epoch: 542 Train loss: 0.017051 Validation loss: 0.016181\n",
            "Epoch: 543 Train loss: 0.017048 Validation loss: 0.016183\n",
            "Epoch: 544 Train loss: 0.017046 Validation loss: 0.016185\n",
            "Epoch: 545 Train loss: 0.017044 Validation loss: 0.016187\n",
            "Epoch: 546 Train loss: 0.017041 Validation loss: 0.016189\n",
            "Epoch: 547 Train loss: 0.017039 Validation loss: 0.01619\n",
            "Epoch: 548 Train loss: 0.017037 Validation loss: 0.016192\n",
            "Epoch: 549 Train loss: 0.017035 Validation loss: 0.016194\n",
            "Epoch: 550 Train loss: 0.017032 Validation loss: 0.016196\n",
            "Epoch: 551 Train loss: 0.01703 Validation loss: 0.016198\n",
            "Epoch: 552 Train loss: 0.017028 Validation loss: 0.0162\n",
            "Epoch: 553 Train loss: 0.017025 Validation loss: 0.016202\n",
            "Epoch: 554 Train loss: 0.017023 Validation loss: 0.016203\n",
            "Epoch: 555 Train loss: 0.017021 Validation loss: 0.016205\n",
            "Epoch: 556 Train loss: 0.017019 Validation loss: 0.016207\n",
            "Epoch: 557 Train loss: 0.017016 Validation loss: 0.016209\n",
            "Epoch: 558 Train loss: 0.017014 Validation loss: 0.016211\n",
            "Epoch: 559 Train loss: 0.017012 Validation loss: 0.016213\n",
            "Epoch: 560 Train loss: 0.01701 Validation loss: 0.016214\n",
            "Epoch: 561 Train loss: 0.017007 Validation loss: 0.016216\n",
            "Epoch: 562 Train loss: 0.017005 Validation loss: 0.016218\n",
            "Epoch: 563 Train loss: 0.017003 Validation loss: 0.01622\n",
            "Epoch: 564 Train loss: 0.017001 Validation loss: 0.016222\n",
            "Epoch: 565 Train loss: 0.016998 Validation loss: 0.016224\n",
            "Epoch: 566 Train loss: 0.016996 Validation loss: 0.016226\n",
            "Epoch: 567 Train loss: 0.016994 Validation loss: 0.016228\n",
            "Epoch: 568 Train loss: 0.016992 Validation loss: 0.01623\n",
            "Epoch: 569 Train loss: 0.016989 Validation loss: 0.016232\n",
            "Epoch: 570 Train loss: 0.016987 Validation loss: 0.016233\n",
            "Epoch: 571 Train loss: 0.016985 Validation loss: 0.016235\n",
            "Epoch: 572 Train loss: 0.016983 Validation loss: 0.016237\n",
            "Epoch: 573 Train loss: 0.01698 Validation loss: 0.016239\n",
            "Epoch: 574 Train loss: 0.016978 Validation loss: 0.016241\n",
            "Epoch: 575 Train loss: 0.016976 Validation loss: 0.016242\n",
            "Epoch: 576 Train loss: 0.016974 Validation loss: 0.016244\n",
            "Epoch: 577 Train loss: 0.016972 Validation loss: 0.016246\n",
            "Epoch: 578 Train loss: 0.016969 Validation loss: 0.016248\n",
            "Epoch: 579 Train loss: 0.016967 Validation loss: 0.01625\n",
            "Epoch: 580 Train loss: 0.016965 Validation loss: 0.016252\n",
            "Epoch: 581 Train loss: 0.016963 Validation loss: 0.016254\n",
            "Epoch: 582 Train loss: 0.016961 Validation loss: 0.016256\n",
            "Epoch: 583 Train loss: 0.016958 Validation loss: 0.016257\n",
            "Epoch: 584 Train loss: 0.016956 Validation loss: 0.016259\n",
            "Epoch: 585 Train loss: 0.016954 Validation loss: 0.016261\n",
            "Epoch: 586 Train loss: 0.016952 Validation loss: 0.016263\n",
            "Epoch: 587 Train loss: 0.016949 Validation loss: 0.016265\n",
            "Epoch: 588 Train loss: 0.016947 Validation loss: 0.016266\n",
            "Epoch: 589 Train loss: 0.016945 Validation loss: 0.016268\n",
            "Epoch: 590 Train loss: 0.016943 Validation loss: 0.01627\n",
            "Epoch: 591 Train loss: 0.016941 Validation loss: 0.016272\n",
            "Epoch: 592 Train loss: 0.016939 Validation loss: 0.016273\n",
            "Epoch: 593 Train loss: 0.016936 Validation loss: 0.016275\n",
            "Epoch: 594 Train loss: 0.016934 Validation loss: 0.016277\n",
            "Epoch: 595 Train loss: 0.016932 Validation loss: 0.016279\n",
            "Epoch: 596 Train loss: 0.01693 Validation loss: 0.016281\n",
            "Epoch: 597 Train loss: 0.016928 Validation loss: 0.016283\n",
            "Epoch: 598 Train loss: 0.016926 Validation loss: 0.016285\n",
            "Epoch: 599 Train loss: 0.016923 Validation loss: 0.016287\n",
            "Epoch: 600 Train loss: 0.016921 Validation loss: 0.016289\n",
            "Epoch: 601 Train loss: 0.016919 Validation loss: 0.01629\n",
            "Epoch: 602 Train loss: 0.016917 Validation loss: 0.016292\n",
            "Epoch: 603 Train loss: 0.016915 Validation loss: 0.016294\n",
            "Epoch: 604 Train loss: 0.016912 Validation loss: 0.016295\n",
            "Epoch: 605 Train loss: 0.01691 Validation loss: 0.016297\n",
            "Epoch: 606 Train loss: 0.016908 Validation loss: 0.016299\n",
            "Epoch: 607 Train loss: 0.016906 Validation loss: 0.016301\n",
            "Epoch: 608 Train loss: 0.016904 Validation loss: 0.016303\n",
            "Epoch: 609 Train loss: 0.016902 Validation loss: 0.016305\n",
            "Epoch: 610 Train loss: 0.016899 Validation loss: 0.016307\n",
            "Epoch: 611 Train loss: 0.016897 Validation loss: 0.016308\n",
            "Epoch: 612 Train loss: 0.016895 Validation loss: 0.01631\n",
            "Epoch: 613 Train loss: 0.016893 Validation loss: 0.016312\n",
            "Epoch: 614 Train loss: 0.016891 Validation loss: 0.016314\n",
            "Epoch: 615 Train loss: 0.016889 Validation loss: 0.016316\n",
            "Epoch: 616 Train loss: 0.016887 Validation loss: 0.016317\n",
            "Epoch: 617 Train loss: 0.016884 Validation loss: 0.016319\n",
            "Epoch: 618 Train loss: 0.016882 Validation loss: 0.016321\n",
            "Epoch: 619 Train loss: 0.01688 Validation loss: 0.016323\n",
            "Epoch: 620 Train loss: 0.016878 Validation loss: 0.016325\n",
            "Epoch: 621 Train loss: 0.016876 Validation loss: 0.016327\n",
            "Epoch: 622 Train loss: 0.016874 Validation loss: 0.016328\n",
            "Epoch: 623 Train loss: 0.016872 Validation loss: 0.01633\n",
            "Epoch: 624 Train loss: 0.016869 Validation loss: 0.016332\n",
            "Epoch: 625 Train loss: 0.016867 Validation loss: 0.016334\n",
            "Epoch: 626 Train loss: 0.016865 Validation loss: 0.016336\n",
            "Epoch: 627 Train loss: 0.016863 Validation loss: 0.016337\n",
            "Epoch: 628 Train loss: 0.016861 Validation loss: 0.016339\n",
            "Epoch: 629 Train loss: 0.016859 Validation loss: 0.016341\n",
            "Epoch: 630 Train loss: 0.016857 Validation loss: 0.016343\n",
            "Epoch: 631 Train loss: 0.016855 Validation loss: 0.016344\n",
            "Epoch: 632 Train loss: 0.016852 Validation loss: 0.016346\n",
            "Epoch: 633 Train loss: 0.01685 Validation loss: 0.016348\n",
            "Epoch: 634 Train loss: 0.016848 Validation loss: 0.01635\n",
            "Epoch: 635 Train loss: 0.016846 Validation loss: 0.016352\n",
            "Epoch: 636 Train loss: 0.016844 Validation loss: 0.016353\n",
            "Epoch: 637 Train loss: 0.016842 Validation loss: 0.016355\n",
            "Epoch: 638 Train loss: 0.01684 Validation loss: 0.016357\n",
            "Epoch: 639 Train loss: 0.016838 Validation loss: 0.016359\n",
            "Epoch: 640 Train loss: 0.016836 Validation loss: 0.016361\n",
            "Epoch: 641 Train loss: 0.016834 Validation loss: 0.016362\n",
            "Epoch: 642 Train loss: 0.016831 Validation loss: 0.016364\n",
            "Epoch: 643 Train loss: 0.016829 Validation loss: 0.016366\n",
            "Epoch: 644 Train loss: 0.016827 Validation loss: 0.016368\n",
            "Epoch: 645 Train loss: 0.016825 Validation loss: 0.01637\n",
            "Epoch: 646 Train loss: 0.016823 Validation loss: 0.016371\n",
            "Epoch: 647 Train loss: 0.016821 Validation loss: 0.016373\n",
            "Epoch: 648 Train loss: 0.016819 Validation loss: 0.016375\n",
            "Epoch: 649 Train loss: 0.016817 Validation loss: 0.016376\n",
            "Epoch: 650 Train loss: 0.016815 Validation loss: 0.016378\n",
            "Epoch: 651 Train loss: 0.016813 Validation loss: 0.01638\n",
            "Epoch: 652 Train loss: 0.01681 Validation loss: 0.016382\n",
            "Epoch: 653 Train loss: 0.016808 Validation loss: 0.016383\n",
            "Epoch: 654 Train loss: 0.016806 Validation loss: 0.016385\n",
            "Epoch: 655 Train loss: 0.016804 Validation loss: 0.016387\n",
            "Epoch: 656 Train loss: 0.016802 Validation loss: 0.016389\n",
            "Epoch: 657 Train loss: 0.0168 Validation loss: 0.01639\n",
            "Epoch: 658 Train loss: 0.016798 Validation loss: 0.016392\n",
            "Epoch: 659 Train loss: 0.016796 Validation loss: 0.016394\n",
            "Epoch: 660 Train loss: 0.016794 Validation loss: 0.016396\n",
            "Epoch: 661 Train loss: 0.016792 Validation loss: 0.016398\n",
            "Epoch: 662 Train loss: 0.01679 Validation loss: 0.0164\n",
            "Epoch: 663 Train loss: 0.016788 Validation loss: 0.016401\n",
            "Epoch: 664 Train loss: 0.016786 Validation loss: 0.016403\n",
            "Epoch: 665 Train loss: 0.016784 Validation loss: 0.016404\n",
            "Epoch: 666 Train loss: 0.016781 Validation loss: 0.016406\n",
            "Epoch: 667 Train loss: 0.016779 Validation loss: 0.016408\n",
            "Epoch: 668 Train loss: 0.016777 Validation loss: 0.01641\n",
            "Epoch: 669 Train loss: 0.016775 Validation loss: 0.016411\n",
            "Epoch: 670 Train loss: 0.016773 Validation loss: 0.016413\n",
            "Epoch: 671 Train loss: 0.016771 Validation loss: 0.016415\n",
            "Epoch: 672 Train loss: 0.016769 Validation loss: 0.016416\n",
            "Epoch: 673 Train loss: 0.016767 Validation loss: 0.016418\n",
            "Epoch: 674 Train loss: 0.016765 Validation loss: 0.01642\n",
            "Epoch: 675 Train loss: 0.016763 Validation loss: 0.016422\n",
            "Epoch: 676 Train loss: 0.016761 Validation loss: 0.016423\n",
            "Epoch: 677 Train loss: 0.016759 Validation loss: 0.016425\n",
            "Epoch: 678 Train loss: 0.016757 Validation loss: 0.016427\n",
            "Epoch: 679 Train loss: 0.016755 Validation loss: 0.016429\n",
            "Epoch: 680 Train loss: 0.016753 Validation loss: 0.01643\n",
            "Epoch: 681 Train loss: 0.016751 Validation loss: 0.016432\n",
            "Epoch: 682 Train loss: 0.016749 Validation loss: 0.016434\n",
            "Epoch: 683 Train loss: 0.016747 Validation loss: 0.016435\n",
            "Epoch: 684 Train loss: 0.016744 Validation loss: 0.016437\n",
            "Epoch: 685 Train loss: 0.016742 Validation loss: 0.016438\n",
            "Epoch: 686 Train loss: 0.01674 Validation loss: 0.016441\n",
            "Epoch: 687 Train loss: 0.016738 Validation loss: 0.016442\n",
            "Epoch: 688 Train loss: 0.016736 Validation loss: 0.016444\n",
            "Epoch: 689 Train loss: 0.016734 Validation loss: 0.016445\n",
            "Epoch: 690 Train loss: 0.016732 Validation loss: 0.016447\n",
            "Epoch: 691 Train loss: 0.01673 Validation loss: 0.016449\n",
            "Epoch: 692 Train loss: 0.016728 Validation loss: 0.01645\n",
            "Epoch: 693 Train loss: 0.016726 Validation loss: 0.016452\n",
            "Epoch: 694 Train loss: 0.016724 Validation loss: 0.016454\n",
            "Epoch: 695 Train loss: 0.016722 Validation loss: 0.016455\n",
            "Epoch: 696 Train loss: 0.01672 Validation loss: 0.016457\n",
            "Epoch: 697 Train loss: 0.016718 Validation loss: 0.016459\n",
            "Epoch: 698 Train loss: 0.016716 Validation loss: 0.01646\n",
            "Epoch: 699 Train loss: 0.016714 Validation loss: 0.016462\n",
            "Epoch: 700 Train loss: 0.016712 Validation loss: 0.016464\n",
            "Epoch: 701 Train loss: 0.01671 Validation loss: 0.016465\n",
            "Epoch: 702 Train loss: 0.016708 Validation loss: 0.016467\n",
            "Epoch: 703 Train loss: 0.016706 Validation loss: 0.016468\n",
            "Epoch: 704 Train loss: 0.016704 Validation loss: 0.01647\n",
            "Epoch: 705 Train loss: 0.016702 Validation loss: 0.016472\n",
            "Epoch: 706 Train loss: 0.0167 Validation loss: 0.016474\n",
            "Epoch: 707 Train loss: 0.016698 Validation loss: 0.016476\n",
            "Epoch: 708 Train loss: 0.016696 Validation loss: 0.016477\n",
            "Epoch: 709 Train loss: 0.016694 Validation loss: 0.016478\n",
            "Epoch: 710 Train loss: 0.016692 Validation loss: 0.01648\n",
            "Epoch: 711 Train loss: 0.01669 Validation loss: 0.016482\n",
            "Epoch: 712 Train loss: 0.016688 Validation loss: 0.016483\n",
            "Epoch: 713 Train loss: 0.016686 Validation loss: 0.016485\n",
            "Epoch: 714 Train loss: 0.016684 Validation loss: 0.016487\n",
            "Epoch: 715 Train loss: 0.016682 Validation loss: 0.016488\n",
            "Epoch: 716 Train loss: 0.01668 Validation loss: 0.01649\n",
            "Epoch: 717 Train loss: 0.016678 Validation loss: 0.016492\n",
            "Epoch: 718 Train loss: 0.016676 Validation loss: 0.016493\n",
            "Epoch: 719 Train loss: 0.016674 Validation loss: 0.016495\n",
            "Epoch: 720 Train loss: 0.016672 Validation loss: 0.016496\n",
            "Epoch: 721 Train loss: 0.01667 Validation loss: 0.016498\n",
            "Epoch: 722 Train loss: 0.016668 Validation loss: 0.0165\n",
            "Epoch: 723 Train loss: 0.016666 Validation loss: 0.016502\n",
            "Epoch: 724 Train loss: 0.016664 Validation loss: 0.016503\n",
            "Epoch: 725 Train loss: 0.016662 Validation loss: 0.016505\n",
            "Epoch: 726 Train loss: 0.01666 Validation loss: 0.016506\n",
            "Epoch: 727 Train loss: 0.016658 Validation loss: 0.016508\n",
            "Epoch: 728 Train loss: 0.016656 Validation loss: 0.01651\n",
            "Epoch: 729 Train loss: 0.016654 Validation loss: 0.016511\n",
            "Epoch: 730 Train loss: 0.016652 Validation loss: 0.016513\n",
            "Epoch: 731 Train loss: 0.01665 Validation loss: 0.016514\n",
            "Epoch: 732 Train loss: 0.016648 Validation loss: 0.016516\n",
            "Epoch: 733 Train loss: 0.016646 Validation loss: 0.016517\n",
            "Epoch: 734 Train loss: 0.016644 Validation loss: 0.016519\n",
            "Epoch: 735 Train loss: 0.016642 Validation loss: 0.016521\n",
            "Epoch: 736 Train loss: 0.01664 Validation loss: 0.016522\n",
            "Epoch: 737 Train loss: 0.016638 Validation loss: 0.016524\n",
            "Epoch: 738 Train loss: 0.016636 Validation loss: 0.016525\n",
            "Epoch: 739 Train loss: 0.016634 Validation loss: 0.016527\n",
            "Epoch: 740 Train loss: 0.016632 Validation loss: 0.016528\n",
            "Epoch: 741 Train loss: 0.01663 Validation loss: 0.01653\n",
            "Epoch: 742 Train loss: 0.016628 Validation loss: 0.016532\n",
            "Epoch: 743 Train loss: 0.016626 Validation loss: 0.016533\n",
            "Epoch: 744 Train loss: 0.016624 Validation loss: 0.016535\n",
            "Epoch: 745 Train loss: 0.016622 Validation loss: 0.016536\n",
            "Epoch: 746 Train loss: 0.01662 Validation loss: 0.016538\n",
            "Epoch: 747 Train loss: 0.016618 Validation loss: 0.016539\n",
            "Epoch: 748 Train loss: 0.016616 Validation loss: 0.016541\n",
            "Epoch: 749 Train loss: 0.016614 Validation loss: 0.016543\n",
            "Epoch: 750 Train loss: 0.016612 Validation loss: 0.016544\n",
            "Epoch: 751 Train loss: 0.01661 Validation loss: 0.016546\n",
            "Epoch: 752 Train loss: 0.016608 Validation loss: 0.016547\n",
            "Epoch: 753 Train loss: 0.016606 Validation loss: 0.016548\n",
            "Epoch: 754 Train loss: 0.016604 Validation loss: 0.01655\n",
            "Epoch: 755 Train loss: 0.016602 Validation loss: 0.016552\n",
            "Epoch: 756 Train loss: 0.0166 Validation loss: 0.016554\n",
            "Epoch: 757 Train loss: 0.016598 Validation loss: 0.016555\n",
            "Epoch: 758 Train loss: 0.016596 Validation loss: 0.016556\n",
            "Epoch: 759 Train loss: 0.016594 Validation loss: 0.016558\n",
            "Epoch: 760 Train loss: 0.016592 Validation loss: 0.01656\n",
            "Epoch: 761 Train loss: 0.01659 Validation loss: 0.016561\n",
            "Epoch: 762 Train loss: 0.016588 Validation loss: 0.016563\n",
            "Epoch: 763 Train loss: 0.016586 Validation loss: 0.016564\n",
            "Epoch: 764 Train loss: 0.016584 Validation loss: 0.016566\n",
            "Epoch: 765 Train loss: 0.016582 Validation loss: 0.016567\n",
            "Epoch: 766 Train loss: 0.01658 Validation loss: 0.016569\n",
            "Epoch: 767 Train loss: 0.016578 Validation loss: 0.01657\n",
            "Epoch: 768 Train loss: 0.016576 Validation loss: 0.016571\n",
            "Epoch: 769 Train loss: 0.016574 Validation loss: 0.016573\n",
            "Epoch: 770 Train loss: 0.016572 Validation loss: 0.016575\n",
            "Epoch: 771 Train loss: 0.01657 Validation loss: 0.016577\n",
            "Epoch: 772 Train loss: 0.016568 Validation loss: 0.016578\n",
            "Epoch: 773 Train loss: 0.016566 Validation loss: 0.016579\n",
            "Epoch: 774 Train loss: 0.016564 Validation loss: 0.016581\n",
            "Epoch: 775 Train loss: 0.016562 Validation loss: 0.016582\n",
            "Epoch: 776 Train loss: 0.01656 Validation loss: 0.016584\n",
            "Epoch: 777 Train loss: 0.016558 Validation loss: 0.016585\n",
            "Epoch: 778 Train loss: 0.016556 Validation loss: 0.016587\n",
            "Epoch: 779 Train loss: 0.016554 Validation loss: 0.016588\n",
            "Epoch: 780 Train loss: 0.016552 Validation loss: 0.01659\n",
            "Epoch: 781 Train loss: 0.01655 Validation loss: 0.016591\n",
            "Epoch: 782 Train loss: 0.016548 Validation loss: 0.016593\n",
            "Epoch: 783 Train loss: 0.016547 Validation loss: 0.016594\n",
            "Epoch: 784 Train loss: 0.016545 Validation loss: 0.016596\n",
            "Epoch: 785 Train loss: 0.016543 Validation loss: 0.016597\n",
            "Epoch: 786 Train loss: 0.016541 Validation loss: 0.016599\n",
            "Epoch: 787 Train loss: 0.016539 Validation loss: 0.0166\n",
            "Epoch: 788 Train loss: 0.016537 Validation loss: 0.016601\n",
            "Epoch: 789 Train loss: 0.016535 Validation loss: 0.016603\n",
            "Epoch: 790 Train loss: 0.016533 Validation loss: 0.016605\n",
            "Epoch: 791 Train loss: 0.016531 Validation loss: 0.016606\n",
            "Epoch: 792 Train loss: 0.016529 Validation loss: 0.016608\n",
            "Epoch: 793 Train loss: 0.016527 Validation loss: 0.016609\n",
            "Epoch: 794 Train loss: 0.016525 Validation loss: 0.01661\n",
            "Epoch: 795 Train loss: 0.016523 Validation loss: 0.016612\n",
            "Epoch: 796 Train loss: 0.016521 Validation loss: 0.016613\n",
            "Epoch: 797 Train loss: 0.016519 Validation loss: 0.016615\n",
            "Epoch: 798 Train loss: 0.016517 Validation loss: 0.016616\n",
            "Epoch: 799 Train loss: 0.016515 Validation loss: 0.016618\n",
            "Epoch: 800 Train loss: 0.016513 Validation loss: 0.016619\n",
            "Epoch: 801 Train loss: 0.016511 Validation loss: 0.016621\n",
            "Epoch: 802 Train loss: 0.016509 Validation loss: 0.016623\n",
            "Epoch: 803 Train loss: 0.016507 Validation loss: 0.016623\n",
            "Epoch: 804 Train loss: 0.016505 Validation loss: 0.016625\n",
            "Epoch: 805 Train loss: 0.016503 Validation loss: 0.016626\n",
            "Epoch: 806 Train loss: 0.016501 Validation loss: 0.016628\n",
            "Epoch: 807 Train loss: 0.016499 Validation loss: 0.016629\n",
            "Epoch: 808 Train loss: 0.016497 Validation loss: 0.016631\n",
            "Epoch: 809 Train loss: 0.016495 Validation loss: 0.016632\n",
            "Epoch: 810 Train loss: 0.016493 Validation loss: 0.016634\n",
            "Epoch: 811 Train loss: 0.016491 Validation loss: 0.016635\n",
            "Epoch: 812 Train loss: 0.016489 Validation loss: 0.016637\n",
            "Epoch: 813 Train loss: 0.016487 Validation loss: 0.016638\n",
            "Epoch: 814 Train loss: 0.016485 Validation loss: 0.016639\n",
            "Epoch: 815 Train loss: 0.016483 Validation loss: 0.016641\n",
            "Epoch: 816 Train loss: 0.016481 Validation loss: 0.016642\n",
            "Epoch: 817 Train loss: 0.016479 Validation loss: 0.016643\n",
            "Epoch: 818 Train loss: 0.016477 Validation loss: 0.016645\n",
            "Epoch: 819 Train loss: 0.016475 Validation loss: 0.016646\n",
            "Epoch: 820 Train loss: 0.016473 Validation loss: 0.016648\n",
            "Epoch: 821 Train loss: 0.016472 Validation loss: 0.016649\n",
            "Epoch: 822 Train loss: 0.01647 Validation loss: 0.016651\n",
            "Epoch: 823 Train loss: 0.016468 Validation loss: 0.016652\n",
            "Epoch: 824 Train loss: 0.016466 Validation loss: 0.016653\n",
            "Epoch: 825 Train loss: 0.016464 Validation loss: 0.016655\n",
            "Epoch: 826 Train loss: 0.016462 Validation loss: 0.016656\n",
            "Epoch: 827 Train loss: 0.01646 Validation loss: 0.016658\n",
            "Epoch: 828 Train loss: 0.016458 Validation loss: 0.016659\n",
            "Epoch: 829 Train loss: 0.016456 Validation loss: 0.01666\n",
            "Epoch: 830 Train loss: 0.016454 Validation loss: 0.016662\n",
            "Epoch: 831 Train loss: 0.016452 Validation loss: 0.016663\n",
            "Epoch: 832 Train loss: 0.01645 Validation loss: 0.016665\n",
            "Epoch: 833 Train loss: 0.016448 Validation loss: 0.016666\n",
            "Epoch: 834 Train loss: 0.016446 Validation loss: 0.016667\n",
            "Epoch: 835 Train loss: 0.016444 Validation loss: 0.016668\n",
            "Epoch: 836 Train loss: 0.016442 Validation loss: 0.01667\n",
            "Epoch: 837 Train loss: 0.01644 Validation loss: 0.016672\n",
            "Epoch: 838 Train loss: 0.016438 Validation loss: 0.016673\n",
            "Epoch: 839 Train loss: 0.016436 Validation loss: 0.016674\n",
            "Epoch: 840 Train loss: 0.016434 Validation loss: 0.016675\n",
            "Epoch: 841 Train loss: 0.016432 Validation loss: 0.016677\n",
            "Epoch: 842 Train loss: 0.01643 Validation loss: 0.016678\n",
            "Epoch: 843 Train loss: 0.016428 Validation loss: 0.016679\n",
            "Epoch: 844 Train loss: 0.016426 Validation loss: 0.016681\n",
            "Epoch: 845 Train loss: 0.016424 Validation loss: 0.016682\n",
            "Epoch: 846 Train loss: 0.016422 Validation loss: 0.016684\n",
            "Epoch: 847 Train loss: 0.01642 Validation loss: 0.016685\n",
            "Epoch: 848 Train loss: 0.016418 Validation loss: 0.016686\n",
            "Epoch: 849 Train loss: 0.016416 Validation loss: 0.016687\n",
            "Epoch: 850 Train loss: 0.016414 Validation loss: 0.016689\n",
            "Epoch: 851 Train loss: 0.016412 Validation loss: 0.01669\n",
            "Epoch: 852 Train loss: 0.01641 Validation loss: 0.016692\n",
            "Epoch: 853 Train loss: 0.016408 Validation loss: 0.016693\n",
            "Epoch: 854 Train loss: 0.016406 Validation loss: 0.016694\n",
            "Epoch: 855 Train loss: 0.016404 Validation loss: 0.016696\n",
            "Epoch: 856 Train loss: 0.016402 Validation loss: 0.016697\n",
            "Epoch: 857 Train loss: 0.0164 Validation loss: 0.016698\n",
            "Epoch: 858 Train loss: 0.016398 Validation loss: 0.0167\n",
            "Epoch: 859 Train loss: 0.016396 Validation loss: 0.016701\n",
            "Epoch: 860 Train loss: 0.016394 Validation loss: 0.016702\n",
            "Epoch: 861 Train loss: 0.016392 Validation loss: 0.016703\n",
            "Epoch: 862 Train loss: 0.01639 Validation loss: 0.016705\n",
            "Epoch: 863 Train loss: 0.016388 Validation loss: 0.016706\n",
            "Epoch: 864 Train loss: 0.016386 Validation loss: 0.016708\n",
            "Epoch: 865 Train loss: 0.016384 Validation loss: 0.016709\n",
            "Epoch: 866 Train loss: 0.016382 Validation loss: 0.01671\n",
            "Epoch: 867 Train loss: 0.01638 Validation loss: 0.016711\n",
            "Epoch: 868 Train loss: 0.016378 Validation loss: 0.016713\n",
            "Epoch: 869 Train loss: 0.016376 Validation loss: 0.016714\n",
            "Epoch: 870 Train loss: 0.016374 Validation loss: 0.016715\n",
            "Epoch: 871 Train loss: 0.016372 Validation loss: 0.016716\n",
            "Epoch: 872 Train loss: 0.01637 Validation loss: 0.016718\n",
            "Epoch: 873 Train loss: 0.016368 Validation loss: 0.016719\n",
            "Epoch: 874 Train loss: 0.016366 Validation loss: 0.016721\n",
            "Epoch: 875 Train loss: 0.016364 Validation loss: 0.016722\n",
            "Epoch: 876 Train loss: 0.016362 Validation loss: 0.016723\n",
            "Epoch: 877 Train loss: 0.01636 Validation loss: 0.016725\n",
            "Epoch: 878 Train loss: 0.016358 Validation loss: 0.016726\n",
            "Epoch: 879 Train loss: 0.016356 Validation loss: 0.016727\n",
            "Epoch: 880 Train loss: 0.016354 Validation loss: 0.016728\n",
            "Epoch: 881 Train loss: 0.016352 Validation loss: 0.01673\n",
            "Epoch: 882 Train loss: 0.01635 Validation loss: 0.016731\n",
            "Epoch: 883 Train loss: 0.016348 Validation loss: 0.016732\n",
            "Epoch: 884 Train loss: 0.016346 Validation loss: 0.016733\n",
            "Epoch: 885 Train loss: 0.016344 Validation loss: 0.016735\n",
            "Epoch: 886 Train loss: 0.016342 Validation loss: 0.016736\n",
            "Epoch: 887 Train loss: 0.01634 Validation loss: 0.016737\n",
            "Epoch: 888 Train loss: 0.016338 Validation loss: 0.016738\n",
            "Epoch: 889 Train loss: 0.016336 Validation loss: 0.01674\n",
            "Epoch: 890 Train loss: 0.016334 Validation loss: 0.016741\n",
            "Epoch: 891 Train loss: 0.016332 Validation loss: 0.016742\n",
            "Epoch: 892 Train loss: 0.01633 Validation loss: 0.016744\n",
            "Epoch: 893 Train loss: 0.016328 Validation loss: 0.016745\n",
            "Epoch: 894 Train loss: 0.016326 Validation loss: 0.016746\n",
            "Epoch: 895 Train loss: 0.016324 Validation loss: 0.016747\n",
            "Epoch: 896 Train loss: 0.016322 Validation loss: 0.016749\n",
            "Epoch: 897 Train loss: 0.016319 Validation loss: 0.016749\n",
            "Epoch: 898 Train loss: 0.016317 Validation loss: 0.016751\n",
            "Epoch: 899 Train loss: 0.016315 Validation loss: 0.016752\n",
            "Epoch: 900 Train loss: 0.016313 Validation loss: 0.016753\n",
            "Epoch: 901 Train loss: 0.016311 Validation loss: 0.016755\n",
            "Epoch: 902 Train loss: 0.016309 Validation loss: 0.016756\n",
            "Epoch: 903 Train loss: 0.016307 Validation loss: 0.016757\n",
            "Epoch: 904 Train loss: 0.016305 Validation loss: 0.016759\n",
            "Epoch: 905 Train loss: 0.016303 Validation loss: 0.01676\n",
            "Epoch: 906 Train loss: 0.016301 Validation loss: 0.016761\n",
            "Epoch: 907 Train loss: 0.016299 Validation loss: 0.016762\n",
            "Epoch: 908 Train loss: 0.016297 Validation loss: 0.016763\n",
            "Epoch: 909 Train loss: 0.016295 Validation loss: 0.016764\n",
            "Epoch: 910 Train loss: 0.016293 Validation loss: 0.016766\n",
            "Epoch: 911 Train loss: 0.016291 Validation loss: 0.016767\n",
            "Epoch: 912 Train loss: 0.016289 Validation loss: 0.016768\n",
            "Epoch: 913 Train loss: 0.016287 Validation loss: 0.01677\n",
            "Epoch: 914 Train loss: 0.016284 Validation loss: 0.016771\n",
            "Epoch: 915 Train loss: 0.016282 Validation loss: 0.016772\n",
            "Epoch: 916 Train loss: 0.01628 Validation loss: 0.016773\n",
            "Epoch: 917 Train loss: 0.016278 Validation loss: 0.016774\n",
            "Epoch: 918 Train loss: 0.016276 Validation loss: 0.016776\n",
            "Epoch: 919 Train loss: 0.016274 Validation loss: 0.016777\n",
            "Epoch: 920 Train loss: 0.016272 Validation loss: 0.016778\n",
            "Epoch: 921 Train loss: 0.01627 Validation loss: 0.016779\n",
            "Epoch: 922 Train loss: 0.016268 Validation loss: 0.016781\n",
            "Epoch: 923 Train loss: 0.016266 Validation loss: 0.016782\n",
            "Epoch: 924 Train loss: 0.016264 Validation loss: 0.016783\n",
            "Epoch: 925 Train loss: 0.016262 Validation loss: 0.016784\n",
            "Epoch: 926 Train loss: 0.016259 Validation loss: 0.016785\n",
            "Epoch: 927 Train loss: 0.016257 Validation loss: 0.016787\n",
            "Epoch: 928 Train loss: 0.016255 Validation loss: 0.016788\n",
            "Epoch: 929 Train loss: 0.016253 Validation loss: 0.016789\n",
            "Epoch: 930 Train loss: 0.016251 Validation loss: 0.01679\n",
            "Epoch: 931 Train loss: 0.016249 Validation loss: 0.016791\n",
            "Epoch: 932 Train loss: 0.016247 Validation loss: 0.016793\n",
            "Epoch: 933 Train loss: 0.016245 Validation loss: 0.016794\n",
            "Epoch: 934 Train loss: 0.016243 Validation loss: 0.016795\n",
            "Epoch: 935 Train loss: 0.016241 Validation loss: 0.016796\n",
            "Epoch: 936 Train loss: 0.016238 Validation loss: 0.016797\n",
            "Epoch: 937 Train loss: 0.016236 Validation loss: 0.016798\n",
            "Epoch: 938 Train loss: 0.016234 Validation loss: 0.016799\n",
            "Epoch: 939 Train loss: 0.016232 Validation loss: 0.016801\n",
            "Epoch: 940 Train loss: 0.01623 Validation loss: 0.016802\n",
            "Epoch: 941 Train loss: 0.016228 Validation loss: 0.016803\n",
            "Epoch: 942 Train loss: 0.016226 Validation loss: 0.016804\n",
            "Epoch: 943 Train loss: 0.016223 Validation loss: 0.016805\n",
            "Epoch: 944 Train loss: 0.016221 Validation loss: 0.016807\n",
            "Epoch: 945 Train loss: 0.016219 Validation loss: 0.016808\n",
            "Epoch: 946 Train loss: 0.016217 Validation loss: 0.016809\n",
            "Epoch: 947 Train loss: 0.016215 Validation loss: 0.01681\n",
            "Epoch: 948 Train loss: 0.016213 Validation loss: 0.016811\n",
            "Epoch: 949 Train loss: 0.016211 Validation loss: 0.016812\n",
            "Epoch: 950 Train loss: 0.016208 Validation loss: 0.016813\n",
            "Epoch: 951 Train loss: 0.016206 Validation loss: 0.016815\n",
            "Epoch: 952 Train loss: 0.016204 Validation loss: 0.016816\n",
            "Epoch: 953 Train loss: 0.016202 Validation loss: 0.016817\n",
            "Epoch: 954 Train loss: 0.0162 Validation loss: 0.016818\n",
            "Epoch: 955 Train loss: 0.016198 Validation loss: 0.016819\n",
            "Epoch: 956 Train loss: 0.016196 Validation loss: 0.01682\n",
            "Epoch: 957 Train loss: 0.016193 Validation loss: 0.016821\n",
            "Epoch: 958 Train loss: 0.016191 Validation loss: 0.016822\n",
            "Epoch: 959 Train loss: 0.016189 Validation loss: 0.016824\n",
            "Epoch: 960 Train loss: 0.016187 Validation loss: 0.016825\n",
            "Epoch: 961 Train loss: 0.016185 Validation loss: 0.016826\n",
            "Epoch: 962 Train loss: 0.016182 Validation loss: 0.016827\n",
            "Epoch: 963 Train loss: 0.01618 Validation loss: 0.016828\n",
            "Epoch: 964 Train loss: 0.016178 Validation loss: 0.016829\n",
            "Epoch: 965 Train loss: 0.016176 Validation loss: 0.016831\n",
            "Epoch: 966 Train loss: 0.016174 Validation loss: 0.016832\n",
            "Epoch: 967 Train loss: 0.016171 Validation loss: 0.016833\n",
            "Epoch: 968 Train loss: 0.016169 Validation loss: 0.016834\n",
            "Epoch: 969 Train loss: 0.016167 Validation loss: 0.016835\n",
            "Epoch: 970 Train loss: 0.016165 Validation loss: 0.016836\n",
            "Epoch: 971 Train loss: 0.016163 Validation loss: 0.016837\n",
            "Epoch: 972 Train loss: 0.01616 Validation loss: 0.016839\n",
            "Epoch: 973 Train loss: 0.016158 Validation loss: 0.016839\n",
            "Epoch: 974 Train loss: 0.016156 Validation loss: 0.016841\n",
            "Epoch: 975 Train loss: 0.016154 Validation loss: 0.016842\n",
            "Epoch: 976 Train loss: 0.016151 Validation loss: 0.016843\n",
            "Epoch: 977 Train loss: 0.016149 Validation loss: 0.016844\n",
            "Epoch: 978 Train loss: 0.016147 Validation loss: 0.016845\n",
            "Epoch: 979 Train loss: 0.016145 Validation loss: 0.016846\n",
            "Epoch: 980 Train loss: 0.016143 Validation loss: 0.016847\n",
            "Epoch: 981 Train loss: 0.01614 Validation loss: 0.016848\n",
            "Epoch: 982 Train loss: 0.016138 Validation loss: 0.016849\n",
            "Epoch: 983 Train loss: 0.016136 Validation loss: 0.016851\n",
            "Epoch: 984 Train loss: 0.016134 Validation loss: 0.016852\n",
            "Epoch: 985 Train loss: 0.016131 Validation loss: 0.016853\n",
            "Epoch: 986 Train loss: 0.016129 Validation loss: 0.016854\n",
            "Epoch: 987 Train loss: 0.016127 Validation loss: 0.016855\n",
            "Epoch: 988 Train loss: 0.016124 Validation loss: 0.016856\n",
            "Epoch: 989 Train loss: 0.016122 Validation loss: 0.016857\n",
            "Epoch: 990 Train loss: 0.01612 Validation loss: 0.016858\n",
            "Epoch: 991 Train loss: 0.016118 Validation loss: 0.016859\n",
            "Epoch: 992 Train loss: 0.016115 Validation loss: 0.01686\n",
            "Epoch: 993 Train loss: 0.016113 Validation loss: 0.016861\n",
            "Epoch: 994 Train loss: 0.016111 Validation loss: 0.016862\n",
            "Epoch: 995 Train loss: 0.016108 Validation loss: 0.016864\n",
            "Epoch: 996 Train loss: 0.016106 Validation loss: 0.016865\n",
            "Epoch: 997 Train loss: 0.016104 Validation loss: 0.016866\n",
            "Epoch: 998 Train loss: 0.016102 Validation loss: 0.016867\n",
            "Epoch: 999 Train loss: 0.016099 Validation loss: 0.016868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylr1C2ovWXbO",
        "colab_type": "text"
      },
      "source": [
        "During the training of the regularized model we can already notice, that, although there is still a difference between training and validation loss, the validation loss decreases as the training loss dreases. The effect of the regularization becomes even more evident if we plot the predictions of the regularized model and the overfitting model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdMRV0vAWLmm",
        "colab_type": "code",
        "outputId": "73a627f2-e350-4a7c-88df-26ea0c7e1730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "\"\"\" We now want to plot the prediction of the regularized and unregularized big model. \"\"\"\n",
        "\n",
        "y_pred = big_reg_mdl(x)# Predict with \"big_reg_mdl\" on \"x\"\n",
        "y_pred_overfit = big_mdl(x)# Predict with \"reg_mdl\" on \"x\"\n",
        "plt.scatter(x_train_overfit, y_train_overfit)\n",
        "plt.plot(x, y_true)\n",
        "plt.plot(x, y_pred.numpy())\n",
        "plt.plot(x, y_pred_overfit.numpy())\n",
        "plt.legend([\"Target\", \"Regularization\", \"No regularization\", \"Training samples\"])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhUJRDVHGX6FQKKoh/w/xNy3w5mrtoAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sePQBaGqZzT6",
        "colab_type": "text"
      },
      "source": [
        "The model with regularization seems to follow the overall trend of the data, while the model without any regularization very precisely fits the training samples. This is espacilly evident in the interval $\\left[0,0.5\\right]$, where the prediction of the unregularized model shows an oscillating behavior. Such oscillations are however not present in the ground truth and therefore undesirable. The regularized model on the other hand is not as flexible as the unregularized model and therefore does not fit the target function well in the interval $\\left[2.25, 3.0\\right]$.\n",
        "\n",
        "## Conclusion\n",
        "In this exercise we revisited the mathematical background for a simple regression task and covered it's practical implementation in Tensorflow 2. We also explored the phenomenon of overfitting and derived different regularizations from a probabilistic perspective. This exercise covers a very simple task with a very basic neural architecture and is intended as a primer for the second part of the regression exercise, which is dealing with a bigger and more realistic problem. In this second part we will consider the problem of regressing the age of a person from a potrait picture."
      ]
    }
  ]
}